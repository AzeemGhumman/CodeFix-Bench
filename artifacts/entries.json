[
    {
        "pr": "sqlfluff/sqlfluff/4764",
        "problem": "Enable quiet mode/no-verbose in CLI for use in pre-commit hook\nThere seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\r\n\r\nIt would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\r\n![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\r\n\r\nThis hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\n",
        "hint": "",
        "base": "a820c139ccbe6d1865d73c4a459945cd69899f8f",
        "env": "d19de0ecd16d298f9e3bfb91da122734c40c01e5",
        "files": [
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/cli/formatters.py",
            "src/sqlfluff/core/linter/linted_dir.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2862",
        "problem": "fix keep adding new line on wrong place \n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nTo replicate this issue you can create a file eg. test.template.sql \r\n\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n{%- endif %}\r\n```\r\n\r\nthen run:\r\n```\r\nsqlfluff fix test.template.sql  \r\n```\r\n\r\nThis will give you:\r\n```\r\nL:   2 | P:  12 | L009 | Files must end with a trailing newline.\r\n```\r\n\r\nAnd the result of the file is now:\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n\r\n{%- endif %}\r\n```\r\n\r\nIf i run it again it will complain on the same issue and the result of the file would be: \r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n\r\n\r\n{%- endif %}\r\n```\r\n\r\nAnd so on. \n\n### Expected Behaviour\n\nThe expected behavior would be to add the new line at the end of the file, that is after `{%- endif %}` instead of adding the new line at the end of the SQL query - so the result should look like this: \r\n\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n{%- endif %}\r\n\r\n```\n\n### Observed Behaviour\n\nAdds a new line to the end of the SQL query instead of in the end of the file. \n\n### How to reproduce\n\nAlready mentioned above (in What Happened section).\n\n### Dialect\n\nsnowflake\n\n### Version\n\nsqlfluff, version 0.6.2\n\n### Configuration\n\n[sqlfluff]\r\nverbose = 1\r\ndialect = snowflake\r\ntemplater = jinja\r\nexclude_rules = L027,L031,L032,L036,L044,L046,L034\r\noutput_line_length = 121\r\nsql_file_exts=.sql\r\n\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 250\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = aliases\r\n\r\n\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L030]  # function names\r\ncapitalisation_policy = upper\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "> Version\r\n> sqlfluff, version 0.6.2\r\n\r\nIs this correct? If so that is a VERY old version so please upgrade. Though confirmed this is still an issue in latest. But still, going to need to upgrade to get any fix for this.\n> > Version\r\n> > sqlfluff, version 0.6.2\r\n> \r\n> Is this correct? If so that is a VERY old version so please upgrade. Though confirmed this is still an issue in latest. But still, going to need to upgrade to get any fix for this.\r\n\r\nThanks for your response! I had sqlfluff globally installed with version 0.6.2 but i changed it now to 0.11.0 and still it is the same issue.\nThe rule probably needs updating to be \"template aware\".  A few other rules have required similar updates and may provide useful inspiration for a fix.\r\n\r\n```\r\nsrc/sqlfluff/rules/L019.py\r\n140:                    and not last_seg.is_templated\r\n209:                if last_seg.is_type(\"comma\") and not context.segment.is_templated:\r\n\r\nsrc/sqlfluff/rules/L003.py\r\n77:        if elem.is_type(\"whitespace\") and elem.is_templated:\r\n148:                templated_line = elem.is_templated\r\n\r\nsrc/sqlfluff/rules/L010.py\r\n87:        if context.segment.is_templated:\r\n```\nI can't reproduce this issue with SQLFluff 0.11.0. This is the terminal output I get:\r\n```\r\n(sqlfluff-0.11.0) \u279c  /tmp sqlfluff fix test.template.sql\r\n==== sqlfluff ====\r\nsqlfluff:               0.11.0 python:                  3.9.1\r\nimplementation:        cpython dialect:             snowflake\r\nverbosity:                   1 templater:               jinja\r\n\r\n==== finding fixable violations ====\r\n=== [ path: test.template.sql ] ===\r\n\r\n== [test.template.sql] FAIL                                                                                                                                                                             \r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1                                                                                                                              \r\n==== fixing violations ====\r\n1 fixable linting violations found\r\nAre you sure you wish to attempt to fix these? [Y/n] ...\r\nAttempting fixes...\r\nPersisting Changes...\r\n== [test.template.sql] PASS\r\nDone. Please check your files to confirm.\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n```\r\n\r\nAnd this is the resulting file. SQLFluff indented line 2 but no newline was added.\r\n```\r\n{% if true %}\r\n    SELECT 1 + 1\r\n{%- endif %}\r\n```\nI can @barrywhart but it only works when the final newline in the file doesn't exist.\r\n\r\nIf on mac you can run something like this to strip the final newline character:\r\n\r\n```\r\ntruncate -s -1 test.sql > test2.sql\r\n```\r\n\r\nThen fix `test2.sql` with default config and you'll see it.\nThere's a bug in `JinjaTracer` -- if a Jinja block (e.g. `{% endif %}` is the final slice in the file (i. there's no final newline), that slice is missing from the output. This will have to be fixed before we can fix L009, because at present, L009 cannot \"see\" that `{% endif %}` after the `1`.",
        "base": "447ecf862a4d2b977d0add9f444655357b9c4f1f",
        "env": "3d52e8270d82aeccf4c516d059a80a6947919aea",
        "files": [
            "src/sqlfluff/core/linter/common.py",
            "src/sqlfluff/core/linter/linted_file.py",
            "src/sqlfluff/core/parser/lexer.py",
            "src/sqlfluff/core/parser/segments/base.py",
            "src/sqlfluff/core/rules/base.py",
            "src/sqlfluff/core/templaters/slicers/tracer.py",
            "src/sqlfluff/rules/L009.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2336",
        "problem": "L026: Rule incorrectly flag column does not exist in `FROM` clause in an UPDATE statement.\n## Expected Behaviour\r\n\r\nL026 should not fail when a subquery in an UPDATE statement references a column from the UPDATE target.\r\n\r\n## Observed Behaviour\r\n\r\nL026 failed due to reference was not found in the FROM clause with the following error printed (When using `sample.sql` content below)\r\n\r\n```\r\nL:   7 | P:  28 | L026 | Reference 'my_table.id' refers to table/view not found\r\n                       | in the FROM clause or found in parent subquery.\r\n```\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create `sample.sql` with the content below\r\n```\r\nUPDATE my_table\r\nSET row_sum = (\r\n    SELECT COUNT(*) AS row_sum\r\n    FROM\r\n        another_table\r\n    WHERE\r\n        another_table.id = my_table.id\r\n);\r\n```\r\n2. Run SQLFluff by `sqlfluff lint sample.sql`\r\n\r\n## Dialect\r\n\r\nDefault / Ansi (No dialect specified)\r\n\r\n## Version\r\n```\r\n(.venv) ~/code/sqlfluff (main) $ sqlfluff --version\r\nsqlfluff, version 0.9.0\r\n```\r\n\r\n```\r\n(.venv) ~/code/sqlfluff (main) $ python --version\r\nPython 3.9.9\r\n```\r\n\r\n## Configuration\r\nDefault. No customization.\r\n\n",
        "hint": "",
        "base": "37a993f7ad841ab3035d1db5ce6525f2e5584fd5",
        "env": "a5c4eae4e3e419fe95460c9afd9cf39a35a470c4",
        "files": [
            "src/sqlfluff/core/rules/analysis/select.py",
            "src/sqlfluff/core/rules/reference.py",
            "src/sqlfluff/dialects/dialect_ansi.py",
            "src/sqlfluff/dialects/dialect_bigquery.py",
            "src/sqlfluff/rules/L025.py",
            "src/sqlfluff/rules/L026.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/5074",
        "problem": "Inconsistent output depending on --processes flag when --ignore linting is used\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nDepending on the value you set for the `--processes` flag when also using `--ignore linting`, different output with different exit codes are generated.\n\n### Expected Behaviour\n\nThe same exit code should be generated, independently of the `--processes` flag. Furthermore, from https://docs.sqlfluff.com/en/stable/production.html#using-sqlfluff-on-a-whole-sql-codebase I would expect that exit codes should be either `0` or `65`, not `1`.\n\n### Observed Behaviour\n\nSee the How to reproduce section.\n\n### How to reproduce\n\nCreate a `test.sql` file with the following content:\r\n\r\n```SQL\r\nCREATE TABLE example (\r\n    id TEXT DEFAULT 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. In condimentum congue est, ac orci aliquam.' PRIMARY KEY\r\n);\r\n```\r\n\r\nThe line is too long according to SQLFluff, caused by the large default value, so let's see the the output of SQLFluff.\r\n\r\nRunning\r\n\r\n```SHELL\r\nsqlfluff fix --dialect postgres --ignore linting --processes 2\r\n```\r\n\r\nresults in \r\n\r\n```\r\n==== finding fixable violations ====\r\n==== no fixable linting violations found ====                                                                                                                                                                      \r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n  [1 unfixable linting violations found]\r\n```\r\n\r\nwith exit code `1`. Running the same with one process instead:\r\n\r\n```SHELL\r\nsqlfluff fix --dialect postgres --ignore linting --processes 1\r\n```\r\n\r\nresults in\r\n\r\n```\r\n==== finding fixable violations ====\r\n==== no fixable linting violations found ====                                                                                                                                                                      \r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n```\r\n\r\nand exit code `0`\r\n\r\nSame behaviour for `lint` and `format` commands.\n\n### Dialect\n\nPostgres\n\n### Version\n\n2.2.0, Python 3.10.6\n\n### Configuration\n\nNone, it's all in the CLI flags.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "This is _very_ interesting! I'll pick this one up \ud83d\udc4d \nIt's worth calling out that I think part of the problem here is that the docs are also out of date - but you're still right that the return codes should be _the same_ regardless of the `processes` setting.",
        "base": "7b7fd603a19755a9f3707ebbf95d18ee635716d8",
        "env": "7b7fd603a19755a9f3707ebbf95d18ee635716d8",
        "files": [
            "src/sqlfluff/core/errors.py",
            "src/sqlfluff/core/parser/markers.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3436",
        "problem": "Fatal templating error with Jinja templater. Tracer produces odd results.\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nIssue found while assessing an Airflow project.\r\n\r\nThe smallest query I can make which triggers the issue is: \r\n```sql\r\nSELECT\r\n\t{% block table_name %}a{% endblock %}.b\r\nFROM d.{{ self.table_name() }}\r\n```\r\n\r\nWhen running this query through `lint` I get an `AssertionError`, or if running on the more friendly error message PR (#3433) I get: `WARNING    Length of templated file mismatch with final slice: 21 != 19.`.\n\n### Expected Behaviour\n\nThis query should slice properly and probably eventually give a jinja error that the required variables are undefined.\n\n### Observed Behaviour\n\nI've dug a little into the error and the sliced file being produced is:\r\n\r\n```python\r\n[\r\n    TemplatedFileSlice(slice_type='literal', source_slice=slice(0, 8, None), templated_slice=slice(0, 8, None)),\r\n    TemplatedFileSlice(slice_type='block_start', source_slice=slice(8, 30, None), templated_slice=slice(8, 8, None)),\r\n    TemplatedFileSlice(slice_type='literal', source_slice=slice(30, 31, None), templated_slice=slice(8, 9, None)),\r\n    TemplatedFileSlice(slice_type='block_end', source_slice=slice(31, 45, None), templated_slice=slice(9, 9, None)),\r\n    TemplatedFileSlice(slice_type='literal', source_slice=slice(45, 55, None), templated_slice=slice(9, 19, None)),\r\n    TemplatedFileSlice(slice_type='templated', source_slice=slice(55, 78, None), templated_slice=slice(19, 19, None)),\r\n    TemplatedFileSlice(slice_type='literal', source_slice=slice(78, 79, None), templated_slice=slice(19, 19, None))\r\n]\r\n```\r\n\r\nThe issue is that while the `source_slice` looks correct for the slices, almost all of the `templated_slices` values have zero length, and importantly the last one doesn't end at position 21.\r\n\r\nThe rendered file is `SELECT\\n\\ta.b\\nFROM d.a\\n` (I've included the escape chars) which is indeed 21 chars long.\r\n\r\n@barrywhart I might need your help to work out what's going on with the Jinja tracer here.\n\n### How to reproduce\n\nRun provided query, `main` branch. Set to the `jinja` templater.\n\n### Dialect\n\ndialect is set to `snowflake`, but I don't think we're getting far enough for that to make a difference.\n\n### Version\n\n`main` branch commit `cb6357c540d2d968f766f3a7a4fa16f231cb80e4` (and a few branches derived from it)\n\n### Configuration\n\nN/A\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "I'll take a look.\r\n\r\nAnd darn it -- first bug report against this code in the past couple months, I think. \ud83d\ude05\nStarting to look at this. One problem I noticed (perhaps not the only one) is that the trailing literal newline in the source string has no corresponding templated slice, so it's like building the templated slice array has stopped early for some reason.\r\n\r\nThe 0-length slices may be legit. Will share more as I learn things, but is `{% block %}` a Jinja builtin or an extension? If it's an extension, maybe base Jinja is just skipping it (i.e. rendering it as empty string). \nOk, I think the issue is not related to undefined variables. I get the same assertion error if I define the variable prior to the block, e.g.:\r\n```\r\n{% set table_name = \"abc\" %}\r\nSELECT {% block table_name %}a{% endblock %} FROM {{ self.table_name() }}\r\n```\r\n\r\nI'm pretty sure the real issue is that we aren't handling `{% block %}` correctly **at all** (probably because I hadn't heard of it before \ud83e\udd2a).\r\n\r\nII think it should be handled similarly to `{% set %}` or `{% macro %}` blocks, i.e. basically don't trace when they are **defined**, only when they are **used**.\r\n\r\nI should be able to fix it this week. For now, just need to let my brain recover from looking at this code again. Even though I wrote it, it's a little too \"meta\" for me to stare at it for more than 1-2 hours at a time. \ud83d\ude05",
        "base": "23cd31e77a712a210c734e38488d7a34afd83a25",
        "env": "6e8ce43a4958dbaa56256365c2a89d8db92e07d6",
        "files": [
            "src/sqlfluff/core/templaters/slicers/tracer.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2849",
        "problem": "Lint and fix throws exception when having jinja for loop inside set\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nTo reproduce the error, create test.template.sql\r\n```\r\n{% set whitelisted= [\r\n    {'name': 'COL_1'},\r\n    {'name': 'COL_2'},\r\n    {'name': 'COL_3'}\r\n] %}\r\n\r\n{% set some_part_of_the_query %}\r\n    {% for col in whitelisted %}\r\n    {{col.name}}{{ \", \" if not loop.last }}\r\n    {% endfor %}\r\n{% endset %}\r\n\r\nSELECT {{some_part_of_the_query}}\r\nFROM SOME_TABLE\r\n\r\n```\r\n\r\nwhen running lint i get this error:\r\n```\r\n==== sqlfluff ====\r\nsqlfluff:               0.11.0 python:                 3.8.12\r\nimplementation:        cpython dialect:             snowflake\r\nverbosity:                   1 templater:               jinja\r\n\r\n==== readout ====\r\n\r\n=== [ path: test.template.sql ] ===\r\n\r\nWARNING    Unable to lint test.template.sql due to an internal error. Please report this as an issue with your query's contents and stacktrace below!\r\nTo hide this warning, add the failing file to .sqlfluffignore\r\nTraceback (most recent call last):\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/linter/runner.py\", line 103, in run\r\n    yield partial()\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 666, in lint_rendered\r\n    parsed = cls.parse_rendered(rendered)\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 352, in parse_rendered\r\n    tokens, lvs, config = cls._lex_templated_file(\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 139, in _lex_templated_file\r\n    tokens, lex_vs = lexer.lex(templated_file)\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/parser/lexer.py\", line 321, in lex\r\n    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/parser/lexer.py\", line 348, in elements_to_segments\r\n    source_slice = templated_file.templated_slice_to_source_slice(\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/templaters/base.py\", line 258, in templated_slice_to_source_slice\r\n    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/templaters/base.py\", line 177, in _find_slice_indices_of_templated_pos\r\n    raise ValueError(\"Position Not Found\")\r\nValueError: Position Not Found\r\n \r\n==== summary ====\r\nviolations:        0 status:         PASS\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n\r\n```\r\n\r\nThis is the rendered query:\r\n```\r\n SELECT\r\n\r\n    COL_1,\r\n\r\n    COL_2,\r\n\r\n    COL_3\r\n\r\n\r\nFROM SOME_TABLE\r\n\r\n```\r\n\r\nAnd when trying around to make this work i removed the new lines between the selected columns like this:\r\n```\r\n{% set whitelisted= [\r\n    {'name': 'COL_1'},\r\n    {'name': 'COL_2'},\r\n    {'name': 'COL_3'}\r\n] %}\r\n\r\n{% set some_part_of_the_query %}\r\n    {% for col in whitelisted -%}\r\n    {{col.name}}{{ \", \" if not loop.last }}\r\n    {% endfor -%}\r\n{% endset %}\r\n\r\nSELECT {{some_part_of_the_query}}\r\nFROM SOME_TABLE\r\n\r\n```\r\n\r\nwhich renders:\r\n```\r\nSELECT\r\n    COL_1,\r\n    COL_2,\r\n    COL_3\r\n\r\nFROM SOME_TABLE\r\n\r\n```\r\n\r\nAnd this will make the linter pass:\r\n\r\n```\r\n==== sqlfluff ====\r\nsqlfluff:               0.11.0 python:                 3.8.12\r\nimplementation:        cpython dialect:             snowflake\r\nverbosity:                   1 templater:               jinja\r\n\r\n==== readout ====\r\n\r\n=== [ path: test.template.sql ] ===\r\n\r\n== [test.template.sql] PASS                                                                                                                          \r\n==== summary ====\r\nviolations:        0 status:         PASS\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n\r\n```\r\n\r\n\n\n### Expected Behaviour\n\nMy expectations is that the linter and fix should pass.\n\n### Observed Behaviour\n\nRight now lint and fix throws exception (see \"What Happened\" section)\n\n### How to reproduce\n\nMentioned above.\n\n### Dialect\n\nsnowflake\n\n### Version\n\nsqlfluff, version 0.11.0\n\n### Configuration\n\n[sqlfluff]\r\nverbose = 1\r\ndialect = snowflake\r\ntemplater = jinja\r\nexclude_rules = L027,L031,L032,L036,L044,L046,L034,L050\r\noutput_line_length = 121\r\nsql_file_exts=.sql\r\n\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 250\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L042]\r\nforbid_subquery_in = both\r\n\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L030]  # function names\r\nextended_capitalisation_policy = upper\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "",
        "base": "0bbd70f38a3318b9a488d988d06e8005e222d6ac",
        "env": "3d52e8270d82aeccf4c516d059a80a6947919aea",
        "files": [
            "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py",
            "src/sqlfluff/core/templaters/base.py",
            "src/sqlfluff/core/templaters/slicers/tracer.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/884",
        "problem": "Whitespace token is_whitespace is False\nI expect segment.is_whitespace of a Whitespace token is True, however, it is set to False.\r\n\r\n## Expected Behaviour\r\nsegment.is_whitespace return True\r\n\r\n## Observed Behaviour\r\nsegment.is_whitespace return False\r\n## Steps to Reproduce\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\n## Configuration\r\n```\r\nInclude your SQLFluff configuration here\r\n```\r\n\n",
        "hint": "To triage this issue, I searched the SQLFluff code to find all uses of `is_whitespace`. This is the only one I found:\r\n```\r\nsrc/sqlfluff/core/parser/segments/base.py:72:    is_whitespace = False\r\n```\r\n\r\n@alanmcruickshank: What's the purpose of `is_whitespace`?\r\n\r\nI see that long ago (2019), there was a class `WhitespaceSegment` (also a `NewlineSegment`). Now it's not a class -- instead, it'd defined in `src/sqlfluff/core/rules/base.py`.\nOnce #866 is merged I'll pick up the rest of this which relates to some of the lexer objects.",
        "base": "c0bad78f3fa9549591738c77f869724f721e6830",
        "env": "cbdcfb09feb4883de91de142956c3be6ac7f827d",
        "files": [
            "src/sqlfluff/core/dialects/dialect_ansi.py",
            "src/sqlfluff/core/parser/lexer.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4151",
        "problem": "--disable_progress_bar Flag Broken for Fix\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nI ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n\n### Expected Behaviour\n\nShould run as expected, with no error and no progress bar.\n\n### Observed Behaviour\n\nExit code 2 and stderr:\r\n```\r\nUsage: sqlfluff fix [OPTIONS] [PATHS]...\r\n        Try 'sqlfluff fix -h' for help.\r\n\r\n        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\r\n```\n\n### How to reproduce\n\nSql file:\r\n```\r\nSELECT foo FROM bar;\r\n```\r\n\r\nCommand:\r\n```\r\nsqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\npython 3.10.3\r\nsqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n\n### Configuration\n\nNo special configuration. Ran hermetically with `trunk`.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n--disable_progress_bar Flag Broken for Fix\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nI ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n\n### Expected Behaviour\n\nShould run as expected, with no error and no progress bar.\n\n### Observed Behaviour\n\nExit code 2 and stderr:\r\n```\r\nUsage: sqlfluff fix [OPTIONS] [PATHS]...\r\n        Try 'sqlfluff fix -h' for help.\r\n\r\n        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\r\n```\n\n### How to reproduce\n\nSql file:\r\n```\r\nSELECT foo FROM bar;\r\n```\r\n\r\nCommand:\r\n```\r\nsqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\npython 3.10.3\r\nsqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n\n### Configuration\n\nNo special configuration. Ran hermetically with `trunk`.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "Looks like #3904 made `lint` work with both but updated `fix` to only accept `--disable-progress-bar`. I assume that was by accident. Should be relatively straightforward to fix by updating to match `lint`. \nLooks like #3904 made `lint` work with both but updated `fix` to only accept `--disable-progress-bar`. I assume that was by accident. Should be relatively straightforward to fix by updating to match `lint`. ",
        "base": "dc59c2a5672aacedaf91f0e6129b467eefad331b",
        "env": "dc59c2a5672aacedaf91f0e6129b467eefad331b",
        "files": [
            "src/sqlfluff/cli/commands.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3354",
        "problem": "TypeError when using integer placeholder\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nAn exception occurs when trying to use integer substituents.\r\n\r\n### Expected Behaviour\r\n\r\nWork without errors.\r\n\r\n### Observed Behaviour\r\n\r\n\r\nAn exception occurs:\r\n```\r\n  ...\r\n  File \"venv/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 816, in render_file\r\n    return self.render_string(raw_file, fname, config, encoding)\r\n  File \"venv/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 787, in render_string\r\n    templated_file, templater_violations = self.templater.process(\r\n  File \"venv/lib/python3.9/site-packages/sqlfluff/core/templaters/placeholder.py\", line 183, in process\r\n    start_template_pos, start_template_pos + len(replacement), None\r\nTypeError: object of type 'int' has no len()\r\n\r\n```\r\n\r\n### How to reproduce\r\n\r\n1. Create a file `example.sql`:\r\n```\r\nSELECT 1\r\nLIMIT %(capacity)s;\r\n```\r\n2. Copy `.sqlfluff` from the Configuration section\r\n3. Run `sqlfluff lint --dialect postgres example.sql`\r\n\r\n### Dialect\r\n\r\npostgres\r\n\r\n### Version\r\n\r\nsqlfluff, version 0.13.1\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\nexclude_rules = L031\r\ntemplater = placeholder\r\n\r\n[sqlfluff:templater:placeholder]\r\nparam_style = pyformat\r\ncapacity = 15\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [ ] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\nSupport Postgres-style variable substitution\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nThe Postgres `psql` utility supports flavor of colon-style variable substitution that currently confuses sqlfluff.  E.g.,\r\n\r\n```sql\r\nALTER TABLE name:variable RENAME TO name;\r\n```\r\n\r\nRunning the above through sqlfluff produces this output:\r\n\r\n```\r\nsqlfluff lint --dialect postgres 2.sql\r\n== [2.sql] FAIL\r\nL:   1 | P:   1 |  PRS | Line 1, Position 1: Found unparsable section: 'ALTER\r\n                       | TABLE name:variable RENAME TO name...'\r\n```\n\n### Use case\n\nI would like it if in the above the string \"name:variable\" were considered a valid table name (and other identifiers similarly).\n\n### Dialect\n\nThis applies to the Postgres dialect.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "\nThis sounds more like a templater feature than a dialect feature. Does psql allow variables to contain SQL fragments, e.g.: `WHERE foo = '3'`?\n> This sounds more like a templater feature than a dialect feature.\r\n\r\nTrue!  After looking over the code some, that may well be the right place to implement this.\r\n\r\n> Does psql allow variables to contain SQL fragments, e.g.: WHERE foo = '3'?\r\n\r\nYes.  E.g.,\r\n\r\n```\r\n% psql -v expression='2 + 2'\r\npsql (14.2, server 10.18)\r\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\r\nType \"help\" for help.\r\n\r\ndb=> select :expression;\r\n ?column?\r\n----------\r\n        4\r\n(1 row)\r\n\r\ndb=> select 5:expression;\r\n ?column?\r\n----------\r\n       54\r\n(1 row)\r\n```\r\n\r\nMore at the [docs](https://www.postgresql.org/docs/current/app-psql.html#APP-PSQL-VARIABLES).",
        "base": "36e89cbf2d13d5d95d2430f905a2fd122cf103c7",
        "env": "8f6fd1d8a8d69b2c463fbcf5bd1131c47f12ad88",
        "files": [
            "src/sqlfluff/core/templaters/placeholder.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3700",
        "problem": "L042 loop limit on fixes reached when CTE itself contains a subquery\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nWhile running `sqlfluff fix --dialect snowflake` on a sql file, I get \r\n```\r\n==== finding fixable violations ====\r\nWARNING    Loop limit on fixes reached [10].                                                                                                                                                              \r\n==== no fixable linting violations found ====                                                                                                                                                             \r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n  [22 unfixable linting violations found]\r\n```\r\n\r\n```\r\nINSERT OVERWRITE INTO dwh.test_table\r\n\r\nWITH cte1 AS (\r\n\tSELECT *\r\n\tFROM (SELECT\r\n\t\t*,\r\n\t\tROW_NUMBER() OVER (PARTITION BY r ORDER BY updated_at DESC) AS latest\r\n\t\tFROM mongo.temp\r\n\tWHERE latest = 1\r\n))\r\n\r\nSELECT * FROM cte1 WHERE 1=1;\r\n```\r\n\r\nAll of the 22  violations are a mix of L002, L003 and L004.\r\n\r\n### Expected Behaviour\r\n\r\n`sqlfluff` should be able to fix the violations\r\n\r\n### Observed Behaviour\r\n\r\nEven if I try to fix the violations manually, it still shows the same error.\r\n\r\n### How to reproduce\r\n\r\nI will try to generate a sql file that will be able to reproduce the issue\r\n\r\n### Dialect\r\n\r\nSnowflake\r\n\r\n### Version\r\n\r\n1.1.0\r\n\r\n### Configuration\r\n\r\n```\r\n# https://docs.sqlfluff.com/en/stable/rules.html\r\n\r\n[sqlfluff]\r\nexclude_rules = L029, L031, L034\r\n\r\n[sqlfluff:indentation]\r\nindented_joins = true\r\nindented_using_on = true\r\n\r\n[sqlfluff:rules:L002]\r\ntab_space_size = 4\r\n\r\n[sqlfluff:rules:L003]\r\nhanging_indents = true\r\nindent_unit = tab\r\ntab_space_size = 4\r\n\r\n[sqlfluff:rules:L004]\r\nindent_unit = tab\r\ntab_space_size = 4\r\n\r\n[sqlfluff:rules:L010]\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L011]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L012]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L016]\r\nignore_comment_clauses = true\r\nignore_comment_lines = true\r\nindent_unit = tab\r\ntab_space_size = 4\r\n\r\n[sqlfluff:rules:L019]\r\ncomma_style = trailing\r\n\r\n[sqlfluff:rules:L022]\r\ncomma_style = trailing\r\n\r\n[sqlfluff:rules:L028]\r\nsingle_table_references = unqualified\r\n\r\n[sqlfluff:rules:L030]\r\nextended_capitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L040]\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L042]\r\nforbid_subquery_in = both\r\n\r\n[sqlfluff:rules:L054]\r\ngroup_by_and_order_by_style = explicit\r\n\r\n[sqlfluff:rules:L063]\r\nextended_capitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L066]\r\nmin_alias_length = 3\r\nmax_alias_length = 15\r\n\r\n[sqlfluff:templater:jinja:context]\r\nparams = {\"DB\": \"DEMO\"}\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
        "hint": "Unfortunately there is not much we can do without the SQL that produces this error (ideally a minimal reproducible example SQL) so will need to close this issue if we don\u2019t get that.\nI have updated the issue with a sample query. The query is very vague but it reproduces the error. Let me know if it helps.\nLooks like this simpler example also produces it:\r\n\r\n```sql\r\nWITH cte1 AS (\r\n\tSELECT a\r\n\tFROM (SELECT a)\r\n)\r\n\r\nSELECT a FROM cte1\r\n```\r\n\r\nThis only has one linting failure:\r\n\r\n```\r\n$ sqlfluff lint test.sql --dialect snowflake                       \r\n== [test.sql] FAIL                                                                                                                                                            \r\nL:   3 | P:   7 | L042 | from_expression_element clauses should not contain\r\n                       | subqueries. Use CTEs instead\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n```\r\n\r\nSo basically L042 gets in a recursive loop when trying to fix CTEs that also break L042.\r\n\r\nFor now you can manually fix that (or exclude L042 for this query) to prevent the error.\nAnother good test query:\r\n```\r\nWITH cte1 AS (\r\n    SELECT *\r\n    FROM (SELECT * FROM mongo.temp)\r\n)\r\n\r\nSELECT * FROM cte1\r\n```\nPR #3697 avoids the looping behavior. Lint issues are still flagged, but the rule does not attempt to fix it _if_ it would cause a loop. We should still try and figure out why this is happening, so the rule can actually autofix the code, but that's lower priority (and probably a separate PR).",
        "base": "1000cf1beae75186cadf3a586c87e86e9f30ecb2",
        "env": "388dd01e05c7dcb880165c7241ed4027d9d0171e",
        "files": [
            "src/sqlfluff/core/parser/segments/base.py",
            "src/sqlfluff/rules/L028.py",
            "src/sqlfluff/rules/L042.py",
            "src/sqlfluff/utils/analysis/select_crawler.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3608",
        "problem": "Return codes are inconsistent\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nWorking on #3431 - I noticed that we're inconsistent in our return codes.\r\n\r\nIn `commands.py` we call `sys.exit()` in 15 places (currently).\r\n\r\n- Twice we call `sys.exit(0)` on success, at the end of `parse` and `lint` (`fix` is a handled differently, see below). \u2714\ufe0f \r\n- Six times we call `sys.exit(1)` for a selection of things:\r\n  - Not having `cProfiler` installed.\r\n  - Failing to apply fixes\r\n  - User Errors and OSError (in `PathAndUserErrorHandler`)\r\n- Five times we call `sys.exit(66)` for a selection of things:\r\n  - User Errors (including unknown dialect or failing to load a dialect or config)\r\n  - If parsing failed when calling `parse`.\r\n- Once we use `handle_files_with_tmp_or_prs_errors` to determine the exit code (which returns 1 or 0)\r\n- Once we use `LintingResult.stats` to determine the exit code (which returns either 65 or 0)\r\n- Once we do a mixture of the above (see end of `fix`)\r\n\r\nThis neither DRY, or consistent ... or helpful?\r\n\r\n### Expected Behaviour\r\n\r\nWe should have consistent return codes for specific scenarios. There are up for discussion, but I would suggest:\r\n\r\n- 0 for success (obviously)\r\n- 1 for a fail which is error related: not having libraries installed, user errors etc...\r\n- 65 for a linting fail (i.e. no errors in running, but issues were found in either parsing or linting).\r\n- 66 for a fixing fail (i.e. we tried to fix errors but failed to do so for some reason).\r\n\r\nThese would be defined as constants at the top of `commands.py`.\r\n\r\n### Observed Behaviour\r\n\r\nsee above\r\n\r\n### How to reproduce\r\n\r\nsee above\r\n\r\n### Dialect\r\n\r\nN/A\r\n\r\n### Version\r\n\r\nDescription is as per code in #3431\r\n\r\n### Configuration\r\n\r\n-\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
        "hint": "I'm happy to contribute the changes for this one, but would appreciate views on what the error codes we should align on first @barrywhart @tunetheweb \nI'm not familiar with any widespread conventions about exit codes, except to keep them below 256.\r\n\r\nThis Stack Overflow post has a lot of discussion, often self-contradictory. https://stackoverflow.com/questions/1101957/are-there-any-standard-exit-status-codes-in-linux\r\n\r\nOverall, your proposal sounds good to me.\r\n\r\nCan you also search the existing issues for any mention of exit codes? I think there may be one or two open issues, perhaps related to the behavior when \"fix\" finds issues but some are unfixable. Because of its multifaceted nature as a linter and fixer that is used both interactively (e.g. during pre-commit) and in batch (CICD), SQLFluff perhaps has more stringent requirements for precise exit codes than some other tools. Do you think it'd be useful to review existing (and or write some new) user documentation before starting the coding, to help get a better understanding of the various use cases?\r\n\r\n\r\n\nAgree with @barrywhart 's comments.\r\n\r\nOnly question is why 65/66 instead of just 2/3?\n> Only question is why 65/66 instead of just 2/3?\r\n\r\nThis was initially because I had read that codes 0-64 were reserved for system usage but it appears things aren't that consistent.\r\n\r\n> This Stack Overflow post has a lot of discussion, often self-contradictory...\r\n\r\nI'm wondering based on this post whether we should simplify things:\r\n- 0: success\r\n- 1: fail (on linting or fixing, but due to finding issues with code or unable to fix, no \"errors\")\r\n- 2: fail because misuse or error\r\n\r\nIt's slightly less granular but a little more consistent with the bash approach (from the most recent post on that SO question):\r\n\r\n> Exit status 0: success\r\n> Exit status 1: \"failure\", as defined by the program\r\n> Exit status 2: command line usage error\nCleaning up the exit codes seems sensible. How likely do we think it is to break things for users?\nRelatively unlikely I reckon - I'm not sure the existing codes are sufficiently granular to be useful right now.",
        "base": "d783e421b714ed989d9e641977ea9b3b6ffaf807",
        "env": "d83ab36bbb21f62cf0780d095a8be8cd366735d7",
        "files": [
            "src/sqlfluff/cli/__init__.py",
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/cli/formatters.py",
            "src/sqlfluff/core/linter/linting_result.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3435",
        "problem": "L027: outer-level table not found in WHERE clause sub-select\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nOuter-level table/view referenced in sub-select inside `WHERE` clause is not being detected.\r\n\r\nThis error seems to only occur when the sub-select contains joins.\r\n\r\n### Expected Behaviour\r\n\r\nNo error\r\n\r\n### Observed Behaviour\r\n\r\n```\r\nL:   7 | P:  32 | L027 | Qualified reference 'my_table.kind' not found in\r\n                       | available tables/view aliases ['other_table',\r\n                       | 'mapping_table'] in select with more than one referenced\r\n                       | table/view.\r\n```\r\n\r\n### How to reproduce\r\n\r\n```sql\r\nSELECT my_col\r\nFROM my_table\r\nWHERE EXISTS (\r\n    SELECT 1\r\n    FROM other_table\r\n    INNER JOIN mapping_table ON (mapping_table.other_fk = other_table.id_pk)\r\n    WHERE mapping_table.kind = my_table.kind\r\n);\r\n```\r\n\r\n### Dialect\r\n\r\npostgres\r\n\r\n### Version\r\n\r\nsqlfluff, version 0.12.0\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\nnocolor = True\r\ndialect = postgres\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [ ] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
        "hint": "Does L027 use `SelectCrawler`? This sounds like an issue where it may be helpful.\r\n\r\nRelated: Rules that use `SelectCrawler` may be good candidates to benefit from setting `recurse_into` to `False`. (Setting the flag is just a start. This also requires reworking the rule code, hopefully no more than 1-2 hours of work.)\nAnswering my own question: It does not seem to use `SelectCrawler`. Rules that currently use it:\r\n* L025\r\n* L026\r\n* L044\r\n* L045\r\n\r\nFrom a quick look at the YML test files for each of these rules, I suggest L044 would be the best one to review in terms of handling similar requirements. Look for test cases that mention \"subquery\".\nI think a very similar fix to that implemented in this [PR for L028](https://github.com/sqlfluff/sqlfluff/pull/3156) will also work here. In particular, notice the code that looks at `query.parent` to find tables that are \"visible\" to a particular query.\r\n\r\nhttps://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/rules/L028.py#L108L114\nRelated to #3380, possibly duplicate",
        "base": "8e724ef8906eecce7179f4b7c52d4fc0672e4bd9",
        "env": "6e8ce43a4958dbaa56256365c2a89d8db92e07d6",
        "files": [
            "src/sqlfluff/rules/L027.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3904",
        "problem": "Standardise `--disable_progress_bar` naming\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nAs noted in https://github.com/sqlfluff/sqlfluff/pull/3610#discussion_r926014745 `--disable_progress_bar` is the only command line option using underscores instead of dashes.\r\n\r\nShould we change this?\r\n\r\nThis would be a breaking change, so do we leave until next major release?\r\nOr do we accept both options?\n\n### Expected Behaviour\n\nWe should be standard in out command line option format\n\n### Observed Behaviour\n\n`--disable_progress_bar` is the only non-standard one\n\n### How to reproduce\n\nN/A\n\n### Dialect\n\nN/A\n\n### Version\n\n1.2.1\n\n### Configuration\n\nN/A\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "I like the idea (of at least as a transition) or accepting both, but then defaulting to the one consistent with the other options.\nSo this is about changing option `--disable_progress_bar` to `--disable-progress-bar`, right? I think I can take care of that, it was me who introduced it here :)\r\n\r\nAdditionally I would make an attempt to have these two options available, but to nicely inform users that one with underscores is deprecated. What do you think @tunetheweb? \r\n\r\nI see I cannot assign myself to that Issue.\r\n\n> So this is about changing option `--disable_progress_bar` to `--disable-progress-bar`, right? I think I can take care of that, it was me who introduced it here :)\r\n\r\nCorrect and thanks for taking on\r\n\r\n> Additionally I would make an attempt to have these two options available, but to nicely inform users that one with underscores is deprecated. What do you think @tunetheweb?\r\n\r\n@alanmcruickshank added some functionality that might help in #3874 but not sure if that applies to command lines options too (I\u2019m having less time to work on SQLFluff lately so not following it as closely as I used to). If not maybe it should?\r\n\r\n> I see I cannot assign myself to that Issue.\r\n\r\nYeah only maintainers can assign, which is a bit of an annoying restriction of GitHub so we tend not to use that field and commenting (like you\u2019ve done here) is sufficient to claim an issue. Please comment again if to \u201cunassign\u201d yourself if it turns out you won\u2019t be able to work on it after all. Though lack of progress is a usually good indicator of that anyway \ud83d\ude04",
        "base": "2ac8e125604aa1f19d9811f7dc5bd56eefa654ac",
        "env": "dc59c2a5672aacedaf91f0e6129b467eefad331b",
        "files": [
            "src/sqlfluff/cli/click_deprecated_option.py",
            "src/sqlfluff/cli/commands.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4753",
        "problem": "layout.end-of-file is the only rule in kebab case\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nOur rules are all in `snake_case`, except for `layout.end-of-file`\n\n### Expected Behaviour\n\nAll rules should be in snake case\n\n### Observed Behaviour\n\nAs above\n\n### How to reproduce\n\n-\n\n### Dialect\n\nNA\n\n### Version\n\nMain\n\n### Configuration\n\nNA\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "",
        "base": "24178a589c279220c6605324c446122d15ebc3fb",
        "env": "d19de0ecd16d298f9e3bfb91da122734c40c01e5",
        "files": [
            "docs/generate-rule-docs.py",
            "src/sqlfluff/rules/layout/LT12.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4778",
        "problem": "2.0.2 - LT02 issues when query contains \"do\" statement.\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nSQLFluff v2.0.2 gives LT02 indentation errors for the Jinja `if`-block when `template_blocks_indent` is set to `True`.\r\nThe example SQL below is a bit contrived, but it's the smallest failing example I could produce based on our real SQL.\r\n\r\nIf I remove the Jinja `do`-expression from the code, the `if` block validates without errors.\r\n\r\n### Expected Behaviour\r\n\r\nI expect the SQL to pass the linting tests.\r\n\r\n### Observed Behaviour\r\n\r\nOutput from SQLFluff v2.0.2:\r\n```\r\nL:   5 | P:   1 | LT02 | Line should not be indented.\r\n                       | [layout.indent]\r\nL:   6 | P:   1 | LT02 | Line should not be indented.\r\n                       | [layout.indent]\r\n```\r\n\r\n### How to reproduce\r\n\r\nSQL to reproduce:\r\n```\r\n{% set cols = ['a', 'b'] %}\r\n{% do cols.remove('a') %}\r\n\r\n{% if true %}\r\n    select a\r\n    from some_table\r\n{% endif %}\r\n```\r\n\r\n### Dialect\r\n\r\n`ansi`\r\n\r\n### Version\r\n\r\n```\r\n> sqlfluff --version\r\nsqlfluff, version 2.0.2\r\n\r\n> python --version\r\nPython 3.9.9\r\n```\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\ndialect = ansi\r\ntemplater = jinja\r\n\r\n[sqlfluff:indentation]\r\ntemplate_blocks_indent = True\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n2.0.2 - LT02 issues when query contains \"do\" statement.\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nSQLFluff v2.0.2 gives LT02 indentation errors for the Jinja `if`-block when `template_blocks_indent` is set to `True`.\r\nThe example SQL below is a bit contrived, but it's the smallest failing example I could produce based on our real SQL.\r\n\r\nIf I remove the Jinja `do`-expression from the code, the `if` block validates without errors.\r\n\r\n### Expected Behaviour\r\n\r\nI expect the SQL to pass the linting tests.\r\n\r\n### Observed Behaviour\r\n\r\nOutput from SQLFluff v2.0.2:\r\n```\r\nL:   5 | P:   1 | LT02 | Line should not be indented.\r\n                       | [layout.indent]\r\nL:   6 | P:   1 | LT02 | Line should not be indented.\r\n                       | [layout.indent]\r\n```\r\n\r\n### How to reproduce\r\n\r\nSQL to reproduce:\r\n```\r\n{% set cols = ['a', 'b'] %}\r\n{% do cols.remove('a') %}\r\n\r\n{% if true %}\r\n    select a\r\n    from some_table\r\n{% endif %}\r\n```\r\n\r\n### Dialect\r\n\r\n`ansi`\r\n\r\n### Version\r\n\r\n```\r\n> sqlfluff --version\r\nsqlfluff, version 2.0.2\r\n\r\n> python --version\r\nPython 3.9.9\r\n```\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\ndialect = ansi\r\ntemplater = jinja\r\n\r\n[sqlfluff:indentation]\r\ntemplate_blocks_indent = True\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
        "hint": "I think think this is almost certainly about the `do` statement, hopefully this should be very solvable.\nAny pointers on where I should start looking if I would work on a fix @alanmcruickshank?\n@fredriv - great question. I just had a quick look and this is a very strange bug, but hopefully one with a satisfying solution.\r\n\r\nIf I run `sqlfluff parse` on the file I get this:\r\n\r\n```\r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    [META] placeholder:                                       [Type: 'templated', Raw: \"{% set cols = ['a', 'b'] %}\"]\r\n[L:  1, P: 28]      |    newline:                                                  '\\n'\r\n[L:  2, P:  1]      |    [META] placeholder:                                       [Type: 'block_start', Raw: \"{% do cols.remove('a') %}\", Block: '230a18']\r\n[L:  2, P: 26]      |    newline:                                                  '\\n'\r\n[L:  3, P:  1]      |    newline:                                                  '\\n'\r\n[L:  4, P:  1]      |    [META] placeholder:                                       [Type: 'block_start', Raw: '{% if true %}', Block: 'e33036']\r\n[L:  4, P: 14]      |    newline:                                                  '\\n'\r\n[L:  5, P:  1]      |    whitespace:                                               '    '\r\n[L:  5, P:  5]      |    statement:\r\n[L:  5, P:  5]      |        select_statement:\r\n[L:  5, P:  5]      |            select_clause:\r\n[L:  5, P:  5]      |                keyword:                                      'select'\r\n[L:  5, P: 11]      |                [META] indent:\r\n[L:  5, P: 11]      |                whitespace:                                   ' '\r\n[L:  5, P: 12]      |                select_clause_element:\r\n[L:  5, P: 12]      |                    column_reference:\r\n[L:  5, P: 12]      |                        naked_identifier:                     'a'\r\n[L:  5, P: 13]      |            newline:                                          '\\n'\r\n[L:  6, P:  1]      |            whitespace:                                       '    '\r\n[L:  6, P:  5]      |            [META] dedent:\r\n[L:  6, P:  5]      |            from_clause:\r\n[L:  6, P:  5]      |                keyword:                                      'from'\r\n[L:  6, P:  9]      |                whitespace:                                   ' '\r\n[L:  6, P: 10]      |                from_expression:\r\n[L:  6, P: 10]      |                    [META] indent:\r\n[L:  6, P: 10]      |                    from_expression_element:\r\n[L:  6, P: 10]      |                        table_expression:\r\n[L:  6, P: 10]      |                            table_reference:\r\n[L:  6, P: 10]      |                                naked_identifier:             'some_table'\r\n[L:  6, P: 20]      |                    [META] dedent:\r\n[L:  6, P: 20]      |    newline:                                                  '\\n'\r\n[L:  7, P:  1]      |    [META] placeholder:                                       [Type: 'block_end', Raw: '{% endif %}', Block: 'e33036']\r\n[L:  7, P: 12]      |    [META] end_of_file:\r\n```\r\n\r\nNote the difference between that and the output when I remove the `do` line:\r\n\r\n```\r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    [META] placeholder:                                       [Type: 'templated', Raw: \"{% set cols = ['a', 'b'] %}\"]\r\n[L:  1, P: 28]      |    newline:                                                  '\\n'\r\n[L:  2, P:  1]      |    newline:                                                  '\\n'\r\n[L:  3, P:  1]      |    newline:                                                  '\\n'\r\n[L:  4, P:  1]      |    [META] placeholder:                                       [Type: 'block_start', Raw: '{% if true %}', Block: '0d1e98']\r\n[L:  4, P: 14]      |    [META] indent:                                            [Block: '0d1e98']\r\n[L:  4, P: 14]      |    newline:                                                  '\\n'\r\n[L:  5, P:  1]      |    whitespace:                                               '    '\r\n[L:  5, P:  5]      |    statement:\r\n[L:  5, P:  5]      |        select_statement:\r\n[L:  5, P:  5]      |            select_clause:\r\n[L:  5, P:  5]      |                keyword:                                      'select'\r\n[L:  5, P: 11]      |                [META] indent:\r\n[L:  5, P: 11]      |                whitespace:                                   ' '\r\n[L:  5, P: 12]      |                select_clause_element:\r\n[L:  5, P: 12]      |                    column_reference:\r\n[L:  5, P: 12]      |                        naked_identifier:                     'a'\r\n[L:  5, P: 13]      |            newline:                                          '\\n'\r\n[L:  6, P:  1]      |            whitespace:                                       '    '\r\n[L:  6, P:  5]      |            [META] dedent:\r\n[L:  6, P:  5]      |            from_clause:\r\n[L:  6, P:  5]      |                keyword:                                      'from'\r\n[L:  6, P:  9]      |                whitespace:                                   ' '\r\n[L:  6, P: 10]      |                from_expression:\r\n[L:  6, P: 10]      |                    [META] indent:\r\n[L:  6, P: 10]      |                    from_expression_element:\r\n[L:  6, P: 10]      |                        table_expression:\r\n[L:  6, P: 10]      |                            table_reference:\r\n[L:  6, P: 10]      |                                naked_identifier:             'some_table'\r\n[L:  6, P: 20]      |                    [META] dedent:\r\n[L:  6, P: 20]      |    newline:                                                  '\\n'\r\n[L:  7, P:  1]      |    [META] dedent:                                            [Block: '0d1e98']\r\n[L:  7, P:  1]      |    [META] placeholder:                                       [Type: 'block_end', Raw: '{% endif %}', Block: '0d1e98']\r\n[L:  7, P: 12]      |    [META] end_of_file:\r\n```\r\n\r\nSee that in the latter example there are `indent` and `dedent` tokens around the `if` clause, but not in the first example. Something about the `do` call is disrupting the positioning of those indent tokens. Those tokens are inserted during `._iter_segments()` in `lexer.py`, and more specifically in `._handle_zero_length_slice()`. That's probably where you'll find the issue. My guess is that something about the `do` block is throwing off the block tracking?\nThanks! I'll see if I can have a look at it tonight.\r\n\r\nCould it have something to do with the `do` block not having a corresponding `block_end`? \ud83e\udd14\nSo perhaps it should be `templated` instead of `block_start`, similar to the `set` above it?\nIf I add `do` to the list of tag names in `extract_block_type` at https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L527 it regards it as a `templated` element instead of `block_start`, and the indent is added where I want it.\r\n\r\nE.g.\r\n```\r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    [META] placeholder:                                       [Type: 'templated', Raw: \"{% set cols = ['a', 'b'] %}\"]\r\n[L:  1, P: 28]      |    newline:                                                  '\\n'\r\n[L:  2, P:  1]      |    [META] placeholder:                                       [Type: 'templated', Raw: \"{% do cols.remove('a') %}\"]\r\n[L:  2, P: 26]      |    newline:                                                  '\\n'\r\n[L:  3, P:  1]      |    newline:                                                  '\\n'\r\n[L:  4, P:  1]      |    [META] placeholder:                                       [Type: 'block_start', Raw: '{% if true %}', Block: '3e39bd']\r\n[L:  4, P: 14]      |    [META] indent:                                            [Block: '3e39bd']\r\n[L:  4, P: 14]      |    newline:                                                  '\\n'\r\n[L:  5, P:  1]      |    whitespace:                                               '    '\r\n[L:  5, P:  5]      |    statement:\r\n[L:  5, P:  5]      |        select_statement:\r\n[L:  5, P:  5]      |            select_clause:\r\n[L:  5, P:  5]      |                keyword:                                      'select'\r\n[L:  5, P: 11]      |                [META] indent:\r\n[L:  5, P: 11]      |                whitespace:                                   ' '\r\n[L:  5, P: 12]      |                select_clause_element:\r\n[L:  5, P: 12]      |                    column_reference:\r\n[L:  5, P: 12]      |                        naked_identifier:                     'a'\r\n[L:  5, P: 13]      |            newline:                                          '\\n'\r\n[L:  6, P:  1]      |            whitespace:                                       '    '\r\n[L:  6, P:  5]      |            [META] dedent:\r\n[L:  6, P:  5]      |            from_clause:\r\n[L:  6, P:  5]      |                keyword:                                      'from'\r\n[L:  6, P:  9]      |                whitespace:                                   ' '\r\n[L:  6, P: 10]      |                from_expression:\r\n[L:  6, P: 10]      |                    [META] indent:\r\n[L:  6, P: 10]      |                    from_expression_element:\r\n[L:  6, P: 10]      |                        table_expression:\r\n[L:  6, P: 10]      |                            table_reference:\r\n[L:  6, P: 10]      |                                naked_identifier:             'some_table'\r\n[L:  6, P: 20]      |                    [META] dedent:\r\n[L:  6, P: 20]      |    newline:                                                  '\\n'\r\n[L:  7, P:  1]      |    [META] dedent:                                            [Block: '3e39bd']\r\n[L:  7, P:  1]      |    [META] placeholder:                                       [Type: 'block_end', Raw: '{% endif %}', Block: '3e39bd']\r\n[L:  7, P: 12]      |    newline:                                                  '\\n'\r\n[L:  8, P:  1]      |    [META] end_of_file:\r\n```\nSimilarly if I instead add `do` to the list of trimmed parts in `update_inside_set_call_macro_or_block` at https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L252-L255\r\n\r\nMaybe a better place to put it? What do you think @alanmcruickshank?\n@fredriv - based on the [docs for jinja](https://jinja.palletsprojects.com/en/3.0.x/extensions/#expression-statement) it looks like we should never get a \"do block\" (i.e. `{% do ... %} ... {% enddo %}`, it's only ever just `{% do ... %}`). That means that treating it like a `templated` section is the right route, i.e. we should add it in `extract_block_type` and not in `update_inside_set_call_macro_or_block`.\r\n\r\nThanks for your research - I think this should be a neat solution! \ud83d\ude80 \n\ud83d\udc4d Ok, I can make a PR :)\nI think think this is almost certainly about the `do` statement, hopefully this should be very solvable.\nAny pointers on where I should start looking if I would work on a fix @alanmcruickshank?\n@fredriv - great question. I just had a quick look and this is a very strange bug, but hopefully one with a satisfying solution.\r\n\r\nIf I run `sqlfluff parse` on the file I get this:\r\n\r\n```\r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    [META] placeholder:                                       [Type: 'templated', Raw: \"{% set cols = ['a', 'b'] %}\"]\r\n[L:  1, P: 28]      |    newline:                                                  '\\n'\r\n[L:  2, P:  1]      |    [META] placeholder:                                       [Type: 'block_start', Raw: \"{% do cols.remove('a') %}\", Block: '230a18']\r\n[L:  2, P: 26]      |    newline:                                                  '\\n'\r\n[L:  3, P:  1]      |    newline:                                                  '\\n'\r\n[L:  4, P:  1]      |    [META] placeholder:                                       [Type: 'block_start', Raw: '{% if true %}', Block: 'e33036']\r\n[L:  4, P: 14]      |    newline:                                                  '\\n'\r\n[L:  5, P:  1]      |    whitespace:                                               '    '\r\n[L:  5, P:  5]      |    statement:\r\n[L:  5, P:  5]      |        select_statement:\r\n[L:  5, P:  5]      |            select_clause:\r\n[L:  5, P:  5]      |                keyword:                                      'select'\r\n[L:  5, P: 11]      |                [META] indent:\r\n[L:  5, P: 11]      |                whitespace:                                   ' '\r\n[L:  5, P: 12]      |                select_clause_element:\r\n[L:  5, P: 12]      |                    column_reference:\r\n[L:  5, P: 12]      |                        naked_identifier:                     'a'\r\n[L:  5, P: 13]      |            newline:                                          '\\n'\r\n[L:  6, P:  1]      |            whitespace:                                       '    '\r\n[L:  6, P:  5]      |            [META] dedent:\r\n[L:  6, P:  5]      |            from_clause:\r\n[L:  6, P:  5]      |                keyword:                                      'from'\r\n[L:  6, P:  9]      |                whitespace:                                   ' '\r\n[L:  6, P: 10]      |                from_expression:\r\n[L:  6, P: 10]      |                    [META] indent:\r\n[L:  6, P: 10]      |                    from_expression_element:\r\n[L:  6, P: 10]      |                        table_expression:\r\n[L:  6, P: 10]      |                            table_reference:\r\n[L:  6, P: 10]      |                                naked_identifier:             'some_table'\r\n[L:  6, P: 20]      |                    [META] dedent:\r\n[L:  6, P: 20]      |    newline:                                                  '\\n'\r\n[L:  7, P:  1]      |    [META] placeholder:                                       [Type: 'block_end', Raw: '{% endif %}', Block: 'e33036']\r\n[L:  7, P: 12]      |    [META] end_of_file:\r\n```\r\n\r\nNote the difference between that and the output when I remove the `do` line:\r\n\r\n```\r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    [META] placeholder:                                       [Type: 'templated', Raw: \"{% set cols = ['a', 'b'] %}\"]\r\n[L:  1, P: 28]      |    newline:                                                  '\\n'\r\n[L:  2, P:  1]      |    newline:                                                  '\\n'\r\n[L:  3, P:  1]      |    newline:                                                  '\\n'\r\n[L:  4, P:  1]      |    [META] placeholder:                                       [Type: 'block_start', Raw: '{% if true %}', Block: '0d1e98']\r\n[L:  4, P: 14]      |    [META] indent:                                            [Block: '0d1e98']\r\n[L:  4, P: 14]      |    newline:                                                  '\\n'\r\n[L:  5, P:  1]      |    whitespace:                                               '    '\r\n[L:  5, P:  5]      |    statement:\r\n[L:  5, P:  5]      |        select_statement:\r\n[L:  5, P:  5]      |            select_clause:\r\n[L:  5, P:  5]      |                keyword:                                      'select'\r\n[L:  5, P: 11]      |                [META] indent:\r\n[L:  5, P: 11]      |                whitespace:                                   ' '\r\n[L:  5, P: 12]      |                select_clause_element:\r\n[L:  5, P: 12]      |                    column_reference:\r\n[L:  5, P: 12]      |                        naked_identifier:                     'a'\r\n[L:  5, P: 13]      |            newline:                                          '\\n'\r\n[L:  6, P:  1]      |            whitespace:                                       '    '\r\n[L:  6, P:  5]      |            [META] dedent:\r\n[L:  6, P:  5]      |            from_clause:\r\n[L:  6, P:  5]      |                keyword:                                      'from'\r\n[L:  6, P:  9]      |                whitespace:                                   ' '\r\n[L:  6, P: 10]      |                from_expression:\r\n[L:  6, P: 10]      |                    [META] indent:\r\n[L:  6, P: 10]      |                    from_expression_element:\r\n[L:  6, P: 10]      |                        table_expression:\r\n[L:  6, P: 10]      |                            table_reference:\r\n[L:  6, P: 10]      |                                naked_identifier:             'some_table'\r\n[L:  6, P: 20]      |                    [META] dedent:\r\n[L:  6, P: 20]      |    newline:                                                  '\\n'\r\n[L:  7, P:  1]      |    [META] dedent:                                            [Block: '0d1e98']\r\n[L:  7, P:  1]      |    [META] placeholder:                                       [Type: 'block_end', Raw: '{% endif %}', Block: '0d1e98']\r\n[L:  7, P: 12]      |    [META] end_of_file:\r\n```\r\n\r\nSee that in the latter example there are `indent` and `dedent` tokens around the `if` clause, but not in the first example. Something about the `do` call is disrupting the positioning of those indent tokens. Those tokens are inserted during `._iter_segments()` in `lexer.py`, and more specifically in `._handle_zero_length_slice()`. That's probably where you'll find the issue. My guess is that something about the `do` block is throwing off the block tracking?\nThanks! I'll see if I can have a look at it tonight.\r\n\r\nCould it have something to do with the `do` block not having a corresponding `block_end`? \ud83e\udd14\nSo perhaps it should be `templated` instead of `block_start`, similar to the `set` above it?\nIf I add `do` to the list of tag names in `extract_block_type` at https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L527 it regards it as a `templated` element instead of `block_start`, and the indent is added where I want it.\r\n\r\nE.g.\r\n```\r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    [META] placeholder:                                       [Type: 'templated', Raw: \"{% set cols = ['a', 'b'] %}\"]\r\n[L:  1, P: 28]      |    newline:                                                  '\\n'\r\n[L:  2, P:  1]      |    [META] placeholder:                                       [Type: 'templated', Raw: \"{% do cols.remove('a') %}\"]\r\n[L:  2, P: 26]      |    newline:                                                  '\\n'\r\n[L:  3, P:  1]      |    newline:                                                  '\\n'\r\n[L:  4, P:  1]      |    [META] placeholder:                                       [Type: 'block_start', Raw: '{% if true %}', Block: '3e39bd']\r\n[L:  4, P: 14]      |    [META] indent:                                            [Block: '3e39bd']\r\n[L:  4, P: 14]      |    newline:                                                  '\\n'\r\n[L:  5, P:  1]      |    whitespace:                                               '    '\r\n[L:  5, P:  5]      |    statement:\r\n[L:  5, P:  5]      |        select_statement:\r\n[L:  5, P:  5]      |            select_clause:\r\n[L:  5, P:  5]      |                keyword:                                      'select'\r\n[L:  5, P: 11]      |                [META] indent:\r\n[L:  5, P: 11]      |                whitespace:                                   ' '\r\n[L:  5, P: 12]      |                select_clause_element:\r\n[L:  5, P: 12]      |                    column_reference:\r\n[L:  5, P: 12]      |                        naked_identifier:                     'a'\r\n[L:  5, P: 13]      |            newline:                                          '\\n'\r\n[L:  6, P:  1]      |            whitespace:                                       '    '\r\n[L:  6, P:  5]      |            [META] dedent:\r\n[L:  6, P:  5]      |            from_clause:\r\n[L:  6, P:  5]      |                keyword:                                      'from'\r\n[L:  6, P:  9]      |                whitespace:                                   ' '\r\n[L:  6, P: 10]      |                from_expression:\r\n[L:  6, P: 10]      |                    [META] indent:\r\n[L:  6, P: 10]      |                    from_expression_element:\r\n[L:  6, P: 10]      |                        table_expression:\r\n[L:  6, P: 10]      |                            table_reference:\r\n[L:  6, P: 10]      |                                naked_identifier:             'some_table'\r\n[L:  6, P: 20]      |                    [META] dedent:\r\n[L:  6, P: 20]      |    newline:                                                  '\\n'\r\n[L:  7, P:  1]      |    [META] dedent:                                            [Block: '3e39bd']\r\n[L:  7, P:  1]      |    [META] placeholder:                                       [Type: 'block_end', Raw: '{% endif %}', Block: '3e39bd']\r\n[L:  7, P: 12]      |    newline:                                                  '\\n'\r\n[L:  8, P:  1]      |    [META] end_of_file:\r\n```\nSimilarly if I instead add `do` to the list of trimmed parts in `update_inside_set_call_macro_or_block` at https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L252-L255\r\n\r\nMaybe a better place to put it? What do you think @alanmcruickshank?\n@fredriv - based on the [docs for jinja](https://jinja.palletsprojects.com/en/3.0.x/extensions/#expression-statement) it looks like we should never get a \"do block\" (i.e. `{% do ... %} ... {% enddo %}`, it's only ever just `{% do ... %}`). That means that treating it like a `templated` section is the right route, i.e. we should add it in `extract_block_type` and not in `update_inside_set_call_macro_or_block`.\r\n\r\nThanks for your research - I think this should be a neat solution! \ud83d\ude80 \n\ud83d\udc4d Ok, I can make a PR :)",
        "base": "e3f77d58f56149f9c8db3b790ef263b9853a9cb5",
        "env": "d19de0ecd16d298f9e3bfb91da122734c40c01e5",
        "files": [
            "src/sqlfluff/core/linter/linter.py",
            "src/sqlfluff/core/templaters/slicers/tracer.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2907",
        "problem": "sqlfluff doesn't recognise a jinja variable set inside of \"if\" statement\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen I try to define a jinja variable using \"set\" jinja directive inside of an \"if\" jinja statement, sqlfluff complains: \r\n\"Undefined jinja template variable\".\n\n### Expected Behaviour\n\nto not have a linting issue\n\n### Observed Behaviour\n\nsqlfluff lint gives an error:\r\n\"Undefined jinja template variable\"\n\n### How to reproduce\n\ntry to create a \"temp.sql\" file with the following content\r\n\r\n```\r\n{% if True %}\r\n    {% set some_var %}1{% endset %}\r\n    SELECT {{some_var}}\r\n{% endif %}\r\n```\r\n\r\nand run:\r\n```\r\nsqlfluff lint ./temp.sql\r\n```\r\n\r\nYou will get the following error:\r\n```\r\n== [./temp.sql] FAIL                                                                                                                    \r\nL:   2 | P:  12 |  TMP | Undefined jinja template variable: 'some_var'\r\nL:   3 | P:  14 |  TMP | Undefined jinja template variable: 'some_var'\r\n```\n\n### Dialect\n\ntested on 'snowflake' dialect\n\n### Version\n\nsqlfluff, version 0.11.1\r\nPython 3.8.12\n\n### Configuration\n\n[sqlfluff]\r\nverbose = 1\r\ndialect = snowflake\r\ntemplater = jinja\r\nexclude_rules = L027,L031,L032,L036,L044,L046,L034,L050\r\noutput_line_length = 121\r\nsql_file_exts=.sql\r\n\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 250\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L042]\r\nforbid_subquery_in = both\r\n\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L030]  # function names\r\nextended_capitalisation_policy = upper\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "Does Jinja support this? I don't see how this could be a SQLFluff issue.\nYour example does work on this website. I wonder if there's a Jinja runtime setting that affects whether this works.\r\n\r\nhttp://jinja.quantprogramming.com/\nIt also works with `j2cli` on my local machine. Seems like this _has_ to be a Jinja runtime setting...\r\n\r\nhttps://github.com/kolypto/j2cli\nIt was added in Jinja 2.8: https://jinja.palletsprojects.com/en/3.0.x/templates/#block-assignments\r\n\r\nNot sure what version we pull in depending on our other dependencies?\r\n\nI'm digging into this more. SQLFluff contains some additional code that attempts to detect undeclared Jinja variables and provide better error handling. The \"issue\" is being detected and reported by that code, not by Jinja itself. So we should be able to fix this. Need to do this carefully so we don't break error reporting for real errors. \nI think I have a fix. Just need to make the undefined variable check more sophisticated.",
        "base": "305159ea643baf6b4744b98c3566613754b2f659",
        "env": "3d52e8270d82aeccf4c516d059a80a6947919aea",
        "files": [
            "src/sqlfluff/core/templaters/jinja.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4051",
        "problem": "Misleading path does not exist message\nIt looks like if _at least one_ of the paths provided to sqlfluff do not exist, it will display an error message implying that _all_ of the supplied paths do not exist:\r\n\r\n```bash\r\ndbt@b54bee9ced88:/workspaces/dbt-dutchie$ sqlfluff fix models/shared/dispensaries.sql models/shares/dispensary_chains.sql\r\n==== finding fixable violations ====\r\n=== [dbt templater] Compiling dbt project...\r\n== [models/shared/dispensaries.sql] FAIL\r\nL:   6 | P:   2 | L003 | Indentation not consistent with line #376\r\nL:   8 | P:   2 | L003 | Indentation not consistent with line #376\r\nL:   9 | P:   3 | L003 | Line over-indented compared to line #376\r\nL:  10 | P:   2 | L003 | Indentation not consistent with line #376\r\nL:  12 | P:   2 | L003 | Indentation not consistent with line #376\r\nL:  13 | P:   3 | L003 | Line over-indented compared to line #376\r\nL:  14 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  15 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  16 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  17 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  18 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  19 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  20 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  21 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  22 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  23 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  24 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  25 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  26 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  27 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  28 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  29 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  30 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  31 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  32 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  33 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  34 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  58 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL:  35 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  36 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  37 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  38 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  39 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  40 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  41 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  42 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  43 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  44 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  45 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  46 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  47 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  48 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  49 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  50 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  51 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  52 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  53 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  54 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  55 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  56 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  57 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  58 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  59 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  60 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  61 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  62 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  63 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  64 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  65 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  66 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  67 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  68 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  69 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  70 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  71 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  72 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  73 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  74 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  75 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  76 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  77 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  78 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  79 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  80 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  81 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  82 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  83 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  84 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  85 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  86 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  87 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  88 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  89 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  90 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  91 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  92 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  92 | P:  44 | L001 | Unnecessary trailing whitespace.\r\nL:  93 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  94 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  95 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  96 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  97 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  98 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  99 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 100 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 101 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 102 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 103 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 104 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 105 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 106 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 107 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 108 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 109 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 110 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 111 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 112 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 113 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 114 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 115 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 116 | P:   3 | L003 | Line over-indented compared to line #376\r\nL: 235 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 117 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 118 | P:   3 | L003 | Line over-indented compared to line #376\r\nL: 119 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 120 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 121 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 122 | P:   2 | L003 | Indentation not consistent with line #376\r\nL: 339 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 343 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 347 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 351 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 355 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 358 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 361 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 364 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 367 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 370 | P:   1 | L004 | Incorrect indentation type found in file.\r\nThe path(s) ('models/shared/dispensaries.sql', 'models/shares/dispensary_chains.sql') could not be accessed. Check it/they exist(s).\r\n```\r\n\r\n## Expected Behaviour\r\nI would expect only the unaccessible paths to be included in the error message.\r\n\r\n## Observed Behaviour\r\nSee above\r\n\r\n## Version\r\n```bash\r\ndbt@b54bee9ced88:/workspaces/dbt-dutchie$ sqlfluff --version\r\nsqlfluff, version 0.5.2\r\n```\r\n\r\n```bash\r\ndbt@b54bee9ced88:/workspaces/dbt-dutchie$ python --version\r\nPython 3.8.6\r\n```\r\n\r\n## Configuration\r\n```\r\n[sqlfluff]\r\ndialect = snowflake\r\ntemplater = dbt\r\nrules = L001,L002,L003,L004,L005,L006\r\nignore = parsing,templating\r\n\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\ncomma_style = trailing\r\n\r\n[sqlfluff:rules:L010]\r\ncapitalisation_policy = upper\r\n```\r\n\n",
        "hint": "",
        "base": "c3defb095b1aa7fe23c4bd430fdff2ce6ed6161d",
        "env": "dc59c2a5672aacedaf91f0e6129b467eefad331b",
        "files": [
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/core/linter/linter.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/905",
        "problem": "Enhance rule L036 to put all columns on separate lines if any of them are\nThe current description is ambiguous, but after discussion, we decided to update the rule and keep the description at least _similar_ to what it is currently.. See discussion on #769.\n",
        "hint": "FWIW I'm a +1 for this...",
        "base": "62e8dc3a148c40c0c28f62b23e943692a3198846",
        "env": "cbdcfb09feb4883de91de142956c3be6ac7f827d",
        "files": [
            "src/sqlfluff/core/rules/std/L036.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/1625",
        "problem": "TSQL - L031 incorrectly triggers \"Avoid using aliases in join condition\" when no join present\n## Expected Behaviour\r\n\r\nBoth of these queries should pass, the only difference is the addition of a table alias 'a':\r\n\r\n1/ no alias\r\n\r\n```\r\nSELECT [hello]\r\nFROM\r\n    mytable\r\n```\r\n\r\n2/ same query with alias\r\n\r\n```\r\nSELECT a.[hello]\r\nFROM\r\n    mytable AS a\r\n```\r\n\r\n## Observed Behaviour\r\n\r\n1/ passes\r\n2/ fails with: L031: Avoid using aliases in join condition.\r\n\r\nBut there is no join condition :-)\r\n\r\n## Steps to Reproduce\r\n\r\nLint queries above\r\n\r\n## Dialect\r\n\r\nTSQL\r\n\r\n## Version\r\n\r\nsqlfluff 0.6.9\r\nPython 3.6.9\r\n\r\n## Configuration\r\n\r\nN/A\n",
        "hint": "Actually, re-reading the docs I think this is the intended behaviour... closing",
        "base": "14e1a23a3166b9a645a16de96f694c77a5d4abb7",
        "env": "67023b85c41d23d6c6d69812a41b207c4f8a9331",
        "files": [
            "src/sqlfluff/rules/L031.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2326",
        "problem": "`AnySetOf` grammar\n<!--Note: This is for general enhancements to the project. Please use the Bug report template instead to raise parsing/linting/syntax issues for existing supported dialects-->\r\nI know this has been talked about before in PRs so making an issue to formally track.\r\n\r\nIn many grammars there's a common situation where we have to denote several options that can be specified in any order but they cannot be specified more than once.\r\n\r\nOur general approach to this in the project has been denote this using `AnyNumberOf` as this allows for the different orderings:\r\n```python\r\nAnyNumberOf(\r\n    <option_1_grammar>,\r\n    <option_2_grammar>,\r\n    ...\r\n)\r\n```\r\nHowever, the issue with this is that it places no limit on how many times each option can be specified.\r\n\r\nThis means that sqlfluff allows certain invalid statements to parse e.g.\r\n```sql\r\nCREATE TABLE ktw_account_binding (\r\n    ktw_id VARCHAR(32) NOT NULL REFERENCES ref_table(bla)\r\n    ON DELETE RESTRICT ON DELETE CASCADE ON DELETE CASCADE ON DELETE CASCADE\r\n);\r\n```\r\nhttps://github.com/sqlfluff/sqlfluff/pull/2315#issuecomment-1013847846\r\n\r\nWe've accepted this limitation for the time being as it's more important to get the statements parsing for linting/formatting purposes rather than exactly reflecting the grammar (we'd expect a general degree of common sense when it comes to repeating these options).\r\n\r\nThat being said it would be nice to address this to refine our grammar and reduce dealing with contributor confusion.\r\n\r\n`AnySetOf` would essentially allow all of it's grammar arguments to be parsed in any order a maximum of 1 time each. Hopefully we can inherit from `AnyNumberOf` to simplify this.\n",
        "hint": "",
        "base": "38cff664d9505999fb7473a4a7b29ba36aba7883",
        "env": "a5c4eae4e3e419fe95460c9afd9cf39a35a470c4",
        "files": [
            "src/sqlfluff/core/parser/__init__.py",
            "src/sqlfluff/core/parser/grammar/__init__.py",
            "src/sqlfluff/core/parser/grammar/anyof.py",
            "src/sqlfluff/dialects/dialect_ansi.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3220",
        "problem": "Config for fix_even_unparsable not being applied\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nWhen setting the any config file to `fix_even_unparsable = True` the config get's overriden by the default (or lack thereof) on the @click.option decorator for the fix command.\r\n\r\n### Expected Behaviour\r\n\r\nWhen setting the config `fix_even_unparsable` it should be captured by the fix command as well.\r\n\r\n### Observed Behaviour\r\n\r\nThe `fix_even_unparsable` command is not being captured by the fix command\r\n\r\n### How to reproduce\r\n\r\nCreate a config file and include `fix_even_unparsable`\r\nRun `sqlfluff fix`\r\nNote that `fix_even_unparsable` is set to False at runtime\r\n\r\n### Dialect\r\n\r\nAny\r\n\r\n### Version\r\n\r\n0.13.0\r\n\r\n### Configuration\r\n\r\n`pyproject.toml`\r\n\r\n```\r\n[tool.sqlfluff.core]\r\nverbose = 2\r\ndialect = \"snowflake\"\r\nfix_even_unparsable = true\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
        "hint": "I believe the fix would be to just add a `default=None,` to the @click.option decorator.\r\nThis is simple enough for me to create a PR but I don't know how to create tests (or if just adding it is enough) for it as required on the PR template.\n> I believe the fix would be to just add a `default=None,` to the @click.option decorator.\r\nConfirmed that worked\r\n\r\n> This is simple enough for me to create a PR but I don't know how to create tests (or if just adding it is enough) for it as required on the PR template.\r\n\r\nIt would be good to have a test. If you look at `test/fixtures/linter/autofix/snowflake/001_semi_structured` you can see a similar test that uses a .sqlfluff config file for the test run.\nI'm happy to take this unless you want to do it, @pekapa. I fixed a very similar issue with the `--encoding` option a few weeks ago.",
        "base": "262010b91cf5616de242dad504c788e9cd33ac58",
        "env": "8f6fd1d8a8d69b2c463fbcf5bd1131c47f12ad88",
        "files": [
            "src/sqlfluff/cli/commands.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2419",
        "problem": "Rule L060 could give a specific error message\nAt the moment rule L060 flags something like this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.\r\n```\r\n\r\nSince we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.\r\n\r\nThat is it should flag this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\r\n```\r\n Or this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.\r\n```\r\n\r\nAs appropriate.\r\n\r\nWhat do you think @jpy-git ?\r\n\n",
        "hint": "@tunetheweb Yeah definitely, should be a pretty quick change \ud83d\ude0a",
        "base": "f1dba0e1dd764ae72d67c3d5e1471cf14d3db030",
        "env": "a5c4eae4e3e419fe95460c9afd9cf39a35a470c4",
        "files": [
            "src/sqlfluff/rules/L060.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3170",
        "problem": "Commented dash character converted to non utf-8 character\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nUpon fixing a query containing a multi-line comment, SQLFluff attempts to fix a commented line.\r\n\r\nThis:\r\n```sql\r\n/*\r\nTODO\r\n - tariff scenario \u2014> dm_tariff_scenario\r\n*/\r\n```\r\n\r\nBecame:\r\n```sql\r\n/*\r\nTODO\r\n - tariff scenario \u0097> dm_tariff_scenario\r\n*/\r\n``` \r\nThis in an invisible char represented as `<97>`\r\n\r\nThis causes an issue with dbt which can not compile with this char present\r\n\r\nNote this comment comes at the end of the file.\r\n\r\n### Expected Behaviour\r\n\r\nDoes not replace/fix anything that is commented\r\n\r\n### Observed Behaviour\r\n\r\n```bash\r\n $  sqlfluff fix dbt/models/marts/core/f_utility_statements.sql                                                                                                                                                                                               \r\n==== finding fixable violations ====                                                                                                                                                                                                                          \r\n=== [dbt templater] Sorting Nodes...                                                                                                                                                                                                                          \r\n=== [dbt templater] Compiling dbt project...                                                                                                                                                                                                                  \r\n=== [dbt templater] Project Compiled.                                                                                                                                                                                                                         \r\n== [dbt/models/marts/core/f_utility_statements.sql] FAIL                                                                                                                                                                                                      \r\nL:   1 | P:   5 | L001 | Unnecessary trailing whitespace.                                                                                                                                                                                                     \r\nL:   2 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:   3 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \r\nL:   4 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:   4 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \r\nL:   6 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:   7 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \r\nL:   8 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:   8 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \r\nL:  10 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:  11 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \r\nL:  12 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:  12 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \r\nL:  15 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]   \r\nL:  16 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                      [0/47960]\r\nL:  17 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  18 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  19 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  20 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  20 | P:  36 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  21 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  21 | P:  32 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  22 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  22 | P:   6 | L019 | Found trailing comma. Expected only leading.\r\nL:  24 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  26 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  26 | P:  15 | L001 | Unnecessary trailing whitespace.\r\nL:  27 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  28 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  29 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  30 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  31 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  32 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  32 | P:  24 | L011 | Implicit/explicit aliasing of table.\r\nL:  32 | P:  24 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  33 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  33 | P:  49 | L011 | Implicit/explicit aliasing of table.\r\nL:  33 | P:  49 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  33 | P:  52 | L001 | Unnecessary trailing whitespace.\r\nL:  34 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  36 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  37 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  37 | P:   6 | L019 | Found trailing comma. Expected only leading.\r\nL:  39 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  41 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  41 | P:   9 | L034 | Select wildcards then simple targets before calculations\r\n                       | and aggregates.\r\nL:  43 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  46 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  47 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  48 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  51 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  52 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  53 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  54 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  57 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  58 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  61 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  62 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  64 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  65 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  68 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  69 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  70 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  71 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  73 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  73 | P:  36 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  74 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  74 | P:  56 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  75 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  76 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  76 | P:  28 | L001 | Unnecessary trailing whitespace.\r\nL:  77 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  80 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  81 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  83 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  84 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  94 | P:   1 | L009 | Files must end with a single trailing newline.\r\n```\r\n\r\n### How to reproduce\r\n\r\n`sqlfluff fix` with provided `.sqlfluff` configuration\r\n\r\nSQL contains proprietary code and I am, likely, unable to provide a full snippet of the SQL \r\n\r\n### Dialect\r\n\r\nSnowflake\r\n\r\n### Version\r\n\r\n0.13.0 and 0.11.1\r\n\r\n### Configuration\r\n\r\n`.sqlfluff`:\r\n```\r\n[sqlfluff]\r\ntemplater = dbt\r\ndialect = snowflake\r\n\r\n[sqlfluff:templater:dbt]\r\nproject_dir = dbt/\r\n\r\n# Defaults on anything not specified explicitly: https://docs.sqlfluff.com/en/stable/configuration.html#default-configuration\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\ncomma_style = leading\r\n\r\n# Keyword capitalisation\r\n[sqlfluff:rules:L010]\r\ncapitalisation_policy = lower\r\n\r\n# TODO: this supports pascal but not snake\r\n# TODO: this inherits throwing violation on all unquoted identifiers... we can limit to aliases or column aliases\r\n# [sqlfluff:rules:L014]\r\n# extended_capitalisation_policy = pascal\r\n\r\n# TODO: not 100% certain that this default is correct\r\n# [sqlfluff:rules:L029]\r\n## Keywords should not be used as identifiers.\r\n# unquoted_identifiers_policy = aliases\r\n# quoted_identifiers_policy = none\r\n## Comma separated list of words to ignore for this rule\r\n# ignore_words = None\r\n\r\n# Function name capitalisation\r\n[sqlfluff:rules:L030]\r\nextended_capitalisation_policy = lower\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
        "hint": "I can't reproduce this, but this usually happens when the file itself is in some other format, rather than UTF-8, to begin with. Can you confirm it's definitely UTF-8 encoded? some tips here on how to check this: https://stackoverflow.com/questions/6947749/how-to-check-if-a-txt-file-is-in-ascii-or-utf-8-format-in-windows-environment\nYou'll probably need to explicitly set the encoding. SQLFluff defaults to using `autodetect`, which is implemented by the third-party `chardet` package, but it's not guaranteed to always do the right thing. If it misbehaves, we (SQLFluff) can't do anything about it.\r\n\r\n```\r\n# can either be autodetect or a valid encoding e.g. utf-8, utf-8-sig\r\nencoding = autodetect\r\n```\r\n\r\nWe'd like to hear back from you, but this issue is likely to be closed as \"won't fix\"/\"can't fix\"\nI have confirmed that the file is indeed utf-8 encoded and I have explicitly set the encoding to utf-8 and retested with the same result.\r\n\r\nAfter running `sqlfluff fix` I have seen the encoding change from utf-8 to western (Windows 1252)\r\n\r\nEDIT: If i manually force the file to be utf-8 AFTER `sqlfluff fix`, it resolves the issue. Good to know, but not a sustainable solution\nDid you set the encoding property in `.sqlfluff`? It does not appear in the `.sqlfluff` file you provided above.\r\n\r\nAlso, please provide a test SQL file. You only provided a comment, not a complete file. When I run `sqlfluff fix` on the file, I get:\r\n```\r\n(sqlfluff-3.9.1) \u279c  sqlfluff git:(main) \u2717 sqlfluff fix test.sql\r\n==== finding fixable violations ====\r\n==== no fixable linting violations found ====\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n```\nI did. The config file I provided does not contain it but I retested using your suggestion and had the same result\nI tried the above on my Mac. The resulting file looked okay to me:\r\n```\r\n - tariff scenario \u2014> dm_tariff_scenario\r\n```\r\n\r\nWhat operating system are using? Windows? Mac? Linux?\nI am on an intel mac with Montery 12.3.1\r\n\r\nAre you able to run sqlfluff fix twice in succession? The first run is fine, its the second run that fails\r\n\r\n(Depending on my editor, it may or may not show the offending character. ie vim shows it, sublime does not)\r\n\nYes, I can run it twice in succession. The first time, it fixes a bunch of things. The second time, no issues found. Partial output below.\r\n```\r\nL:  83 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  84 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\n==== fixing violations ====\r\n72 fixable linting violations found\r\nAre you sure you wish to attempt to fix these? [Y/n] ...\r\nAttempting fixes...\r\nPersisting Changes...\r\n== [test.sql] PASS\r\nDone. Please check your files to confirm.\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n  [3 unfixable linting violations found]\r\n(sqlfluff-3.9.1) \u279c  sqlfluff git:(main) \u2717 sqlfluff fix test.sql\r\n==== finding fixable violations ====\r\n==== no fixable linting violations found ====\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n  [2 unfixable linting violations found]\r\n```\r\n\r\nI'm on an M1 Mac with Big Sur (11.5.2).\r\n\r\nVery strange behavior:\r\n* That I can't reproduce it on a similar machine\r\n* That setting `encoding = utf-8` in `.sqlfluff` doesn't fix it.\r\n\r\nNote that AFAIK, \"encoding\" is not a real property of most files file. It's a guess made when reading the file. Some file formats let you specify the encoding, but SQL is not one of them. Hence the need to use a package like `chardet`.\r\n\r\nE.g. Python lets you do it with a special comment: https://stackoverflow.com/questions/6289474/working-with-utf-8-encoding-in-python-source\nI just noticed interesting behavior. I ran with `-vv` to ensure my config and although I am specifying `encoding = utf-8`, the -vv output seems to suggest `autodetect`. It is honoring other config (like `dbt`). Attempting to see where I have gone wrong on my side\r\n\r\nEDIT: for context on directory structure: \r\n```\r\n.sqlfluff\r\n./dbt/models/marts/core/file.sql\r\n```\r\nI am running sqlfluff from the same directory as the `.sqlfluff` file ie `sqlfluff fix dbt/models/marts/core/file.sql`\nI've heard the behavior can become tricky if you have multiple .sqlfluff files in subdirectories, etc. Are you certain you added the setting in the correct section of the file? If you put it in the wrong place, it'll be ignored, and it'll use the default setting instead, which is autodetect.\nit is at the top level \r\n```\r\n[sqlfluff]\r\ntemplater = dbt\r\ndialect = snowflake\r\nencoding = utf-8\r\n\r\n...\r\n```\r\nas per your default configuration docs. There are no .sqlfluff files in sub folders in that directory\n@barrywhart Okay... so if I specify `--encoding utf-8` as a CLI command I am able to fix the file with no issue!! Thank you for helping with that!\r\n\r\nI am unsure why it is not honoring that config however. Is there a way you would recommend debugging this issue from my side? We use this both as a CLI tool and as a pre-commit - so we are able to use the `--encoding` option explicitly, but it provides peace of mind to know why it _seems_ to not honor specific configs\r\n\r\nI have changed other configs (ie adding an `excluded_rule`) and it IS honoring that (with no other changes to how i am running it)\r\n\r\nAlso super appreciate all the help :) \nLet me look into it later (probably in the next day or two). Not many people use this option, so I'd like to double check that it's being read correctly from config.\nawesome! I appreciate it @barrywhart (and @tunetheweb )!\r\n\r\nWe, as an organization, are investing in SQLFluff as our production linter and we appreciate your support!\nThanks for the kind words. It's exciting to us seeing the project catching on. I've been involved with the project since late 2019, and I'm proud of the progress it's made. It seems to be becoming pretty mainstream now. One reason I've stayed involved is, how often do you get to help invent a fundamental new industry tool? \ud83d\ude0a\r\n\r\nBTW, feel free to delete your example SQL from the issue. It seems like we may not need it anymore?\nExactly! I have been loosely following this project for the past year and have been pushing to use it widely for a while! We adopted DBT and, since SQLFluff interacts well with DBT, we got the buy-in to invest :)\r\n\r\nAnd yes I will delete the SQL!\r\n\r\nPlease let me know what you find relating to the encoding configuration! I am continuing to fiddle from my side!\nI'm seeing the same issue -- seems that the `encoding` setting in `.sqlfluff` is not being read correctly:\r\n```\r\n[sqlfluff]\r\nencoding = utf-8\r\n```\r\n\r\nWe have automated tests for encoding, but they are lower-level tests (i.e. they exercise internal code directly, not reading encoding from `.sqlfluff`).\r\n\r\nI'll take a closer look. Presumably, this should be easy to fix.",
        "base": "6c026c76aa8b13eae54cd7e18d62b0a57fc71dce",
        "env": "8f6fd1d8a8d69b2c463fbcf5bd1131c47f12ad88",
        "files": [
            "src/sqlfluff/cli/commands.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2846",
        "problem": "ValueError: Position Not Found for lint/parse/fix, not clear why\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nI have admittedly messy dbt sql model that gets the following error when I try to lint, parse or fix it with sqlfluff - every other model can be processed using the same settings, but this one throws the same error below even if I only run a single rule e.g. L009.\r\n\r\nUnfortunately I cannot share the model itself but I can describe some notable features:\r\n- begins with a dbt incremental config\r\n- then sets three variables, each a list of strings\r\n- Has two `for` loops with nested `if` conditions\r\n- Has one very long line doing arithmetic operations involving both hardcoded values and columns from a two joined CTEs\r\n\r\n### Expected Behaviour\r\n\r\nNot the above error\r\n\r\n### Observed Behaviour\r\n\r\n```\r\nWARNING    Unable to lint models/ltv_prediction_model/ltv_prediction.sql due to an internal error. Please report this as an issue w\r\nith your query's contents and stacktrace below!\r\nTo hide this warning, add the failing file to .sqlfluffignore\r\nTraceback (most recent call last):\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/runner.py\", line 103, in run\r\n    yield partial()\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 666, in lint_rendered\r\n    parsed = cls.parse_rendered(rendered)\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 352, in parse_rendere\r\n\r\nd\r\n    tokens, lvs, config = cls._lex_templated_file(\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 139, in _lex_template\r\nd_file\r\n    tokens, lex_vs = lexer.lex(templated_file)\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/parser/lexer.py\", line 321, in lex\r\n    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/parser/lexer.py\", line 348, in elements_to_se\r\ngments\r\n    source_slice = templated_file.templated_slice_to_source_slice(\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 294, in templated_s\r\nlice_to_source_slice\r\n    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 180, in _find_slice\r\n_indices_of_templated_pos\r\n    raise ValueError(\"Position Not Found\")\r\nValueError: Position Not Found\r\n```\r\n\r\n### How to reproduce\r\n\r\n```\r\n{{\r\n    config(\r\n        materialized='incremental',\r\n        unique_key='md5_surrogate_key_main'\r\n    )\r\n}}\r\n\r\n{%- set first_list = [\"value1\", \"value2\", \"value3\"] -%}\r\n{%- set second_list = [\"value4\", \"value5\", \"value6\"] -%}\r\n{%- set third_list = [\"value7\", \"value8\", \"value9\"] -%}\r\n\r\nwith fill_na_values as (\r\n    select\r\n        id,\r\n        run_date,\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            {%- if features in third_list %}\r\n                coalesce({{features}}, (select feature_mode from {{ ref('second_list') }} where features = '{{features}}')) as {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- else -%}\r\n                coalesce({{features}}, (select feature_mean from {{ ref('second_list') }} where features = '{{features}}')) as {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- endif -%}\r\n        {%- endfor %}\r\n    from {{ ref('training_dataset') }}\r\n    {%- if is_incremental() %}\r\n    where current_date >= (select max(run_date) from {{ this }})\r\n    {%- else %}\r\n    where run_date >= '2021-01-01'\r\n    {%- endif %}\r\n),\r\n\r\nwinsorize_data as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            {%- if features in first_list %}\r\n                case\r\n                    when {{features}} < (select fifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    then (select fifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    when {{features}} > (select ninetyfifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    then (select ninetyfifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    else {{features}}\r\n                end as {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- else %}\r\n                {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- endif %}\r\n        {%- endfor %}\r\n    from fill_na_values\r\n),\r\n\r\nscaling_data as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            ({{features}} - (select feature_mean from {{ ref('second_list') }} where features = '{{features}}'))/(select feature_std from {{ ref('second_list') }} where features = '{{features}}') as {{features}}\r\n            {%- if not loop.last -%},{% endif %}\r\n        {%- endfor %}\r\n    from winsorize_data\r\n),\r\n\r\napply_ceofficients as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            {{features}} * (select coefficients from {{ ref('second_list') }} where features = '{{features}}') as {{features}}_coef\r\n            {%- if not loop.last -%},{% endif %}\r\n        {%- endfor %}\r\n    from scaling_data\r\n),\r\n\r\nlogistic_prediction as (\r\n    select\r\n        fan.*,\r\n        1/(1+EXP(-(0.24602303+coef1+coef2+coef3+coef4+coef5+coef6+coef7+coef8+coef9+available_balance_coef+coef10+coef11+coef12+coef13+coef14))) as prediction_probability,\r\n        case when prediction_probability < .5 then 0 else 1 end as prediction_class\r\n    from apply_ceofficients ac\r\n    inner join fill_na_values fan\r\n        on ac.md5_surrogate_key_main = fan.md5_surrogate_key_main\r\n)\r\n\r\nselect * from logistic_prediction\r\n```\r\n\r\n### Dialect\r\n\r\nSnowflake\r\n\r\n### Version\r\n\r\n0.10.1\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\n# verbose is an integer (0-2) indicating the level of log output\r\nverbose = 0\r\n# Turn off color formatting of output\r\nnocolor = False\r\ndialect = snowflake\r\ntemplater = jinja\r\n# Comma separated list of rules to check, or None for all\r\nrules = L001,L002,L003,L004,L005,L009,L010,L013,L014,L015,L017,L018,L019,L020,L021,L022,L023,L024,L026,L027,L028,L030,L036,L037,L038,L039,L040,L044,L045,L046,L050,L051,L058,L061\r\n# Comma separated list of rules to exclude, or None\r\nexclude_rules = L006,L008,L011,L012,L025,L029,L031,L034,L035,L041,L042,L043,L052\r\n# The depth to recursively parse to (0 for unlimited)\r\nrecurse = 0\r\n# Below controls SQLFluff output, see max_line_length for SQL output\r\noutput_line_length = 80\r\n# Number of passes to run before admitting defeat\r\nrunaway_limit = 10\r\n# Ignore errors by category (one or more of the following, separated by commas: lexing,linting,parsing,templating)\r\nignore = None\r\n# Ignore linting errors found within sections of code coming directly from\r\n# templated code (e.g. from within Jinja curly braces. Note that it does not\r\n# ignore errors from literal code found within template loops.\r\nignore_templated_areas = True\r\n# can either be autodetect or a valid encoding e.g. utf-8, utf-8-sig\r\nencoding = autodetect\r\n# Ignore inline overrides (e.g. to test if still required)\r\ndisable_noqa = False\r\n# Comma separated list of file extensions to lint\r\n# NB: This config will only apply in the root folder\r\nsql_file_exts = .sql,.sql.j2,.dml,.ddl\r\n# Allow fix to run on files, even if they contain parsing errors\r\n# Note altering this is NOT RECOMMENDED as can corrupt SQL\r\nfix_even_unparsable = False\r\n\r\n[sqlfluff:indentation]\r\n# See https://docs.sqlfluff.com/en/stable/indentation.html\r\nindented_joins = False\r\nindented_ctes = False\r\nindented_using_on = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n# Some rules can be configured directly from the config common to other rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 80\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# Some rules have their own specific config\r\n[sqlfluff:rules:L007]\r\noperator_new_lines = after\r\n\r\n[sqlfluff:rules:L010]\r\n# Keywords\r\ncapitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L011]\r\n# Aliasing preference for tables\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L012]\r\n# Aliasing preference for columns\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L014]\r\n# Unquoted identifiers\r\nextended_capitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L016]\r\n# Line length\r\nignore_comment_lines = False\r\nignore_comment_clauses = False\r\n\r\n[sqlfluff:rules:L026]\r\n# References must be in FROM clause\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L028]\r\n# References must be consistently used\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]\r\n# Keywords should not be used as identifiers.\r\nunquoted_identifiers_policy = aliases\r\nquoted_identifiers_policy = none\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L030]\r\n# Function names\r\ncapitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L038]\r\n# Trailing commas\r\nselect_clause_trailing_comma = forbid\r\n\r\n[sqlfluff:rules:L040]\r\n# Null & Boolean Literals\r\ncapitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses\r\nforbid_subquery_in = join\r\n\r\n[sqlfluff:rules:L047]\r\n# Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n[sqlfluff:rules:L052]\r\n# Semi-colon formatting approach\r\nmultiline_newline = False\r\nrequire_final_semicolon = False\r\n\r\n[sqlfluff:rules:L054]\r\n# GROUP BY/ORDER BY column references\r\ngroup_by_and_order_by_style = consistent\r\n\r\n[sqlfluff:rules:L057]\r\n# Special characters in identifiers\r\nunquoted_identifiers_policy = all\r\nquoted_identifiers_policy = all\r\nallow_space_in_identifier = False\r\nadditional_allowed_characters = \"\"\r\n\r\n[sqlfluff:rules:L059]\r\n# Policy on quoted and unquoted identifiers\r\nprefer_quoted_identifiers = False\r\n\r\n[sqlfluff:rules:L062]\r\n# Comma separated list of blocked words that should not be used\r\nblocked_words = None\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n```\n",
        "hint": "> Proprietary concerns prevent me from sharing the query itself, I could try to boil it down to a mock version that replicates the error.\r\n\r\nThis would be needed before we can make any progress on this I'm afraid. The stack trace is good, and will help us identify the area of code, but without the SQL we can't know why and this issue will need to be closed.\n@tunetheweb fair play, let me try to create a stripped down representation.\n@tunetheweb there you go!\nI managed to reproduce this in the latest `main`. Taking a quick look...\nIt fails when looking for templated position 198. The highest-numbered position in the `sliced_file` collection is 190. \r\n```\r\n(Pdb) templated_pos\r\n198\r\n(Pdb) pp self.sliced_file\r\n[TemplatedFileSlice(slice_type='templated', source_slice=slice(0, 103, None), templated_slice=slice(0, 0, None)),\r\n TemplatedFileSlice(slice_type='literal', source_slice=slice(103, 105, None), templated_slice=slice(0, 0, None)),\r\n TemplatedFileSlice(slice_type='block_start', source_slice=slice(105, 160, None), templated_slice=slice(0, 0, None)),\r\n...\r\n TemplatedFileSlice(slice_type='literal', source_slice=slice(2822, 2823, None), templated_slice=slice(190, 190, None)),\r\n TemplatedFileSlice(slice_type='block_end', source_slice=slice(2823, 2834, None), templated_slice=slice(190, 190, None)),\r\n TemplatedFileSlice(slice_type='literal', source_slice=slice(2834, 2843, None), templated_slice=slice(190, 190, None)),\r\n TemplatedFileSlice(slice_type='block_end', source_slice=slice(2843, 2856, None), templated_slice=slice(190, 190, None)),\r\n TemplatedFileSlice(slice_type='literal', source_slice=slice(2856, 3358, None), templated_slice=slice(190, 190, None))]\r\n(Pdb) \r\n```\r\n\r\nThe `sliced_file` is clearly wrong, because the rendered SQL is 2,083 characters long:\r\n```\r\n(Pdb) len(str(self))\r\n2083\r\n```\nThe templater is losing track of things at line 19 of the input file:\r\n```\r\n                coalesce({{features}}, (select feature_mode from {{ ref('second_list') }} where features = '{{features}}')) as {{features}}\r\n```\r\n\r\nPosition 198 is where the code `{{features}}` renders, just after `coalesce(`.\nThe following simpler SQL can be used to reproduce the same issue:\r\n```\r\nselect\r\n    {%- for features in [\"value4\", \"value5\"] %}\r\n        {%- if features in [\"value7\"] %}\r\n            {{features}}\r\n            {%- if not loop.last -%},{% endif %}\r\n        {%- else -%}\r\n            {{features}}\r\n            {%- if not loop.last -%},{% endif %}\r\n        {%- endif -%}\r\n    {%- endfor %}\r\nfrom my_table\r\n```\r\n\r\nThis is another test case I extracted (may be the same bug, not sure):\r\n```\r\n{%- set first_list = [\"value1\", \"value2\", \"value3\"] -%}\r\n{%- set second_list = [\"value4\", \"value5\", \"value6\"] -%}\r\n\r\nwith winsorize_data as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            {%- if features in first_list %}\r\n                case\r\n                    when {{features}} < (select fifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    then (select fifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    when {{features}} > (select ninetyfifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    then (select ninetyfifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    else {{features}}\r\n                end as {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- else %}\r\n                {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- endif %}\r\n        {%- endfor %}\r\n    from ref('training_dataset')\r\n),\r\n\r\nscaling_data as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            ({{features}} - (select feature_mean from {{ ref('second_list') }} where features = '{{features}}'))/(select feature_std from {{ ref('second_list') }} where features = '{{features}}') as {{features}}\r\n            {%- if not loop.last -%},{% endif %}\r\n        {%- endfor %}\r\n    from winsorize_data\r\n),\r\n\r\napply_ceofficients as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            {{features}} * (select coefficients from {{ ref('second_list') }} where features = '{{features}}') as {{features}}_coef\r\n            {%- if not loop.last -%},{% endif %}\r\n        {%- endfor %}\r\n    from scaling_data\r\n),\r\n\r\nlogistic_prediction as (\r\n    select\r\n        fan.*,\r\n        1/(1+EXP(-(0.24602303+coef1+coef2+coef3+coef4+coef5+coef6+coef7+coef8+coef9+available_balance_coef+coef10+coef11+coef12+coef13+coef14))) as prediction_probability,\r\n        case when prediction_probability < .5 then 0 else 1 end as prediction_class\r\n    from apply_ceofficients ac\r\n    inner join fill_na_values fan\r\n        on ac.md5_surrogate_key_main = fan.md5_surrogate_key_main\r\n)\r\n\r\nselect * from logistic_prediction\r\n```\n@davesgonechina: I found a workaround if you want to try it. Don't use Jinja whitespace control. In other words, replace all occurrences of `{%-` with `{%` and all occurrences of `-%}` with `%}`.\r\n\r\nI'll keep looking to see if I can find a fix. SQLFluff has had some past bugs involving whitespace control. Basically, it makes SQLFluff's job more challenging, when it tries to \"map\" the input SQL (before running Jinja) to the output file (after running Jinja).\nIn the file `src/sqlfluff/core/templaters/slicers/tracer.py`, I thought that the recently added function `_remove_block_whitespace_control` would eliminate any issues with whitespace control. It was added to fix _some_ issues like this. Perhaps this is a more complex situation?\r\n\r\nGenerally, avoiding whitespace control in the \"alternate\" template results in template output with more \"breadcrumbs\", making it easier for the tracer to deduce the execution path of the template. The issue we saw before (which may be happening here) is that the tracer loses track of the execution path and \"drops\" off the end of the template at some point. Should be fairly easy to find where (and why) this is happening. May be harder to fix. We shall see...",
        "base": "f37dc1410cefc4e08ed8110f820c9071bc4b0c7d",
        "env": "3d52e8270d82aeccf4c516d059a80a6947919aea",
        "files": [
            "src/sqlfluff/core/templaters/slicers/tracer.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3662",
        "problem": "Number of processes configurable in .sqlfluff\nBeing able to set the number of processes to run with in .sqlfluff might be useful to avoid having to pass it in the CLI every time.\n",
        "hint": "One thought on this: The same `.sqlfluff` file will sometimes be used on different machines (e.g. various development machines, CI server). We should allow the setting to be somewhat \"context sensitive\" if desired. Proposal:\r\n* Positive values indicate the number of processes to create\r\n* Zero or negative values are interpreted as `number_of_cpus - specified_number`. Thus, a value of `0` means \"use all processors\" and `-1` means \"use all processors except one\".\nIs there a standard way in python to detect the effective available cpus?\n@alanmcruickshank: Yes. Use [`multiprocessing.cpu_count()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.cpu_count).\nI'm happy to pick this one up. This actually fits with a small reorg of how I think threads should be configured. I think it fits better if the thread argument is passed in to the `Linter` object on instantiation, rather than when calling `lint_paths`. @barrywhart - does that sit well with you? I realise that changes a little of the structure you originally envisaged.\nSounds good -- no concerns from me.",
        "base": "f9a3fe8f639d279226f16bdc51326dfa5c142c3e",
        "env": "388dd01e05c7dcb880165c7241ed4027d9d0171e",
        "files": [
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/cli/formatters.py",
            "src/sqlfluff/core/linter/linter.py",
            "src/sqlfluff/core/linter/runner.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4997",
        "problem": "Validate layout configurations on load\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nAs raised in this comment: https://github.com/sqlfluff/sqlfluff/pull/4558#discussion_r1142745101\r\n\r\nAt the moment, the layout configs are being validated _on use_ which is potentially flaky and convoluted. Better would be to validate configs _on load_.\n\n### Use case\n\n_No response_\n\n### Dialect\n\nall\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "",
        "base": "50bbffd4672aa17af2651f40d533cf55048b7524",
        "env": "3629c3e702939c07264cc5ea903566ddc9ea2bb0",
        "files": [
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/core/config.py",
            "src/sqlfluff/core/linter/linter.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2998",
        "problem": "BigQuery: Accessing `STRUCT` elements evades triggering L027\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nAccessing unreferenced `STRUCT` elements using BigQuery dot notation in a multi table query does not trigger L027.\n\n### Expected Behaviour\n\nL027 gets triggered.\n\n### Observed Behaviour\n\nL027 does not get triggered.\n\n### How to reproduce\n\n```sql\r\nSELECT\r\n    t1.col1,\r\n    t2.col2,\r\n    events.id\r\nFROM t_table1 AS t1\r\nLEFT JOIN t_table2 AS t2\r\n    ON TRUE\r\n```\n\n### Dialect\n\nBigQUery\n\n### Version\n\n`0.11.2` using online.sqlfluff.com\n\n### Configuration\n\nN/A\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "This is tricky.\r\n\r\nBasicaly L026 works to make sure qualified columns only use tables in the from clause. This doesn\u2019t really work for `STRUCT`s as impossible to know if it\u2019s a qualified column or a `STRUCT`, so is off by default for languages that support them - like BigQuery.\r\n\r\nL027 works to make sure columns are qualified for multi-table joins (i.e. have at least one dot). But it doesn\u2019t check the qualifiers are valid - that\u2019s L026\u2019s job, which as I say is off by default for BigQuery.",
        "base": "47c8bb29104761474e455ef2e6fdaa7a8cc20a56",
        "env": "2bdeb9354d33e3fb4dfd6782e1e1921939ecb55a",
        "files": [
            "src/sqlfluff/rules/L027.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/1733",
        "problem": "Extra space when first field moved to new line in a WITH statement\nNote, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.\r\n\r\nGiven the following SQL:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Expected Behaviour\r\n\r\nafter running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n        my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Observed Behaviour\r\n\r\nafter running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n         my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Steps to Reproduce\r\n\r\nNoted above. Create a file with the initial SQL and fun `sqfluff fix` on it.\r\n\r\n## Dialect\r\n\r\nRunning with default config.\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\nsqlfluff, version 0.7.0\r\nPython 3.7.5\r\n\r\n## Configuration\r\n\r\nDefault config.\r\n\n",
        "hint": "Does running `sqlfluff fix` again correct the SQL?\n@tunetheweb yes, yes it does. Is that something that the user is supposed to do (run it multiple times) or is this indeed a bug?\nIdeally not, but there are some circumstances where it\u2019s understandable that would happen. This however seems an easy enough example where it should not happen.\nThis appears to be a combination of rules L036, L003, and L039 not playing nicely together.\r\n\r\nThe original error is rule L036 and it produces this:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\nmy_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\nThat is, it moves the `my_id` down to the newline but does not even try to fix the indentation.\r\n\r\nThen we have another run through and L003 spots the lack of indentation and fixes it by adding the first set of whitespace:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n    my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\nThen we have another run through and L003 spots that there still isn't enough indentation and fixes it by adding the second set of whitespace:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n        my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\nAt this point we're all good.\r\n\r\nHowever then L039 has a look. It never expects two sets of whitespace following a new line and is specifically coded to only assume one set of spaces (which it normally would be if the other rules hadn't interfered as it would be parsed as one big space), so it think's the second set is too much indentation, so it replaces it with a single space.\r\n\r\nThen another run and L003 and the whitespace back in so we end up with two indents, and a single space.\r\n\r\nLuckily the fix is easier than that explanation. PR coming up...\r\n\r\n",
        "base": "a1579a16b1d8913d9d7c7d12add374a290bcc78c",
        "env": "67023b85c41d23d6c6d69812a41b207c4f8a9331",
        "files": [
            "src/sqlfluff/rules/L039.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3648",
        "problem": "dbt & JinjaTracer results in passing invalid query to database (was: DBT Call statement() block causes invalid query generated)\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen using the call statement() to run a query during compile time, the query generated is garbled causing the following sql error:\r\n```\r\n{% call statement('variables', fetch_result=true) %}\r\n\r\nselect 1 as test;\r\n\r\n{% endcall %}\r\n\r\n{% set test = load_result('variables')['table'].columns.TEST.values()[0] %}\r\n```\r\n\r\nThis results in the following error:\r\n\r\ndbt.exceptions.DatabaseException: Database Error\r\n  001003 (42000): SQL compilation error:\r\n  syntax error line 1 at position 0 unexpected '0'.\r\n\r\nThe query ran looks like this when looking at the query runner history in snowflake:\r\n\r\n```\r\n\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a_0\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a_8\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a_0\r\n```\r\n\r\nWhereas it should show:\r\n```\r\nselect 1 as test;\r\n```\n\n### Expected Behaviour\n\nExpected that the query runs properly.\n\n### Observed Behaviour\n\n```\r\n=== [dbt templater] Compiling dbt project...\r\n=== [dbt templater] Project Compiled.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 219, in exception_handler\r\n    yield\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 70, in add_query\r\n    cursor.execute(sql, bindings)\r\n  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/cursor.py\", line 794, in execute\r\n    Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)\r\n  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 273, in errorhandler_wrapper\r\n    handed_over = Error.hand_to_other_handler(\r\n  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 328, in hand_to_other_handler\r\n    cursor.errorhandler(connection, cursor, error_class, error_value)\r\n  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 207, in default_errorhandler\r\n    raise error_class(\r\nsnowflake.connector.errors.ProgrammingError: 001003 (42000): SQL compilation error:\r\nsyntax error line 1 at position 0 unexpected '0'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/sqlfluff\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/cli/commands.py\", line 1008, in parse\r\n    parsed_strings = list(\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 1171, in parse_path\r\n    yield self.parse_string(\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 835, in parse_string\r\n    rendered = self.render_string(in_str, fname, config, encoding)\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 784, in render_string\r\n    templated_file, templater_violations = self.templater.process(\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 47, in _wrapped\r\n    return func(self, in_str=in_str, fname=fname, config=config, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff_templater_dbt/templater.py\", line 331, in process\r\n    processed_result = self._unsafe_process(fname_absolute_path, in_str, config)\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff_templater_dbt/templater.py\", line 552, in _unsafe_process\r\n    raw_sliced, sliced_file, templated_sql = self.slice_file(\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/jinja.py\", line 462, in slice_file\r\n    trace = tracer.trace(append_to_templated=kwargs.pop(\"append_to_templated\", \"\"))\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/slicers/tracer.py\", line 77, in trace\r\n    trace_template_output = trace_template.render()\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 1090, in render\r\n    self.environment.handle_exception()\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception\r\n    reraise(*rewrite_traceback_stack(source=source))\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"<template>\", line 16, in top-level template code\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/sandbox.py\", line 462, in call\r\n    return __context.call(__obj, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/clients/jinja.py\", line 321, in __call__\r\n    return self.call_macro(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/clients/jinja.py\", line 248, in call_macro\r\n    return macro(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/runtime.py\", line 679, in _invoke\r\n    rv = self._func(*arguments)\r\n  File \"<template>\", line 10, in template\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/sandbox.py\", line 462, in call\r\n    return __context.call(__obj, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/base/impl.py\", line 235, in execute\r\n    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 122, in execute\r\n    _, cursor = self.add_query(sql, auto_begin)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 458, in add_query\r\n    connection, cursor = super().add_query(\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 78, in add_query\r\n    return connection, cursor\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 137, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 238, in exception_handler\r\n    raise DatabaseException(msg)\r\ndbt.exceptions.DatabaseException: Database Error\r\n  001003 (42000): SQL compilation error:\r\n  syntax error line 1 at position 0 unexpected '0'.\r\n```\n\n### How to reproduce\n\nUse the statement() block described here:\r\nhttps://docs.getdbt.com/reference/dbt-jinja-functions/statement-blocks\r\n\r\n\n\n### Dialect\n\nSnowflake\n\n### Version\n\n1.2.0 with the dbt formatter\n\n### Configuration\n\n```\r\n[sqlfluff]\r\n# verbose is an integer (0-2) indicating the level of log output\r\nverbose = 2\r\n# Turn off color formatting of output\r\nnocolor = False\r\n# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\r\n# Or run 'sqlfluff dialects'\r\ndialect = snowflake\r\n# One of [raw|jinja|python|placeholder]\r\ntemplater = dbt\r\n# Comma separated list of rules to check, default to all\r\nrules = all\r\n# Comma separated list of rules to exclude, or None\r\nexclude_rules = None\r\n# The depth to recursively parse to (0 for unlimited)\r\nrecurse = 0\r\n# Below controls SQLFluff output, see max_line_length for SQL output\r\noutput_line_length = 80\r\n# Number of passes to run before admitting defeat\r\nrunaway_limit = 10\r\n# Ignore errors by category (one or more of the following, separated by commas: lexing,linting,parsing,templating)\r\nignore = None\r\n# Ignore linting errors found within sections of code coming directly from\r\n# templated code (e.g. from within Jinja curly braces. Note that it does not\r\n# ignore errors from literal code found within template loops.\r\nignore_templated_areas = True\r\n# can either be autodetect or a valid encoding e.g. utf-8, utf-8-sig\r\nencoding = autodetect\r\n# Ignore inline overrides (e.g. to test if still required)\r\ndisable_noqa = False\r\n# Comma separated list of file extensions to lint\r\n# NB: This config will only apply in the root folder\r\nsql_file_exts = .sql,.sql.j2,.dml,.ddl\r\n# Allow fix to run on files, even if they contain parsing errors\r\n# Note altering this is NOT RECOMMENDED as can corrupt SQL\r\nfix_even_unparsable = False\r\n# Very large files can make the parser effectively hang.\r\n# This limit skips files over a certain character length\r\n# and warns the user what has happened.\r\n# Set this to 0 to disable.\r\nlarge_file_skip_char_limit = 20000\r\n\r\n[sqlfluff:indentation]\r\n# See https://docs.sqlfluff.com/en/stable/indentation.html\r\nindented_joins = False\r\nindented_ctes = False\r\nindented_using_on = True\r\nindented_on_contents = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\nload_macros_from_path = macros/\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n[sqlfluff:templater:dbt]\r\nproject_dir = ./\r\n\r\n# Some rules can be configured directly from the config common to other rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 120\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# Some rules have their own specific config\r\n[sqlfluff:rules:L003]\r\nhanging_indents = True\r\n\r\n[sqlfluff:rules:L007]\r\noperator_new_lines = after\r\n\r\n[sqlfluff:rules:L010]\r\n# Keywords\r\ncapitalisation_policy = lower\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L011]\r\n# Aliasing preference for tables\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L012]\r\n# Aliasing preference for columns\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L014]\r\n# Unquoted identifiers\r\nextended_capitalisation_policy = lower\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L016]\r\n# Line length\r\nignore_comment_lines = False\r\nignore_comment_clauses = False\r\n\r\n[sqlfluff:rules:L027]\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L026]\r\n# References must be in FROM clause\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L028]\r\n# References must be consistently used\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]\r\n# Keywords should not be used as identifiers.\r\nunquoted_identifiers_policy = aliases\r\nquoted_identifiers_policy = none\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L030]\r\n# Function names\r\nextended_capitalisation_policy = lower\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L031]\r\n# Avoid table aliases in from clauses and join conditions.\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L036]\r\nwildcard_policy = single\r\n\r\n[sqlfluff:rules:L038]\r\n# Trailing commas\r\nselect_clause_trailing_comma = forbid\r\n\r\n[sqlfluff:rules:L040]\r\n# Null & Boolean Literals\r\ncapitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses\r\nforbid_subquery_in = join\r\n\r\n[sqlfluff:rules:L047]\r\n# Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n[sqlfluff:rules:L051]\r\n# Fully qualify JOIN clause\r\nfully_qualify_join_types = inner\r\n\r\n[sqlfluff:rules:L052]\r\n# Semi-colon formatting approach\r\nmultiline_newline = False\r\nrequire_final_semicolon = False\r\n\r\n[sqlfluff:rules:L054]\r\n# GROUP BY/ORDER BY column references\r\ngroup_by_and_order_by_style = consistent\r\n\r\n[sqlfluff:rules:L057]\r\n# Special characters in identifiers\r\nunquoted_identifiers_policy = all\r\nquoted_identifiers_policy = all\r\nallow_space_in_identifier = False\r\nadditional_allowed_characters = \"\"\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L059]\r\n# Policy on quoted and unquoted identifiers\r\nprefer_quoted_identifiers = False\r\nignore_words = None\r\nignore_words_regex = None\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L062]\r\n# Comma separated list of blocked words that should not be used\r\nblocked_words = None\r\nblocked_regex = None\r\n\r\n[sqlfluff:rules:L063]\r\n# Data Types\r\nextended_capitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L064]\r\n# Consistent usage of preferred quotes for quoted literals\r\npreferred_quoted_literal_style = consistent\r\n# Disabled for dialects that do not support single and double quotes for quoted literals (e.g. Postgres)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L066]\r\nmin_alias_length = None\r\nmax_alias_length = None\r\n```\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "This is a curious error. Can you run `dbt compile` and post what dbt expects the compiled form of this statement to be? I worry that while a query is run at compile time, this query otherwise compiles to an empty file - and that could be causing issues.\ndbt compile doesn't output the call statement blocks since they're interpreted at runtime; however, we can see the output ran on the snowflake query history.\r\n\r\nSource test.sql\r\n```\r\n{% call statement('variables', fetch_result=true) %}\r\n\r\n    select 1\r\n\r\n{% endcall %}\r\n\r\nwith source (\r\n    select 1\r\n)\r\n\r\nselect * from source\r\n```\r\n\r\nCompiled output of test.sql\r\n```\r\n\r\n\r\nwith source (\r\n    select 1\r\n)\r\n\r\nselect * from source\r\n```\nThe dbt [documentation](https://docs.getdbt.com/reference/dbt-jinja-functions/statement-blocks) mentions re: `statement()`:\r\n\r\n>Volatile API\r\n>While the statement and load_result setup works for now, we intend to improve this interface in the future. If you have questions or suggestions, please let us know in GitHub or on Slack.\r\n\r\nSo this might be a relatively lower priority issue. IIUC, it may also be dbt specific (not affecting the `jinja` templater).\r\n\r\n\nI did some preliminary investigation. IIUC, SQLFluff's `JinjaTracer` should treat this:\r\n```\r\n{% call statement('variables', fetch_result=true) %}\r\n\r\nselect 1 as test;\r\n\r\n{% endcall %}\r\n```\r\n\r\nlike this:\r\n```\r\n{{ statement('variables', fetch_result=true) }}\r\n```\r\n\r\nIn both cases, whatever `statement()` returns is passed through to the template output. I think this will be pretty straightforward, other than the usual trickiness of working on this complex area of the code.",
        "base": "e56fc6002dac0fb7eb446d58bd8aa7a839908535",
        "env": "388dd01e05c7dcb880165c7241ed4027d9d0171e",
        "files": [
            "src/sqlfluff/core/templaters/slicers/tracer.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4043",
        "problem": "add ability to render the compiled sql\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nIt would be nice to see the compiled sql in which any templates are rendered. I would be happy to work on this but it may be a struggle and would need some guidance.\n\n### Use case\n\n It would help debug linting errors around jinja templates.\r\nIt would also make it easier to copy and use the query in the bigquery ui, for example. We process our queries through Airflow so currently I can start a dag run and look at the rendered template to get this effect. That's not very efficient though :)\r\n\n\n### Dialect\n\nWe use bigquery but this could apply to all dialects.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "If you're interested in contributing this feature, I suggest starting by looking at the existing `parse` command, implemented [here](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/cli/commands.py#L938). I think this new command will be pretty similar except for the output format.\r\n\r\nI think `sqlfluff render` is a good name for it. \ud83d\udc4d\nIf you have questions, feel free to drop them here or ask in the \"contributing\" Slack channel.",
        "base": "5f639444ddf4afe8b4f0d2c7f0d4ac0b976930de",
        "env": "dc59c2a5672aacedaf91f0e6129b467eefad331b",
        "files": [
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/core/linter/linter.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2509",
        "problem": "noqa is ignored for jinja templated lines\n## Expected Behaviour\r\nLine with `noqa: TMP` should be ignored (despite of evaluation error)\r\n\r\n## Observed Behaviour\r\ntrying to lint airflow sql-template for AWS Athena query\r\nsetting up inline `-- noqa` or `--noqa: TMP` for jinja templated line not silenting templating error (typecasting error due to unable to pass datetime object while linting into template context):\r\n```\r\n== [transform/airflow/dags/queries/sfmc/player_balance.sql] FAIL\r\nL:   0 | P:   0 |  TMP | Unrecoverable failure in Jinja templating: unsupported operand type(s) for -: 'int' and 'datetime.timedelta'. Have you configured your variables?\r\n                       | https://docs.sqlfluff.com/en/latest/configuration.html\r\n```\r\n\r\n## Steps to Reproduce\r\ntemplated file:\r\n```sql\r\nselect *, row_number() over (partition by player_id order by balance_change_date desc)  as rnk\r\nfrom raw\r\nwhere\r\n    balance_change_date >= cast(from_iso8601_timestamp('{{ execution_date - macros.timedelta(hours=2, minutes=10) }}') as timestamp)  and  --noqa: TMP\r\n    balance_change_date < cast(from_iso8601_timestamp('{{ next_execution_date - macros.timedelta(minutes=10) }}') as timestamp) --noqa: TMP\r\n```\r\nrun:\r\n```bash\r\nsqlfluff lint transform/airflow/dags/queries/sfmc/player_balance.sql\r\n```\r\n\r\n## Dialect\r\npostgres (used for AWS Athena)\r\n\r\n## Version\r\ndatalake % sqlfluff --version\r\nsqlfluff, version 0.8.1\r\ndatalake % python3 --version\r\nPython 3.9.8\r\n\r\n## Configuration\r\n```ini\r\n# tox.ini\r\n[sqlfluff]\r\ntemplater = jinja\r\noutput_line_length = 180\r\nexclude_rules = L011,L012,L022,L031,L034\r\ndialect = postgres\r\n\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\n\r\n[sqlfluff:templater:jinja]\r\nlibrary_path = operation/deploy/lint\r\napply_dbt_builtins = false\r\n\r\n[sqlfluff:templater:jinja:context]\r\nds = 2021-11-11\r\nds_nodash = 20211111\r\nstart_date = 2021-11-11\r\nend_date = 2021-11-11\r\ninterval = 1\r\n# passed as int due to inabliity to pass datetime obkject \r\ndata_interval_start = 1636588800\r\ndata_interval_end = 1636588800\r\n```\r\n\r\n```python\r\n# operation/deploy/lint/macro.py\r\nfrom datetime import datetime, timedelta  # noqa: F401\r\n\r\nimport dateutil  # noqa: F401\r\n```\n",
        "hint": "As discussed on slack:\r\n\r\nChecking a few versions back your example has never worked.\r\nI think the templating ignoring is pretty basic (it's not included in our documentation).\r\n\r\nSo this works:\r\n\r\n```sql\r\nSELECT\r\n  {{ test }} --noqa: TMP\r\nFROM\r\n  table1\r\n```\r\n\r\nBut think anything beyond that simple use case, it struggles with.\r\n\r\nWill leave this issue open to see if it can be improved but for now the best solution is to defined that macro in the config (though I don't think dots in macros names are even supported in Jinja so not sure this is even possible?)",
        "base": "5c104a71c54ce4ed83401d62dfaaa86be38e5aff",
        "env": "a5c4eae4e3e419fe95460c9afd9cf39a35a470c4",
        "files": [
            "src/sqlfluff/core/linter/common.py",
            "src/sqlfluff/core/linter/linter.py",
            "src/sqlfluff/core/templaters/jinja.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/5206",
        "problem": "Exception thrown when SELECT DISTINCT not on the same line\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nCheck a file containing this request:\r\n\r\n```sql\r\nSELECT\r\n    DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\nIt fails this way:\r\n\r\n```log\r\nCRITICAL   [RF01] Applying rule RF01 to 'file.sql' threw an Exception:  \r\nTraceback (most recent call last):\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/rules/base.py\", line 864, in crawl\r\n    res = self._eval(context=context)\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 107, in _eval\r\n    self._analyze_table_references(\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 152, in _analyze_table_references\r\n    if not self._should_ignore_reference(r, selectable):\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 168, in _should_ignore_reference\r\n    ref_path = selectable.selectable.path_to(reference)\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 1184, in path_to\r\n    elif not self.get_start_loc() <= midpoint.get_start_loc() <= self.get_end_loc():\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 877, in get_start_loc\r\n    assert self.pos_marker\r\nAssertionError\r\n== [file.sql] FAIL\r\nL:   1 | P:   1 | LT09 | Select targets should be on a new line unless there is\r\n                       | only one select target. [layout.select_targets]\r\nL:   1 | P:   1 | LT10 | 'SELECT' modifiers (e.g. 'DISTINCT') must be on the same\r\n                       | line as 'SELECT'. [layout.select_modifiers]\r\nL:   1 | P:   1 | RF01 | Unexpected exception: ;\r\nCould you open an issue at\r\n                       | https://github.com/sqlfluff/sqlfluff/issues ?\r\nYou can\r\n                       | ignore this exception for now, by adding '-- noqa: RF01'\r\n                       | at the end\r\nof line 1\r\n [references.from]\r\nL:   2 | P:   1 | LT02 | Line should not be indented. [layout.indent]\r\nL:   3 | P:  13 | LT12 | Files must end with a single trailing newline.\r\n                       | [layout.end_of_file]\r\nAll Finished!\r\n```\r\n\r\nChecking the following request does not throw an exception (move `DISTINCT` on same line than `SELECT`):\r\n\r\n```sql\r\nSELECT DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\nAdditionally, I'd like to add that checking the first request on https://online.sqlfluff.com/fluffed leads to the same exception. But if you check this request:\r\n```sql\r\nSELECT \r\nDISTINCT\r\n`FIELD`\r\nFROM `TABLE`;\r\n```\r\nThen the website crashes.\r\n\r\n### Expected Behaviour\r\n\r\nI would expect not to have an exception.\r\n\r\n### Observed Behaviour\r\n\r\nAn exception was thrown whereas, I think, there is no reason to throw it.\r\n\r\n### How to reproduce\r\n\r\nCheck the following SQL:\r\n\r\n```sql\r\nSELECT\r\n    DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\n### Dialect\r\n\r\nMySQL\r\n\r\n### Version\r\n\r\n2.3.2\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\n# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\r\ndialect = mysql\r\nencoding = utf-8\r\n# Exclude rule LT01/layout.spacing: it expects a space even after type of fields (i.e. \"INT (11)\")\r\n# Exclude rule ST05/structure.subquery: MySQL badly supports CTEs.\r\nexclude_rules = LT01, ST05\r\nignore = parsing\r\nmax_line_length = 120\r\n# Below controls SQLFluff output, see max_line_length for SQL output\r\noutput_line_length = 80\r\ntemplater = raw\r\nverbose = 0\r\n\r\n[sqlfluff:layout:type:binary_operator]\r\nline_position = leading\r\n\r\n[sqlfluff:layout:type:comma]\r\nline_position = trailing\r\nspacing_before = touch\r\n\r\n[sqlfluff:indentation]\r\n# See https://docs.sqlfluff.com/en/stable/indentation.html\r\nindent_unit = space\r\nindented_joins = True\r\nindented_using_on = True\r\ntab_space_size = 4\r\n\r\n# Some rules can be configured directly from the config common to other rules\r\n[sqlfluff:rules]\r\nallow_scalar = True\r\nquoted_identifiers_policy = none\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n[sqlfluff:rules:aliasing.column]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:aliasing.table]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:ambiguous.column_references]\r\ngroup_by_and_order_by_style = consistent\r\n\r\n[sqlfluff:rules:capitalisation.functions]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.identifiers]\r\nextended_capitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.keywords]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.literals]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.types]\r\nextended_capitalisation_policy = upper\r\n\r\n[sqlfluff:rules:convention.count_rows]\r\nprefer_count_0 = False\r\nprefer_count_1 = True\r\n\r\n[sqlfluff:rules:convention.select_trailing_comma]\r\nselect_clause_trailing_comma = forbid\r\n\r\n[sqlfluff:rules:convention.terminator]\r\nmultiline_newline = False\r\nrequire_final_semicolon = True\r\n\r\n[sqlfluff:rules:layout.long_lines]\r\nignore_comment_lines = True\r\n\r\n[sqlfluff:rules:references.keywords]\r\nignore_words = None\r\nquoted_identifiers_policy = none\r\nunquoted_identifiers_policy = all\r\n\r\n[sqlfluff:rules:convention.quoted_literals]\r\npreferred_quoted_literal_style = single_quotes\r\n\r\n[sqlfluff:rules:references.quoting]\r\nprefer_quoted_identifiers = True\r\n\r\n[sqlfluff:rules:references.special_chars]\r\nadditional_allowed_characters = \"\"\r\nallow_space_in_identifier = False\r\nquoted_identifiers_policy = all\r\n# Special characters in identifiers\r\nunquoted_identifiers_policy = all\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\nException thrown when SELECT DISTINCT not on the same line\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nCheck a file containing this request:\r\n\r\n```sql\r\nSELECT\r\n    DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\nIt fails this way:\r\n\r\n```log\r\nCRITICAL   [RF01] Applying rule RF01 to 'file.sql' threw an Exception:  \r\nTraceback (most recent call last):\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/rules/base.py\", line 864, in crawl\r\n    res = self._eval(context=context)\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 107, in _eval\r\n    self._analyze_table_references(\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 152, in _analyze_table_references\r\n    if not self._should_ignore_reference(r, selectable):\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 168, in _should_ignore_reference\r\n    ref_path = selectable.selectable.path_to(reference)\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 1184, in path_to\r\n    elif not self.get_start_loc() <= midpoint.get_start_loc() <= self.get_end_loc():\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 877, in get_start_loc\r\n    assert self.pos_marker\r\nAssertionError\r\n== [file.sql] FAIL\r\nL:   1 | P:   1 | LT09 | Select targets should be on a new line unless there is\r\n                       | only one select target. [layout.select_targets]\r\nL:   1 | P:   1 | LT10 | 'SELECT' modifiers (e.g. 'DISTINCT') must be on the same\r\n                       | line as 'SELECT'. [layout.select_modifiers]\r\nL:   1 | P:   1 | RF01 | Unexpected exception: ;\r\nCould you open an issue at\r\n                       | https://github.com/sqlfluff/sqlfluff/issues ?\r\nYou can\r\n                       | ignore this exception for now, by adding '-- noqa: RF01'\r\n                       | at the end\r\nof line 1\r\n [references.from]\r\nL:   2 | P:   1 | LT02 | Line should not be indented. [layout.indent]\r\nL:   3 | P:  13 | LT12 | Files must end with a single trailing newline.\r\n                       | [layout.end_of_file]\r\nAll Finished!\r\n```\r\n\r\nChecking the following request does not throw an exception (move `DISTINCT` on same line than `SELECT`):\r\n\r\n```sql\r\nSELECT DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\nAdditionally, I'd like to add that checking the first request on https://online.sqlfluff.com/fluffed leads to the same exception. But if you check this request:\r\n```sql\r\nSELECT \r\nDISTINCT\r\n`FIELD`\r\nFROM `TABLE`;\r\n```\r\nThen the website crashes.\r\n\r\n### Expected Behaviour\r\n\r\nI would expect not to have an exception.\r\n\r\n### Observed Behaviour\r\n\r\nAn exception was thrown whereas, I think, there is no reason to throw it.\r\n\r\n### How to reproduce\r\n\r\nCheck the following SQL:\r\n\r\n```sql\r\nSELECT\r\n    DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\n### Dialect\r\n\r\nMySQL\r\n\r\n### Version\r\n\r\n2.3.2\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\n# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\r\ndialect = mysql\r\nencoding = utf-8\r\n# Exclude rule LT01/layout.spacing: it expects a space even after type of fields (i.e. \"INT (11)\")\r\n# Exclude rule ST05/structure.subquery: MySQL badly supports CTEs.\r\nexclude_rules = LT01, ST05\r\nignore = parsing\r\nmax_line_length = 120\r\n# Below controls SQLFluff output, see max_line_length for SQL output\r\noutput_line_length = 80\r\ntemplater = raw\r\nverbose = 0\r\n\r\n[sqlfluff:layout:type:binary_operator]\r\nline_position = leading\r\n\r\n[sqlfluff:layout:type:comma]\r\nline_position = trailing\r\nspacing_before = touch\r\n\r\n[sqlfluff:indentation]\r\n# See https://docs.sqlfluff.com/en/stable/indentation.html\r\nindent_unit = space\r\nindented_joins = True\r\nindented_using_on = True\r\ntab_space_size = 4\r\n\r\n# Some rules can be configured directly from the config common to other rules\r\n[sqlfluff:rules]\r\nallow_scalar = True\r\nquoted_identifiers_policy = none\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n[sqlfluff:rules:aliasing.column]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:aliasing.table]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:ambiguous.column_references]\r\ngroup_by_and_order_by_style = consistent\r\n\r\n[sqlfluff:rules:capitalisation.functions]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.identifiers]\r\nextended_capitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.keywords]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.literals]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.types]\r\nextended_capitalisation_policy = upper\r\n\r\n[sqlfluff:rules:convention.count_rows]\r\nprefer_count_0 = False\r\nprefer_count_1 = True\r\n\r\n[sqlfluff:rules:convention.select_trailing_comma]\r\nselect_clause_trailing_comma = forbid\r\n\r\n[sqlfluff:rules:convention.terminator]\r\nmultiline_newline = False\r\nrequire_final_semicolon = True\r\n\r\n[sqlfluff:rules:layout.long_lines]\r\nignore_comment_lines = True\r\n\r\n[sqlfluff:rules:references.keywords]\r\nignore_words = None\r\nquoted_identifiers_policy = none\r\nunquoted_identifiers_policy = all\r\n\r\n[sqlfluff:rules:convention.quoted_literals]\r\npreferred_quoted_literal_style = single_quotes\r\n\r\n[sqlfluff:rules:references.quoting]\r\nprefer_quoted_identifiers = True\r\n\r\n[sqlfluff:rules:references.special_chars]\r\nadditional_allowed_characters = \"\"\r\nallow_space_in_identifier = False\r\nquoted_identifiers_policy = all\r\n# Special characters in identifiers\r\nunquoted_identifiers_policy = all\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
        "hint": "\n",
        "base": "895e668a0047bd31fb87865fcab50d846bfb88ce",
        "env": "3625934f16857ade527f5f7dfa84b874061ea739",
        "files": [
            "src/sqlfluff/core/parser/segments/base.py",
            "src/sqlfluff/core/rules/base.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/1577",
        "problem": "\"ValueError: Position Not Found\" with macro spanning entire file\n## Expected Behaviour\r\n\r\n`sqlfluff parse` should probably not fail with an exception and stack trace.\r\n\r\n## Observed Behaviour\r\n\r\n`sqlfluff parse` throws an exception, given an input file which is entirely spanned by a Jinja macro.\r\n\r\n## Steps to Reproduce\r\n\r\n```console\r\n$ echo -n '{% macro foo() %}{% endmacro %}' | sqlfluff parse -\r\nTraceback (most recent call last):\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/bin/sqlfluff\", line 33, in <module>\r\n    sys.exit(load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')())\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/cli/commands.py\", line 701, in parse\r\n    lnt.parse_string(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 596, in parse_string\r\n    return self.parse_rendered(rendered, recurse=recurse)\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 294, in parse_rendered\r\n    tokens, lvs, config = cls._lex_templated_file(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 127, in _lex_templated_file\r\n    tokens, lex_vs = lexer.lex(templated_file)\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/parser/lexer.py\", line 319, in lex\r\n    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/parser/lexer.py\", line 346, in elements_to_segments\r\n    source_slice = templated_file.templated_slice_to_source_slice(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/templaters/base.py\", line 319, in templated_slice_to_source_slice\r\n    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/templaters/base.py\", line 214, in _find_slice_indices_of_templated_pos\r\n    raise ValueError(\"Position Not Found\")\r\nValueError: Position Not Found\r\n```\r\n\r\nNote: the issue does not occur if the file ends with a newline. \r\n\r\nThe contents of the macro also doesn't matter.\r\n\r\n## Dialect\r\n\r\nNone specified\r\n\r\n## Version\r\nSQLFluff 6011bdbe05669b075045e8127cdf18cc537686d4, Python 3.9.6\r\n\r\n## Configuration\r\n\r\nNone\n",
        "hint": "Hi @CyberShadow, @tunetheweb, what is expected output of\r\n```console\r\n$ echo -n '{% macro foo() %}{% endmacro %}' | sqlfluff parse -\r\n```\r\n?\r\n\nProbably the same as the input. Definitely not an exception, in any case.\r\n\r\nEdit: Whoops, forgot this was a `parse` case. What @tunetheweb said below, then.\nFor parse we don't return input.\r\n\r\nIf we add a newline we get this:\r\n\r\n```\r\n% echo -n '{% macro foo() %}{% endmacro %}\\n' | sqlfluff parse -\r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    [META] placeholder:                                       [Type: 'compound', Raw: '{% macro foo() %}{% endmacro %}']\r\n[L:  1, P: 32]      |    newline:                                                  '\\n'\r\n```\r\n\r\nSo I'd expect the first two lines to be returned if newline isn't given.\r\n\r\nHere's some \"equivalent\" non-SQL that doesn't fail:\r\n\r\n```\r\n% echo \" \" | sqlfluff parse -     \r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    whitespace:                                               ' '\r\n[L:  1, P:  2]      |    newline:                                                  '\\n'\r\n\r\n% echo \"\" | sqlfluff parse - \r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    newline:                                                  '\\n'\r\n\r\n% echo \"--test\" | sqlfluff parse -\r\n[L:  1, P:  1]      |file:\r\n[L:  1, P:  1]      |    comment:                                                  '--test'\r\n[L:  1, P:  7]      |    newline:                                                  '\\n'\r\n``\r\n\r\n\r\n",
        "base": "500505877769ab02504427284e4efdf832d299ea",
        "env": "67023b85c41d23d6c6d69812a41b207c4f8a9331",
        "files": [
            "src/sqlfluff/core/templaters/base.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2386",
        "problem": "Double backticks in Lint description\n![image](https://user-images.githubusercontent.com/80432516/150420352-57452c80-ad25-423b-8251-645e541579ad.png)\r\n(n.b. this affects a lot more rules than L051)\r\n\r\nThis was introduced in #2234 in which docstrings such as\r\n```\r\n`INNER JOIN` must be fully qualified.\r\n```\r\nwere replaced with \r\n```\r\n``INNER JOIN`` must be fully qualified.\r\n```\r\nso that they appear as code blocks in Sphinx for docs.\r\n![image](https://user-images.githubusercontent.com/80432516/150420294-eb9d3127-db1d-457c-a637-d614e0267277.png)\r\n\r\nHowever, our rules will use the first line of these docstrings in the event that no `description` is provided to the lint results.\r\n\r\nThis doesn't look great on the CLI so we should fix this. As far as I'm aware there are two approaches for this:\r\n1. Pass a `description` to all the `LintResult`s.\r\n2. Update the code that gets the default description from the docstring to do something like, replace the double backticks with a single one, or remove them, or do something clever like make them bold for the CLI and remove them for non-CLI.\r\n\r\nMy strong preference is number 2, but I'm open to discussion as to how exactly we do this \ud83d\ude04 \r\n\r\n@barrywhart @tunetheweb \n",
        "hint": "Number 2 sounds good to me!\n@barrywhart which variation?\nI would replace with single \"normal\" quotes: ' rather than \\`.\r\n\r\nThe clever approach could be cool for later, but I wouldn't try it now. I can't remember if we already handle detecting whether we're running in a terminal or not, because the techniques for doing bold or colored text don't work well when redirecting output to a file, etc.\n> The clever approach could be cool for later, but I wouldn't try it now. I can't remember if we already handle detecting whether we're running in a terminal or not, because the techniques for doing bold or colored text don't work well when redirecting output to a file, etc.\r\n\r\nYeah I think there's some `isatty` function we use in the formatter, but agree on the simple replace method for now \ud83d\ude04 ",
        "base": "23d698607b45b8469c766b521d27e9a6e92e8739",
        "env": "a5c4eae4e3e419fe95460c9afd9cf39a35a470c4",
        "files": [
            "src/sqlfluff/core/rules/base.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3411",
        "problem": "Update warning for parsing errors found on the ansi dialect\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nIn the past specifying a dialect was **optional**. If unspecified, the dialect defaulted to `ansi`. Because of this there is a warning presented when sqlfluff runs in parse mode and the dialect is set to ansi and parsing errors are encountered.\r\n\r\n`WARNING: Parsing errors found and dialect is set to 'ansi'. Have you configured your dialect?`\r\n\r\nCurrently, specifying a dialect is **mandatory**. Therefore this warning is perhaps not needed... and certainly not needed in its current form.\r\n\r\nI opened this issue to document the idea and solicit feedback. \r\n1. The simplest improvement to make the message more appropriate is to just change it to this:\r\n\r\n`WARNING: Parsing errors found and dialect is set to 'ansi'. Is 'ansi' the correct dialect?`\r\n\r\n2. On the other hand, we know that the user explicitly set the dialect to `ansi`. So why bother asking if it was intentional? We don't ask if you meant postgres or tsql. There's an argument to simply remove the warning altogether.\r\n\r\n3. Finally, we could potentially differentiate between `--dialect ansi` passed on the command line vs the dialect being picked up from a `.sqlfluff` config file. Perhaps the warning should be displayed only the in the case where the dialect was picked up implicitly from the config file.\n\n### Use case\n\n_No response_\n\n### Dialect\n\nansi\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "Pending feedback from the admins, I'm happy to submit a pull request on this one.\nBeen thinking about it, and I think we should show the message for all dialects. Some people might be using an incorrect dialect if their dialect is not supported.",
        "base": "e60272a859e37e335088ae79a7ad59ea771545a1",
        "env": "6e8ce43a4958dbaa56256365c2a89d8db92e07d6",
        "files": [
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/cli/formatters.py",
            "src/sqlfluff/core/linter/linter.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2641",
        "problem": "L045:  Unused CTEs are not automatically detected when using jinja/dbt as a templater\n## Expected Behaviour\r\nWhen unused CTEs are used with jinja or dbt as a templater, these are detected by L045. \r\n\r\n## Observed Behaviour\r\nWhen ref() statements are included in a SQL file and dbt is used as a templater, these seem to interfere with the ability for rule L045 to detect the unused CTEs.  The same behavior is observed when Jinja is included under the \"FROM\" statement of the relevant queries.\r\n\r\n## Steps to Reproduce\r\n(1). Generate a valid dbt project with at least two models with one variable each.  For the purposes of this reproduction example, I am going to assume that one model is 'foo' with variable 'var_foo' and one model is 'bar' with variable 'var_bar'.\r\n \r\n(2) Using DBT as a templater and BigQuery as a dialect, run dbt lint on the following SQL file:\r\n\r\n```sql\r\nWITH\r\nrandom_gibberish AS (\r\n    SELECT var_foo\r\n    FROM\r\n        {{ ref('foo') }}\r\n)\r\n\r\nSELECT var_bar\r\nFROM\r\n    {{ ref('bar') }}\r\n```\r\n\r\nIf the templater is switched to Jinja, L045 again doesn't produce any errors.\r\n\r\n## Dialect\r\nBigquery\r\n\r\n## Version\r\nSQLFluff version is 0.10.0.  Python version is 3.8.10.\r\nI'm using dbt 1.0.1 but the same issue occurs when Jinja is used as a templater.\r\n\r\n## Configuration\r\n```\r\n[sqlfluff]\r\ndialect = bigquery\r\nexclude_rules = L003,L008,L011,L014,L016,L029,L031,L034\r\n\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\ncomma_style = leading\r\n\r\n[sqlfluff:rules:L010]\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L030]\r\ncapitalisation_policy = upper\r\n```\n",
        "hint": "",
        "base": "b0ad239095b3cefb294c1b12c73c41e9f229aa81",
        "env": "a4dcf3f08d95cbde4efb39969b0ab8e33a791f21",
        "files": [
            "src/sqlfluff/core/rules/analysis/select_crawler.py",
            "src/sqlfluff/rules/L045.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4777",
        "problem": "`fix` per file linted instead of at the end\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nI am just testing sqlfluff on a small example project.\r\nWe have configured it as part of `pre-commit`.\r\n\r\n```\r\n-   repo: https://github.com/sqlfluff/sqlfluff\r\n    rev: 1.0.0\r\n    hooks:\r\n    -   id: sqlfluff-fix\r\n        args: [--config, \".sqlfluff\", --disable_progress_bar, --processes, \"2\", --bench]\r\n        files: \\.(sql)$\r\n        exclude: sp_whoisactive.sql\r\n```\r\n\r\nProcessing our example already takes 30 minutes, I thus think formatting any real project would take 4+ hours.\r\n\r\nAt the moment the files are all formated first and _all together_ written at the very end. I see no benefit in writing at the very end, why are they not written sequentially?\n\n### Use case\n\nInstead of writing all formatted sql at the end, I would like to see files written sequentially.\n\n### Dialect\n\nmost likely all, i am working with t-sql.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n`fix` per file linted instead of at the end\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nI am just testing sqlfluff on a small example project.\r\nWe have configured it as part of `pre-commit`.\r\n\r\n```\r\n-   repo: https://github.com/sqlfluff/sqlfluff\r\n    rev: 1.0.0\r\n    hooks:\r\n    -   id: sqlfluff-fix\r\n        args: [--config, \".sqlfluff\", --disable_progress_bar, --processes, \"2\", --bench]\r\n        files: \\.(sql)$\r\n        exclude: sp_whoisactive.sql\r\n```\r\n\r\nProcessing our example already takes 30 minutes, I thus think formatting any real project would take 4+ hours.\r\n\r\nAt the moment the files are all formated first and _all together_ written at the very end. I see no benefit in writing at the very end, why are they not written sequentially?\n\n### Use case\n\nInstead of writing all formatted sql at the end, I would like to see files written sequentially.\n\n### Dialect\n\nmost likely all, i am working with t-sql.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "This is actually the same as #427 \nThis is actually the same as #427 ",
        "base": "0243d4a1ba29e6cc3dc96bd9ea178d0f8a576a8f",
        "env": "d19de0ecd16d298f9e3bfb91da122734c40c01e5",
        "files": [
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/core/linter/linted_dir.py",
            "src/sqlfluff/core/linter/linted_file.py",
            "src/sqlfluff/core/linter/linter.py",
            "src/sqlfluff/core/linter/linting_result.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/1517",
        "problem": "\"Dropped elements in sequence matching\" when doubled semicolon\n## Expected Behaviour\r\nFrankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.\r\n## Observed Behaviour\r\n```console\r\n(.venv) ?master ~/prod/_inne/sqlfluff> echo \"select id from tbl;;\" | sqlfluff lint -\r\nTraceback (most recent call last):\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff\", line 11, in <module>\r\n    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py\", line 347, in lint\r\n    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 789, in lint_string_wrapped\r\n    linted_path.add(self.lint_string(string, fname=fname, fix=fix))\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 668, in lint_string\r\n    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 607, in parse_string\r\n    return self.parse_rendered(rendered, recurse=recurse)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 313, in parse_rendered\r\n    parsed, pvs = cls._parse_tokens(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 190, in _parse_tokens\r\n    parsed: Optional[BaseSegment] = parser.parse(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py\", line 32, in parse\r\n    parsed = root_segment.parse(parse_context=ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py\", line 821, in parse\r\n    check_still_complete(segments, m.matched_segments, m.unmatched_segments)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py\", line 30, in check_still_complete\r\n    raise RuntimeError(\r\nRuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'\r\n\r\n```\r\n## Steps to Reproduce\r\nRun \r\n```console\r\necho \"select id from tbl;;\" | sqlfluff lint -\r\n```\r\n## Dialect\r\ndefault (ansi)\r\n## Version\r\n```\r\nsqlfluff, version 0.6.6\r\nPython 3.9.5\r\n```\r\n## Configuration\r\nNone\r\n\n",
        "hint": "Sounds similar to #1458 where we should handle \"empty\" statement/files better?\nNope, that's the different issue. I doubt that solving one of them would help in other one. I think both issues should stay, just in the case.\nBut what do you think @tunetheweb - should it just ignore these `;;` or raise something like `Found unparsable section:`? \nJust tested and in BigQuery it's an error.\r\nInterestingly Oracle is fine with it.\r\n\r\nI think it should be raised as `Found unparsable section`.",
        "base": "304a197829f98e7425a46d872ada73176137e5ae",
        "env": "67023b85c41d23d6c6d69812a41b207c4f8a9331",
        "files": [
            "src/sqlfluff/core/parser/helpers.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/880",
        "problem": "Rename BaseCrawler class as BaseRule to be clearer, avoid confusion with analysis helper classes, e.g. SelectCrawler\nDiscussed here:\r\nhttps://github.com/sqlfluff/sqlfluff/pull/779#pullrequestreview-604167034\r\n\n",
        "hint": "Yes. The fact that it's called a crawler is a historic artifact when I thought that the crawlers and rules might be separate. Given they're not, this makes total sense.",
        "base": "12f272627cd263f2acadf2ad976aec4819ee98c3",
        "env": "cbdcfb09feb4883de91de142956c3be6ac7f827d",
        "files": [
            "plugins/sqlfluff-plugin-example/src/example/rules.py",
            "src/sqlfluff/core/linter.py",
            "src/sqlfluff/core/rules/base.py",
            "src/sqlfluff/core/rules/std/L001.py",
            "src/sqlfluff/core/rules/std/L002.py",
            "src/sqlfluff/core/rules/std/L003.py",
            "src/sqlfluff/core/rules/std/L004.py",
            "src/sqlfluff/core/rules/std/L005.py",
            "src/sqlfluff/core/rules/std/L006.py",
            "src/sqlfluff/core/rules/std/L007.py",
            "src/sqlfluff/core/rules/std/L008.py",
            "src/sqlfluff/core/rules/std/L009.py",
            "src/sqlfluff/core/rules/std/L010.py",
            "src/sqlfluff/core/rules/std/L011.py",
            "src/sqlfluff/core/rules/std/L013.py",
            "src/sqlfluff/core/rules/std/L015.py",
            "src/sqlfluff/core/rules/std/L017.py",
            "src/sqlfluff/core/rules/std/L018.py",
            "src/sqlfluff/core/rules/std/L019.py",
            "src/sqlfluff/core/rules/std/L020.py",
            "src/sqlfluff/core/rules/std/L021.py",
            "src/sqlfluff/core/rules/std/L022.py",
            "src/sqlfluff/core/rules/std/L023.py",
            "src/sqlfluff/core/rules/std/L029.py",
            "src/sqlfluff/core/rules/std/L031.py",
            "src/sqlfluff/core/rules/std/L032.py",
            "src/sqlfluff/core/rules/std/L033.py",
            "src/sqlfluff/core/rules/std/L034.py",
            "src/sqlfluff/core/rules/std/L035.py",
            "src/sqlfluff/core/rules/std/L036.py",
            "src/sqlfluff/core/rules/std/L037.py",
            "src/sqlfluff/core/rules/std/L038.py",
            "src/sqlfluff/core/rules/std/L039.py",
            "src/sqlfluff/core/rules/std/L041.py",
            "src/sqlfluff/core/rules/std/L042.py",
            "src/sqlfluff/core/rules/std/L043.py",
            "src/sqlfluff/core/rules/std/L044.py",
            "src/sqlfluff/core/rules/std/L045.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4041",
        "problem": "Deduplicate violations in the same position\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nWhen linting jinja files with loops we get multiple output violations for each time around the loop. e.g.\r\n\r\n```sql\r\nselect\r\n    a,\r\n    {% for val in [1, 2, 3, 4, 5, 6] %}\r\n        d+ {{ val }},\r\n    {% endfor %}\r\n    b\r\n```\r\n\r\nwe get\r\n\r\n```\r\n== [test.sql] FAIL\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   7 | P:   1 | L001 | Unnecessary trailing whitespace.\r\n```\r\n\r\nThe duplicated `Missing whitespace` isn't helpful for the user. Regardless of whether we keep them in the background (perhaps we should), they shouldn't be shown to the user here because we're showing the same issue multiple times.\n\n### Use case\n\nCLI linting\n\n### Dialect\n\nall\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "",
        "base": "66fd602d0824138f212082c8fdf381266a9edad3",
        "env": "dc59c2a5672aacedaf91f0e6129b467eefad331b",
        "files": [
            "src/sqlfluff/core/errors.py",
            "src/sqlfluff/core/linter/linted_file.py",
            "src/sqlfluff/core/linter/linter.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4834",
        "problem": "Running `lint` on an empty file fails with critical Exception\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nThis is a bit of an odd one. When running `sqlfluff lint` on an empty file it fails with (Python) exception.\r\n\r\nWhile trying to lint empty file is probably not the main use-case for SQLFluff I still consider this somewhat relevant, when applying SQLFluff in a dynamic code base. \n\n### Expected Behaviour\n\nI'm not entirely sure what the correct result is. Feasible option are\r\n\r\n- Passing\r\n- Raise some kind of lint error (but not a critical exception)\r\n\r\nMy personal take is that lint should pass, which (I think) is similar behaviour to other linters.\n\n### Observed Behaviour\n\n`LT01` and `LT12` with an critical Exception\r\n\r\n```\r\nCRITICAL   [LT01] Applying rule LT01 to 'stdin' threw an Exception: ReflowSequence has empty elements.\r\nCRITICAL   [LT12] Applying rule LT12 to 'stdin' threw an Exception: tuple index out of range\r\n```\r\n\n\n### How to reproduce\n\n```sh\r\ncat /dev/null | sqlfluff lint --dialect ansi -\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\nlatest main branch\r\n\r\n```\r\ngit rev-parse HEAD\r\nd19de0ecd16d298f9e3bfb91da122734c40c01e5\r\n```\n\n### Configuration\n\ndefault\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\nRunning `lint` on an empty file fails with critical Exception\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nThis is a bit of an odd one. When running `sqlfluff lint` on an empty file it fails with (Python) exception.\r\n\r\nWhile trying to lint empty file is probably not the main use-case for SQLFluff I still consider this somewhat relevant, when applying SQLFluff in a dynamic code base. \n\n### Expected Behaviour\n\nI'm not entirely sure what the correct result is. Feasible option are\r\n\r\n- Passing\r\n- Raise some kind of lint error (but not a critical exception)\r\n\r\nMy personal take is that lint should pass, which (I think) is similar behaviour to other linters.\n\n### Observed Behaviour\n\n`LT01` and `LT12` with an critical Exception\r\n\r\n```\r\nCRITICAL   [LT01] Applying rule LT01 to 'stdin' threw an Exception: ReflowSequence has empty elements.\r\nCRITICAL   [LT12] Applying rule LT12 to 'stdin' threw an Exception: tuple index out of range\r\n```\r\n\n\n### How to reproduce\n\n```sh\r\ncat /dev/null | sqlfluff lint --dialect ansi -\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\nlatest main branch\r\n\r\n```\r\ngit rev-parse HEAD\r\nd19de0ecd16d298f9e3bfb91da122734c40c01e5\r\n```\n\n### Configuration\n\ndefault\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "I'll pick up this one - I've been in the \"empty\" file code recently and might have influenced this outcome.\nI'll pick up this one - I've been in the \"empty\" file code recently and might have influenced this outcome.",
        "base": "e884df2d00473a6ab166cb92a68d0b500a89d159",
        "env": "3629c3e702939c07264cc5ea903566ddc9ea2bb0",
        "files": [
            "src/sqlfluff/rules/layout/LT12.py",
            "src/sqlfluff/utils/reflow/sequence.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/5170",
        "problem": "[EXPERIMENT]: Rethink Matching routines\nThis is another experiment, and also a biggie. It's a rethink of matching as part of #5124.\r\n\r\nThis will need some tidying to get it into a state that it's reviewable, but given the scale of it - I think I shouldn't take it much further without getting some of it merged.\r\n\r\nIt's mostly additions for now, so I now need to strip out the things that we can get rid of as a result. Opening PR for testing and in particular for coverage.\n",
        "hint": "This is far too big, I'll divide it up into pieces before merging. I think other things need to come first.",
        "base": "d5a0ba0838b9058d815e5376241c96b0eb48e889",
        "env": "3625934f16857ade527f5f7dfa84b874061ea739",
        "files": [
            "src/sqlfluff/core/parser/grammar/base.py",
            "src/sqlfluff/core/parser/grammar/greedy.py",
            "src/sqlfluff/core/parser/grammar/sequence.py",
            "src/sqlfluff/core/parser/match_algorithms.py",
            "src/sqlfluff/core/parser/match_logging.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3066",
        "problem": "Jinja: sqlfluff fails in the presence of assignments with multiple targets\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\nI did search, and I think this _may_ be related, but since no more information was provided I cannot confirm it: https://github.com/sqlfluff/sqlfluff/issues/2947. For this reason, I opened a new issue.\r\n\r\n### What Happened\r\n\r\nJinja templates support multiple targets in [assignments](https://jinja.palletsprojects.com/en/3.0.x/templates/#assignments). However, `sqlfluff` fails to lint a file in the presence of an assignment with multiple targets.\r\n\r\nI traced this back to the `update_inside_set_or_macro` function, specifically [this line](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L244=).\r\n\r\nThe way `sqlfluff` is determining whether we are inside a [block assignment](https://jinja.palletsprojects.com/en/3.0.x/templates/#block-assignments) is by checking for the presence of an equals in the second index of the trimmed parts of the current raw slice:\r\n\r\n```python\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[2] != \"=\":\r\n```\r\n\r\nThis condition is false for single target assignments:\r\n\r\n```sql\r\n{% set a = 1 %}\r\n```\r\n\r\nWhich produce the expected trimmed parts (with spaces removed as in [line 243](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L243=)):\r\n\r\n```python\r\n['set', 'a', '=', '1']\r\n#             2    \r\n```\r\n\r\nHowever, with multiple targets:\r\n\r\n```sql\r\n{% set a, b = 1, 2 %}\r\n```\r\n\r\n```python\r\n['set', 'a', ',', 'b', '=', '1', '2']\r\n#                       4    \r\n```\r\n\r\nEquals is no longer in the index 2, but has been bumped to index 4, yet we are not in the expanded block form of set assignments. This causes the `inside_set_or_macro` flag to be incorrectly set to `True`, as if we were using a block assignment, which causes the entire template to be ignored (or something like that), and leads to the eventual `ValueError` raised.\r\n\r\nI played around a bit with potential solutions: first, I tried incrementing the index of the equals by the number of commas:\r\n\r\n```python\r\nequals_index = 2 + sum((c == ',' for c in  filtered_trimmed_parts))\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[equals_index] != \"=\":\r\n```\r\n\r\nHowever, this would bring issues if using the expanded form of set assignments with any commas in it, or in the presence of an uneven number of commas on both sides of the assignment.\r\n\r\nAnother simpler option would be to check for the presence of a single equals:\r\n\r\n```python\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts.count(\"=\") != 1:\r\n```\r\n\r\nThis one seems more promising, specially considering that multiple targets appear not to be supported with block assignments (at least, that's what I think, as the docs don't mention it, and trying it locally raises a too many values to unpack error). Thus, the first condition will always be true for block assignments (so, even the presence of an equals in the body of the assignment would not cause issues).\r\n\r\n### Expected Behaviour\r\n\r\nsqlfluff should lint files properly, even in the presence of assignments with multiple targets.\r\n\r\n### Observed Behaviour\r\n\r\nLinting fails when an exception is raised:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10dbt/bin/sqlfluff\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/cli/commands.py\", line 541, in lint\r\n    result = lnt.lint_paths(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 1098, in lint_paths\r\n    self.lint_path(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 1050, in lint_path\r\n    for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 101, in run\r\n    for fname, partial in self.iter_partials(fnames, fix=fix):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 54, in iter_partials\r\n    for fname, rendered in self.iter_rendered(fnames):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 43, in iter_rendered\r\n    yield fname, self.linter.render_file(fname, self.config)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 771, in render_file\r\n    return self.render_string(raw_file, fname, config, encoding)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 742, in render_string\r\n    templated_file, templater_violations = self.templater.process(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/templaters/jinja.py\", line 394, in process\r\n    TemplatedFile(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/templaters/base.py\", line 94, in __init__\r\n    raise ValueError(\"Cannot instantiate a templated file unsliced!\")\r\nValueError: Cannot instantiate a templated file unsliced!\r\n```\r\n\r\n### How to reproduce\r\n\r\n1. Save the following template to `model.sql` in an empty directory:\r\n```sql\r\n{% set a, b = 1, 2 %}\r\n\r\nSELECT {{ a }}\r\n```\r\n2. Run `sqlfluff lint model.sql --dialect 'postgres'`\r\n\r\n\r\n### Dialect\r\n\r\nTried with postgres and redshift dialects, however I think others may be affected as long as they use jinja templates.\r\n\r\n### Version\r\n\r\nv0.12.0\r\n\r\n### Configuration\r\n\r\nNothing, ran from an empty directory.\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
        "hint": "Heh, I didn't know this syntax was possible, but not surprising since it's valid in Python itself.\r\n\r\nI have wondered if we could potentially run the Python parser on the text inside the brackets. Jinja syntax is mostly the same as Python.\nPerhaps we can leverage [this](https://github.com/pallets/jinja/blob/main/src/jinja2/parser.py#L223L232) from the Jinja parser, either calling it directly or mimicking its behavior.\r\n\r\nNote that (IIUC) it returns an `Assign` object if it's a standalone tag or an `AssignBlock` if it's part of a set/endset pair.\r\n\r\n```\r\n    def parse_set(self) -> t.Union[nodes.Assign, nodes.AssignBlock]:\r\n        \"\"\"Parse an assign statement.\"\"\"\r\n        lineno = next(self.stream).lineno\r\n        target = self.parse_assign_target(with_namespace=True)\r\n        if self.stream.skip_if(\"assign\"):\r\n            expr = self.parse_tuple()\r\n            return nodes.Assign(target, expr, lineno=lineno)\r\n        filter_node = self.parse_filter(None)\r\n        body = self.parse_statements((\"name:endset\",), drop_needle=True)\r\n        return nodes.AssignBlock(target, filter_node, body, lineno=lineno)\r\n```",
        "base": "2b93a26e6f15129fd1846bee52f51077eef7ca0c",
        "env": "8f6fd1d8a8d69b2c463fbcf5bd1131c47f12ad88",
        "files": [
            "src/sqlfluff/core/templaters/slicers/tracer.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2625",
        "problem": "Suppress dbt logs and warnings when using --format github-annotation\nSometimes, running:\r\n```\r\nsqlfluff lint --format github-annotation --annotation-level failure --nofail \r\n```\r\n\r\nCan result in the first couple of output lines being logs which break the annotations, for example:\r\n```\r\n14:21:42  Partial parse save file not found. Starting full parse.\r\nWarning:  [WARNING]: Did not find matching node for patch with name 'xxxx' in the 'models' section of file 'models/production/xxxxx/xxxxx.yml'\r\n```\r\n\r\n## Version\r\ndbt 1.0.0, SQLFLuff 0.9.0\r\n\n",
        "hint": "my workaround was to add `sed -i '/^\\[/!d' annotations.json` to the git actions command to delete the extra lines from dbt that were not part of the annotations beginning with `[`\nPerhaps the better solution here is to add an ability for SQLFluff to write an annotations.json file itself with a command like\r\n```\r\nsqlfluff lint --format github-annotation --annotation-level failure --nofail ${{ steps.get_files_to_lint.outputs.lintees }} --write-output annotations.json\r\n```\r\nwhich would still allow the user to see log outputs, rather than the user having to stream the logs into a file with:\r\n\r\n```\r\nsqlfluff lint --format github-annotation --annotation-level failure --nofail ${{ steps.get_files_to_lint.outputs.lintees }} > annotations.json\r\n```\nRelates to https://github.com/sqlfluff/sqlfluff-github-actions/issues/15\n@NiallRees: That sounds like a great suggestion -- I had the same thought. I was the original author of the `github-annotation` format, and it seemed natural to add it to the existing list of formats. TBH, of the 4 formats, only one is intended for humans. If we make this change, I suggest we consider changing all the formats to support this.",
        "base": "d44c83e7cee923869e3ca6149da4e6d1ad0286eb",
        "env": "a4dcf3f08d95cbde4efb39969b0ab8e33a791f21",
        "files": [
            "src/sqlfluff/cli/commands.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/891",
        "problem": "Add \"enable\" and \"disable\" syntax to noqa to allow rules disabling across multiple lines\nSee the `pylint` docs for an example: https://docs.pylint.org/en/1.6.0/faq.html#is-it-possible-to-locally-disable-a-particular-message\n",
        "hint": "",
        "base": "bcc986e7d217f017130385b89cbda837f3e650ac",
        "env": "cbdcfb09feb4883de91de142956c3be6ac7f827d",
        "files": [
            "src/sqlfluff/core/errors.py",
            "src/sqlfluff/core/linter.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/2573",
        "problem": "Configuration from current working path not being loaded when path provided.\nI have the following directory structure.\r\n```\r\n~/GitHub/sqlfluff-bug\r\n\u279c  tree -a\r\n.\r\n\u251c\u2500\u2500 .sqlfluffignore\r\n\u251c\u2500\u2500 ignore_me_1.sql\r\n\u251c\u2500\u2500 path_a\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ignore_me_2.sql\r\n\u2514\u2500\u2500 path_b\r\n    \u251c\u2500\u2500 ignore_me_3.sql\r\n    \u2514\u2500\u2500 lint_me_1.sql\r\n\r\n2 directories, 5 files\r\n```\r\nAnd the following ignore file\r\n\r\n```\r\n~/GitHub/sqlfluff-bug\r\n\u279c  cat .sqlfluffignore\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  cat .sqlfluffignore\r\nignore_me_1.sql\r\npath_a/\r\npath_b/ignore_me_3.sql%\r\n```\r\n\r\nWhen I run the following I get the expected result. Sqlfluff only lints the one file that is not ignored.\r\n```\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint .\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint .\r\n== [path_b/lint_me_1.sql] FAIL\r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\r\nL:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\r\n```\r\n\r\nHowever when I run the lint explicitly on one of the two directories then ignored files are also linted.\r\n\r\n```\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint path_a\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint path_a\r\n== [path_a/ignore_me_2.sql] FAIL\r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\r\nL:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint path_b\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint path_b\r\n== [path_b/ignore_me_3.sql] FAIL\r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\r\nL:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\r\n== [path_b/lint_me_1.sql] FAIL\r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\r\nL:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\r\n```\r\n\r\nIf this is the expected behaviour then it might be worthwhile to add an example to the [docs](https://docs.sqlfluff.com/en/latest/configuration.html#sqlfluffignore).\r\n\r\nEdit: I've replicated this issue on sqlfluff version 0.3.2 to 0.3.6.\n",
        "hint": "This is currently functioning as expected in that it only looks for `.sqlfluffignore` files within the directories you specify. So if you point sqlfluff at `/path_b`, it would only looks for a `.sqlfluffignore` file at `/path_b/.sqlfluffignore` and any child directories of that. It won't check in parents of the given file.\r\n\r\nI think that's the expected behavior consistent with `.dockerignore` and `.gitignore` .\r\n\r\nI agree about clarifying the documentation, which uses the phrase `placed in the root of your project` which I think alone is misleading.\nI think the behavior described in this issue is desirable.\r\n\r\nFor CI with pre-commit for example, right now I would need to add a `.sqlfluffignore` to each sub-directory containing sql files I want to ignore. That's because pre-commit will give the full path pointing to each file that changed before commit.\r\n\r\nI'm not sure the behavior is consistent with `.gitignore` because the \"project root\" stays the same and `.gitignore` files are applied from top level down to the subdirectory of each file, while in `sqlfluff` we don't really have a project root, which I think could come from a new configuration in `.sqlfluff` (or we could assume `cwd` if it's parent directory of the file we're trying to lint).\nI've just hit this issue myself (but for configuration file) and I agree with @dmateusp on this one.\r\n\r\nI think this is more than a documentation issue.\r\n\r\nFrom putting together the initial configuration code, the config loader, *should* check the current working directory for config loading, but it feels like that isn't working right now.",
        "base": "8822bf4d831ccbf6bc63a44b34978daf6939d996",
        "env": "a4dcf3f08d95cbde4efb39969b0ab8e33a791f21",
        "files": [
            "src/sqlfluff/core/config.py",
            "src/sqlfluff/core/linter/linter.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3109",
        "problem": "Write-output human format does not produce result\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen running SQLFluff using the following statement:\r\n`python -m sqlfluff lint --write-output test.txt --config=config/sql-lint.cfg`\r\nno result was produced.\n\n### Expected Behaviour\n\nI expect a file to appear, in this case called test,txt, containing all violations found.\n\n### Observed Behaviour\n\nLooking through the code I saw human was the default format so expected adding --format=human would not make a difference. To be sure, I also ran the statement using the flag and it still produced nothing.\r\n\r\nTo make sure it was just the human format which was having problems, I also executed the statement using --format=json,yaml,github-annotations, all of which did produce the expected result which leads me to believe there is something wrong with the human format.\n\n### How to reproduce\n\nI imagine simply executing `sqlfluff lint --write-output test.txt example.sql`\n\n### Dialect\n\nT-SQL\n\n### Version\n\n0.11.2\n\n### Configuration\n\n[sqlfluff]\r\ndialect = tsql\r\nexclude_rules = L014,\r\n                L016,\r\n                L031,\r\n                L035,\r\n                L059\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "This line in cli.py seems largely to blame -- it somewhat conflates output _format_ with writing to a file or not.\r\n```\r\nnon_human_output = (format != FormatType.human.value) or (write_output is not None)\r\n```\r\n\r\nIt will require some care to fix this. Simply removing `or (write_output is not None)` didn't seem to fix it.\r\n\r\nAs a workaround until this is fixed, you may be able to use output redirection, e.g.\r\n```\r\npython -m sqlfluff lint --config=config/sql-lint.cfg > test.txt\r\n```\r\n\r\n\nYour workaround does work for me, thank you. Seeing as this solution is only a workaround I imagine closing the ticket is not preferable.",
        "base": "38c31c99f6be6c2ffcc9e4132387edb7af8c5d06",
        "env": "8f6fd1d8a8d69b2c463fbcf5bd1131c47f12ad88",
        "files": [
            "src/sqlfluff/cli/commands.py",
            "src/sqlfluff/cli/formatters.py",
            "src/sqlfluff/cli/outputstream.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/1763",
        "problem": "dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file\n_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._\r\n\r\n## Expected Behaviour\r\nViolation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.\r\n\r\n## Observed Behaviour\r\nReported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\\u2192' in position 120: character maps to <undefined>`\r\n\r\n## Steps to Reproduce\r\nSQL file:\r\n```sql\r\nSELECT\r\n    reacted_table_name_right.descendant_id AS category_id,\r\n    string_agg(redacted_table_name_left.name, ' \u2192 ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa\r\nFROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left\r\nINNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right\r\n    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id\r\nGROUP BY reacted_table_name_right.descendant_id\r\n```\r\nRunning `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.\r\n\r\n## Dialect\r\n`postgres`, with `dbt` templater\r\n\r\n## Version\r\n`python 3.7.12`\r\n`sqlfluff 0.7.0`\r\n`sqlfluff-templater-dbt 0.7.0`\r\n\r\n## Configuration\r\nI've tried a few, here's one:\r\n```\r\n[sqlfluff]\r\nverbose = 2\r\ndialect = postgres\r\ntemplater = dbt\r\nexclude_rules = None\r\noutput_line_length = 80\r\nrunaway_limit = 10\r\nignore_templated_areas = True\r\nprocesses = 3\r\n# Comma separated list of file extensions to lint.\r\n\r\n# NB: This config will only apply in the root folder.\r\nsql_file_exts = .sql\r\n\r\n[sqlfluff:indentation]\r\nindented_joins = False\r\nindented_using_on = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n# Common config across rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nindent_unit = space\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# L001 - Remove trailing whitespace (fix)\r\n# L002 - Single section of whitespace should not contain both tabs and spaces (fix)\r\n# L003 - Keep consistent indentation (fix)\r\n# L004 - We use 4 spaces for indentation just for completeness (fix)\r\n# L005 - Remove space before commas (fix)\r\n# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)\r\n\r\n# L007 - Operators should not be at the end of a line\r\n[sqlfluff:rules:L007]  # Keywords\r\noperator_new_lines = after\r\n\r\n# L008 - Always use a single whitespace after a comma (fix)\r\n# L009 - Files will always end with a trailing newline\r\n\r\n# L010 - All keywords will use full upper case (fix)\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n# L011 - Always explicitly alias tables (fix)\r\n[sqlfluff:rules:L011]  # Aliasing\r\naliasing = explicit\r\n\r\n# L012 - Do not have to explicitly alias all columns\r\n[sqlfluff:rules:L012]  # Aliasing\r\naliasing = explicit\r\n\r\n# L013 - Always explicitly alias a column with an expression in it (fix)\r\n[sqlfluff:rules:L013]  # Aliasing\r\nallow_scalar = False\r\n\r\n# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)\r\n[sqlfluff:rules:L014]  # Unquoted identifiers\r\nextended_capitalisation_policy = lower\r\n\r\n# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)\r\n\r\n# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)\r\n[sqlfluff:rules:L016]\r\nignore_comment_lines = False\r\nmax_line_length = 120\r\n\r\n# L017 - There should not be whitespace between function name and brackets (fix)\r\n# L018 - Always align closing bracket of WITH to the WITH keyword (fix)\r\n\r\n# L019 - Always use trailing commas / commas at the end of the line (fix)\r\n[sqlfluff:rules:L019]\r\ncomma_style = trailing\r\n\r\n# L020 - Table aliases will always be unique per statement\r\n# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.\r\n# L022 - Add blank lines after common table expressions (CTE) / WITH.\r\n# L023 - Always add a single whitespace after AS in a WITH clause (fix)\r\n\r\n[sqlfluff:rules:L026]\r\nforce_enable = False\r\n\r\n# L027 - Always add references if more than one referenced table or view is used\r\n\r\n[sqlfluff:rules:L028]\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]  # Keyword identifiers\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L030]  # Function names\r\ncapitalisation_policy = upper\r\n\r\n# L032 - We prefer use of join keys rather than USING\r\n# L034 - We prefer ordering of columns in select statements as (fix):\r\n# 1. wildcards\r\n# 2. single identifiers\r\n# 3. calculations and aggregates\r\n\r\n# L035 - Omit 'else NULL'; it is redundant (fix)\r\n# L036 - Move select targets / identifiers onto new lines each (fix)\r\n# L037 - When using ORDER BY, make the direction explicit (fix)\r\n\r\n# L038 - Never use trailing commas at the end of the SELECT clause\r\n[sqlfluff:rules:L038]\r\nselect_clause_trailing_comma = forbid\r\n\r\n# L039 - Remove unnecessary whitespace (fix)\r\n\r\n[sqlfluff:rules:L040]  # Null & Boolean Literals\r\ncapitalisation_policy = upper\r\n\r\n# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses.\r\nforbid_subquery_in = join\r\n\r\n# L043 - Reduce CASE WHEN conditions to COALESCE (fix)\r\n# L044 - Prefer a known number of columns along the path to the source data\r\n# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)\r\n# L046 - Jinja tags should have a single whitespace on both sides\r\n\r\n# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)\r\n[sqlfluff:rules:L047]  # Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n# L048 - Quoted literals should be surrounded by a single whitespace (fix)\r\n# L049 - Always use IS or IS NOT for comparisons with NULL (fix)\r\n```\r\n\n",
        "hint": "I get a dbt-related error -- can you provide your project file as well? Also, what operating system are you running this on? I tested a simplified (non-dbt) version of your file on my Mac, and it worked okay.\r\n\r\n```\r\ndbt.exceptions.DbtProjectError: Runtime Error\r\n  no dbt_project.yml found at expected path /Users/bhart/dev/sqlfluff/dbt_project.yml\r\n```\nNever mind the questions above -- I managed to reproduce the error in a sample dbt project. Taking a look now...\n@Tumble17: Have you tried setting the `encoding` parameter in `.sqlfluff`? Do you know what encoding you're using? The default is `autodetect`, and SQLFluff \"thinks\" the file uses \"Windows-1252\" encoding, which I assume is incorrect -- that's why SQLFluff is unable to write out the updated file.\nI added this line to the first section of your `.sqlfluff`, and now it seems to work. I'll look into changing the behavior of `sqlfluff fix` so it doesn't erase the file when it fails.\r\n\r\n```\r\nencoding = utf-8\r\n```",
        "base": "a10057635e5b2559293a676486f0b730981f037a",
        "env": "67023b85c41d23d6c6d69812a41b207c4f8a9331",
        "files": [
            "src/sqlfluff/core/linter/linted_file.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/4084",
        "problem": "Multiple processes not used when list of explicit filenames is passed\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen providing a long list of file names to `sqlfluff lint -p -1`, only a single CPU is used. This seems to stem from the fact that https://github.com/sqlfluff/sqlfluff/blob/a006378af8b670f9235653694dbcddd4c62d1ab9/src/sqlfluff/core/linter/linter.py#L1190 is iterating over the list of files. For each listed path there, it would run the found files in parallel. As we are inputting whole filenames here, a path equals a single file and thus `sqlfluff` would only process one file at a time.\r\n\r\nThe context here is the execution of `sqlfluff lint` inside a `pre-commit` hook.\n\n### Expected Behaviour\n\nAll CPU cores are used as `-p -1` is passed on the commandline.\n\n### Observed Behaviour\n\nOnly a single CPU core is used.\n\n### How to reproduce\n\nRun `sqlfluff lint -p -1` with a long list of files.\n\n### Dialect\n\nAffects all. \n\n### Version\n\n1.4.2\n\n### Configuration\n\nNone.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "I have been wondering for some time why sqlfluff never manages to use 100% of CPU. Running it on my Code base takes about 90 minutes. Though never more than 30% of cpu is used\u2026 maybe this sis the reason\u2026\nYeah - this looks like an accurate diagnosis. Most of the testing for the multiprocessing feature was done on large projects of multiple files, but _where a single path was passed_ e.g. `sqlfluff lint .`.\r\n\r\nThis seems like a very sensible improvement for people using the commit hook.\r\n\r\n@barrywhart - you did a lot of the original multiprocessing work. Reckon you could take this one on?\nI'll take a look, sure!",
        "base": "181918e9c2840dc3f5ff1c713bf6b5a00d0725b5",
        "env": "dc59c2a5672aacedaf91f0e6129b467eefad331b",
        "files": [
            "src/sqlfluff/core/linter/linter.py"
        ]
    },
    {
        "pr": "sqlfluff/sqlfluff/3330",
        "problem": "Rule suggestion: `UNION [ALL|DISTINCT]` on new line\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nI would like to suggest a new rule that puts `UNION [ALL|DISTINCT]` statements on their own line, aligned to the surrounding `SELECT` statements.\r\n\r\nFor example, currently \r\n\r\n```sql\r\nSELECT 1 UNION ALL\r\nSELECT 2\r\n```\r\n\r\npasses without errors. This new rule could fix that to\r\n\r\n```sql\r\nSELECT 1 \r\nUNION ALL\r\nSELECT 2\r\n```\r\n\r\nOr in a more complex example\r\n\r\n```sql\r\nSELECT * FROM (\r\n    SELECT 1 UNION ALL\r\n    SELECT 2\r\n)\r\n```\r\n\r\nfixed to\r\n\r\n```sql\r\nSELECT * FROM (\r\n    SELECT 1 \r\n    UNION ALL\r\n    SELECT 2\r\n)\r\n```\n\n### Use case\n\nI have looked at a few SQL style guides and they don't really seem to mention any policy regarding `UNION` statements. However, in 99% of the SQL I have encountered `UNION` statements always seemed to be on a new line. It would be great to have an option to lint the remaining 1% \ud83d\ude09 \n\n### Dialect\n\nansi\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
        "hint": "",
        "base": "c2b1ec442131a70ac5b1560396ce1bbe636e4864",
        "env": "8f6fd1d8a8d69b2c463fbcf5bd1131c47f12ad88",
        "files": [
            "src/sqlfluff/rules/L065.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/1164",
        "problem": "2.x: Nested(many=True) eats first element from generator value when dumping\nAs reproduced in Python 3.6.8:\r\n\r\n```py\r\nfrom marshmallow import Schema, fields\r\n\r\nclass O(Schema):\r\n    i = fields.Int()\r\n\r\nclass P(Schema):\r\n    os = fields.Nested(O, many=True)\r\n\r\ndef gen():\r\n    yield {'i': 1}\r\n    yield {'i': 0}\r\n\r\np = P()\r\np.dump({'os': gen()})\r\n# MarshalResult(data={'os': [{'i': 0}]}, errors={})\r\n```\r\n\r\nProblematic code is here:\r\n\r\nhttps://github.com/marshmallow-code/marshmallow/blob/2.x-line/src/marshmallow/fields.py#L447\r\n\r\nAnd here:\r\n\r\nhttps://github.com/marshmallow-code/marshmallow/blob/2.x-line/src/marshmallow/schema.py#L832\r\n\r\nThe easiest solution would be to cast `nested_obj` to list before calling `schema._update_fields`, just like a normal Schema with `many=True` does.\n",
        "hint": "I confirmed that this is no longer an issue in marshmallow 3. I was able to reproduce this with python 2 and 3 using the latest version of marshmallow 2.\n`next(iter(...))` is not a safe operation for generators.\r\n\r\n```py\r\ndef gen():\r\n    yield 1\r\n    yield 2\r\n\r\nx = gen()\r\nnext(iter(x))\r\n# 1\r\nlist(x)\r\n# [2]\r\n```\r\n\r\nI suspect `list` would be an acceptable solution. If it was a performance concern we could use `itertools.tee` to copy the generator before peeking at the first item.\n`next(iter(...))` is apparently fine because `obj` is guaranteed to be a list here:\r\n\r\nhttps://github.com/marshmallow-code/marshmallow/blob/2.x-line/src/marshmallow/schema.py#L489\r\n\r\nIt's just that usage of `Schema._update_fileds` in `Nested` ignores the requirement.\r\n",
        "base": "1e26d14facab213df5009300b997481aa43df80a",
        "env": "1e26d14facab213df5009300b997481aa43df80a",
        "files": [
            "src/marshmallow/fields.py",
            "src/marshmallow/schema.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/1252",
        "problem": "ISO8601 DateTimes ending with Z considered not valid in 2.19.4\nProbably related to #1247 and #1234 - in marshmallow `2.19.4`, with `python-dateutil` _not_ installed, it seems that loading a datetime in ISO8601 that ends in `Z` (UTC time) results in an error:\r\n\r\n```python\r\nclass Foo(Schema):\r\n    date = DateTime(required=True)\r\n\r\n\r\nfoo_schema = Foo(strict=True)\r\n\r\na_date_with_z = '2019-06-17T00:57:41.000Z'\r\nfoo_schema.load({'date': a_date_with_z})\r\n```\r\n\r\n```\r\nmarshmallow.exceptions.ValidationError: {'date': ['Not a valid datetime.']}\r\n```\r\n\r\nDigging a bit deeper, it seems [`from_iso_datetime`](https://github.com/marshmallow-code/marshmallow/blob/dev/src/marshmallow/utils.py#L213-L215) is failing with a `unconverted data remains: Z` - my understanding of the spec is rather limited, but it seems that they are indeed valid ISO8601 dates (and in `marshmallow==2.19.3` and earlier, the previous snippet seems to work without raising validation errors).\r\n\n",
        "hint": "@lafrech Would you mind looking into this?\nThanks for reporting.\r\n\r\nThis is definitely a side effect of https://github.com/marshmallow-code/marshmallow/pull/1249/files. Sorry about that.\r\n\r\nI don't own a copy of the spec, so the work on this is based on examples... I assumed that microseconds always came as a six-pack. It seems only three digits (your example) is acceptable. From what I understand in the regex we copied from Django, we could even expect any number of digits in [1; 6].\r\n\r\nI see two solutions to this:\r\n\r\n- Split around `\".\"`, then in the right part, get all numbers and ignore letters/symbols.\r\n- Split around `\".\"`, then split the right part around anything that delimitates a timezone (`\"Z\"`, `\"+\"`, `\"-\"`, what else?).\r\n\r\n\nThanks both for the prompt reply! I don't have a copy of the spec myself either - for the timezone suffix, I have based my previous comment on [the Wikipedia entry](https://en.wikipedia.org/wiki/ISO_8601#Time_zone_designators), which seems to hint at the following designators being allowed:\r\n```\r\n<time>Z\r\n<time>\u00b1hh:mm\r\n<time>\u00b1hhmm\r\n<time>\u00b1hh\r\n```\nI also use this WP page, but it doesn't show much about milli/microseconds.",
        "base": "b063a103ae5222a5953cd7453a1eb0d161dc5b52",
        "env": "dd72a797ceeea63ee04d5e1838c3a5a1432347e3",
        "files": [
            "src/marshmallow/utils.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/1359",
        "problem": "3.0: DateTime fields cannot be used as inner field for List or Tuple fields\nBetween releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):\r\n\r\n```python\r\nfrom marshmallow import fields, Schema\r\n\r\nclass MySchema(Schema):\r\n    times = fields.List(fields.DateTime())\r\n\r\ns = MySchema()\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test-mm.py\", line 8, in <module>\r\n    s = MySchema()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 383, in __init__\r\n    self.fields = self._init_fields()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 913, in _init_fields\r\n    self._bind_field(field_name, field_obj)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 969, in _bind_field\r\n    field_obj._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 636, in _bind_to_schema\r\n    self.inner._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 1117, in _bind_to_schema\r\n    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\r\nAttributeError: 'List' object has no attribute 'opts'\r\n```\r\n\r\nIt seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.\n",
        "hint": "Thanks for reporting. I don't think I'll have time to look into this until the weekend. Would you like to send a PR? \nI'm afraid I don't have any time either, and I don't really have enough context on the `_bind_to_schema` process to make sure I'm not breaking stuff.\nOK, no problem. @lafrech Will you have a chance to look into this?\nI've found the patch below to fix the minimal example above, but I'm not really sure what it's missing out on or how to test it properly:\r\n```patch\r\ndiff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py\r\nindex 0b18e7d..700732e 100644\r\n--- a/src/marshmallow/fields.py\r\n+++ b/src/marshmallow/fields.py\r\n@@ -1114,7 +1114,7 @@ class DateTime(Field):\r\n         super()._bind_to_schema(field_name, schema)\r\n         self.format = (\r\n             self.format\r\n-            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\r\n+            or getattr(getattr(schema, \"opts\", None), self.SCHEMA_OPTS_VAR_NAME, None)\r\n             or self.DEFAULT_FORMAT\r\n         )\r\n```\n    git difftool 3.0.0rc8 3.0.0rc9 src/marshmallow/fields.py\r\n\r\nWhen reworking container stuff, I changed\r\n\r\n```py\r\n        self.inner.parent = self\r\n        self.inner.name = field_name\r\n```\r\ninto\r\n\r\n```py\r\n        self.inner._bind_to_schema(field_name, self)\r\n```\r\n\r\nAFAIR, I did this merely to avoid duplication. On second thought, I think it was the right thing to do, not only for duplication but to actually bind inner fields to the `Schema`.\r\n\r\nReverting this avoids the error but the inner field's `_bind_to_schema` method is not called so I'm not sure it is desirable.\r\n\r\nI think we really mean to call that method, not only in this case but also generally.\r\n\r\nChanging\r\n\r\n```py\r\n            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\r\n```\r\n\r\ninto\r\n\r\n```py\r\n            or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\r\n```\r\n\r\nmight be a better fix. Can anyone confirm (@sloria, @deckar01)?\r\n\r\nThe fix in https://github.com/marshmallow-code/marshmallow/issues/1357#issuecomment-523465528 removes the error but also the feature: `DateTime` fields buried into container fields won't respect the format set in the `Schema`.\r\n\r\nI didn't double-check that but AFAIU, the change I mentioned above (in container stuff rework) was the right thing to do. The feature was already broken (format set in `Schema` not respected if `DateTime` field in container field) and that's just one of the issues that may arise due to the inner field not being bound to the `Schema`. But I may be wrong.\nOn quick glance, your analysis and fix look correct @lafrech \nLet's do that, then.\r\n\r\nNot much time either. The first who gets the time can do it.\r\n\r\nFor the non-reg tests :\r\n\r\n1/ a test that checks the format set in the schema is respected if the `DateTime` field is in a container field\r\n\r\n2/ a set of tests asserting the `_bind_to_schema` method of inner fields `List`, `Dict`, `Tuple` is called from container fields (we can use `DateTime` with the same test case for that)\r\n\r\nPerhaps 1/ is useless if 2/ is done.",
        "base": "b40a0f4e33823e6d0f341f7e8684e359a99060d1",
        "env": "8b3a32614fd4a74e93e9a63a042e74c1fea34466",
        "files": [
            "src/marshmallow/fields.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/1343",
        "problem": "[version 2.20.0] TypeError: 'NoneType' object is not subscriptable\nAfter update from version 2.19.5 to 2.20.0 I got error for code like:\r\n\r\n```python\r\nfrom marshmallow import Schema, fields, validates\r\n\r\n\r\nclass Bar(Schema):\r\n    value = fields.String()\r\n\r\n    @validates('value')  # <- issue here\r\n    def validate_value(self, value):\r\n        pass\r\n\r\n\r\nclass Foo(Schema):\r\n    bar = fields.Nested(Bar)\r\n\r\n\r\nsch = Foo()\r\n\r\nsch.validate({\r\n    'bar': 'invalid',\r\n})\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/_/bug_mschema.py\", line 19, in <module>\r\n    'bar': 'invalid',\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 628, in validate\r\n    _, errors = self._do_load(data, many, partial=partial, postprocess=False)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 670, in _do_load\r\n    index_errors=self.opts.index_errors,\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 292, in deserialize\r\n    index=(index if index_errors else None)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 65, in call_and_store\r\n    value = getter_func(data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 285, in <lambda>\r\n    data\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 265, in deserialize\r\n    output = self._deserialize(value, attr, data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 465, in _deserialize\r\n    data, errors = self.schema.load(value)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 588, in load\r\n    result, errors = self._do_load(data, many, partial=partial, postprocess=True)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 674, in _do_load\r\n    self._invoke_field_validators(unmarshal, data=result, many=many)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 894, in _invoke_field_validators\r\n    value = data[field_obj.attribute or field_name]\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\n",
        "hint": "Thanks for reporting. I was able to reproduce this on 2.20.0. This is likely a regression from https://github.com/marshmallow-code/marshmallow/pull/1323 . I don't have time to look into it now. Would appreciate a PR.",
        "base": "2be2d83a1a9a6d3d9b85804f3ab545cecc409bb0",
        "env": "7015fc4333a2f32cd58c3465296e834acd4496ff",
        "files": [
            "src/marshmallow/schema.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/2123",
        "problem": "fields.URL should allow relative-only validation\nRelative URLs may be used to redirect the user within the site, such as to sign in, and allowing absolute URLs without extra validation opens up a possibility of nefarious redirects.\r\n\r\nCurrent `fields.URL(relative = True)` allows relative URLs _in addition_ to absolute URLs, so one must set up extra validation to catch either all absolute URLs or just those that don't have a valid domain names.\r\n\r\nIt would be helpful if there was a way to set up URL validation to allow only relative URLs. \r\n\r\n~One quick and dirty way to do this would be if there was a `validate.Not` operator, then at the expense of matching the value twice, it would be possible to use something like this:~\r\n\r\n~`fields.URL(relative = True, validate=validate.Not(validate.URL()))`~\r\n\r\nEDIT: Never mind the crossed out thought above - failed validations are handled only via exceptions and while failing the inner validator works in general, it requires suppressing exception handlers and is just not a good way to go about it. \n",
        "hint": "Maybe an additional flag `absolute` that would default to `True`?\r\n\r\nWould you like to work on this?\n@lafrech Thank you for commenting. \r\n\r\nMy thinking would be that flavors could be selected individually, as if flags are used, so they could be combined. Something along these lines:\r\n\r\n```\r\nargs = {\r\n    \"ref\": fields.URL(kind=fields.URL.absolute | fields.URL.relative)\r\n    # OR\r\n    \"ref\": fields.URL(kind=[\"absolute\", \"relative\"])\r\n```\r\n\r\nThis also would allow to retain backward compatibility for existing `relative=True|False`, which would be translated to these flags combined or just absolute flag being used.\r\n\r\nAn extra Boolean would work fine as well.  It would be similar to how Joi handles it:\r\n\r\nhttps://joi.dev/api/?v=17.9.1#stringurioptions\r\n\r\nAs for me fixing it, I'm not sure - the evaluation of the attribute for relative URLs is integrated in a pretty intricate regex construction with those Booleans used as keys. This change sounds bigger than I could commit to at this point. My apologies. Please feel free to close this issue if there's no one else interested in it.\nI'm not very proficient with regexes, although the change involved here should be marginal (the regex is already conditional due to the absolute flag).\r\n\r\nI won't take the time to do it but I don't mind keeping this open for now.",
        "base": "5a10e83c557d2ee97799c2b85bec49fc90381656",
        "env": "819749204b9a7271c189401e5f5aa00cab624514",
        "files": [
            "src/marshmallow/fields.py",
            "src/marshmallow/validate.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/1229",
        "problem": "`only` argument inconsistent between Nested(S, many=True) and List(Nested(S))\n```python\r\nfrom pprint import pprint\r\n\r\nfrom marshmallow import Schema\r\nfrom marshmallow.fields import Integer, List, Nested, String\r\n\r\n\r\nclass Child(Schema):\r\n    name = String()\r\n    age = Integer()\r\n\r\n\r\nclass Family(Schema):\r\n    children = List(Nested(Child))\r\n\r\n\r\nclass Family2(Schema):\r\n    children = Nested(Child, many=True)\r\n\r\nfamily = {'children':[\r\n    {'name': 'Tommy', 'age': 12},\r\n    {'name': 'Lily', 'age': 15},\r\n]}\r\n\r\npprint(Family( only=['children.name']).dump(family).data)\r\npprint(Family2( only=['children.name']).dump(family).data)\r\n```\r\nreturns\r\n```\r\n{'children': [{'age': 12, 'name': 'Tommy'}, {'age': 15, 'name': 'Lily'}]}\r\n{'children': [{'name': 'Tommy'}, {'name': 'Lily'}]}\r\n```\r\n\r\ntested with marshmallow 2.15.4\r\n\r\nThe same applies to `exclude` argument.\n",
        "hint": "For now I'm using following workaround:\r\n```python\r\nclass ListFix(List):\r\n    @property\r\n    def only(self):\r\n        return getattr(self.container, 'only')\r\n\r\n    @only.setter\r\n    def only(self, new_options):\r\n        original_options = getattr(self.container, 'only', ())\r\n        if original_options:\r\n            new_options &= type(new_options)(original_options)\r\n        setattr(self.container, 'only', new_options)\r\n\r\n\r\nclass Child(Schema):\r\n    name = String()\r\n    age = Integer()\r\n\r\n\r\nclass Family(Schema):\r\n    children = ListFix(Nested(Child))\r\n```\r\n\r\nthe option propagation code was taken from `BaseSchema.__apply_nested_option`\r\n\r\nmaybe apply option code (the part I have copied to ListFix property) should be moved to field?\r\n\r\n**Edited:** Just found a nasty side effect of my \"fix\"\r\n\r\n```python\r\nfamily = {'children': [\r\n    {'name': 'Tommy', 'age': 12},\r\n    {'name': 'Lily', 'age': 15},\r\n]}\r\n\r\nfor family_schema in (\r\n        Family(),\r\n        Family(only=['children.name']),\r\n        Family2(),\r\n        Family2(only=['children.name']),\r\n):\r\n    pprint(family_schema.dump(family).data)\r\n```\r\nprints\r\n```\r\n{'children': [{'name': 'Tommy'}, {'name': 'Lily'}]}\r\n{'children': [{'name': 'Tommy'}, {'name': 'Lily'}]}\r\n{'children': [{'age': 12, 'name': 'Tommy'}, {'age': 15, 'name': 'Lily'}]}\r\n{'children': [{'name': 'Tommy'}, {'name': 'Lily'}]}\r\n```\nThanks @rooterkyberian . Let's continue discussion of this in #779.",
        "base": "456bacbbead4fa30a1a82892c9446ac9efb8055b",
        "env": "8b3a32614fd4a74e93e9a63a042e74c1fea34466",
        "files": [
            "src/marshmallow/fields.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/1810",
        "problem": "3.12 no longer supports fields named `parent`\nPretty sure that #1631 broke it. Reproducible example:\r\n\r\n```py\r\nfrom marshmallow import INCLUDE\r\nfrom marshmallow.fields import Nested\r\nfrom sqlalchemy import Column, DATE, create_engine, ForeignKey\r\nfrom sqlalchemy.dialects.postgresql import UUID\r\nfrom sqlalchemy.orm import declarative_base, relationship\r\nfrom marshmallow_sqlalchemy import SQLAlchemyAutoSchema\r\nfrom testing.postgresql import Postgresql\r\n\r\n\r\nBase = declarative_base()\r\n\r\n\r\nclass Author(Base):\r\n    __tablename__ = 'author'\r\n    id = Column(UUID(as_uuid=True), primary_key=True)\r\n    docs = relationship('Document', back_populates='parent')\r\n\r\n\r\nclass Document(Base):\r\n    __tablename__ = 'document'\r\n    id = Column(UUID(as_uuid=True), primary_key=True)\r\n    parent_id = Column(UUID(as_uuid=True), ForeignKey('author.id'))\r\n    parent = relationship(Author, back_populates='docs')\r\n    last_updated = Column(DATE)\r\n\r\n\r\nclass AuthorSchema(SQLAlchemyAutoSchema):\r\n    class Meta(SQLAlchemyAutoSchema.Meta):\r\n        model = Author\r\n\r\n\r\nclass DocumentSchema(SQLAlchemyAutoSchema):\r\n    parent = Nested(AuthorSchema)\r\n\r\n    class Meta(SQLAlchemyAutoSchema.Meta):\r\n        model = Document\r\n\r\n\r\nwith Postgresql() as postgresql:\r\n    url = postgresql.url(drivername='postgresql+psycopg2')\r\n    engine = create_engine(url, echo=True)\r\n    Base.metadata.create_all(engine)\r\n\r\n    DocumentSchema(unknown=INCLUDE)\r\n```\r\n\r\nResults in:\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"/home/phil/.config/JetBrains/PyCharm2021.1/scratches/sqlalchemy-marshmallow-reprex.py\", line 44, in <module>\r\n    DocumentSchema(unknown=INCLUDE)\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow_sqlalchemy/schema/load_instance_mixin.py\", line 43, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 392, in __init__\r\n    self._init_fields()\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 971, in _init_fields\r\n    self._bind_field(field_name, field_obj)\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 1030, in _bind_field\r\n    field_obj._bind_to_schema(field_name, self)\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/fields.py\", line 1201, in _bind_to_schema\r\n    or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\r\nAttributeError: 'NoneType' object has no attribute 'opts'\r\n```\r\n\r\nHere, `self.root` resolves to `None` for the `last_updated` field:\r\n\r\nhttps://github.com/marshmallow-code/marshmallow/blob/69270215ab9275dc566b010ecdb8777c186aa776/src/marshmallow/fields.py#L411-L420\r\n\r\nThis happens since that field\u2019s `.parent` is the `DocumentSchema` class, which *does* have a `.parent` attribute. However that attribute is a `Nested` instance, not another schema as expected\nReturn a field\u2019s root schema as soon as it is found\nThis prevents accessing a schema\u2019s `.parent` attribute if it has one (e.g. a field called `parent`)\r\n\r\nFixes #1808, I think.\n",
        "hint": "\nThanks @flying-sheep. This looks correct. Do you have time to write a regression test for this? If not, I can do it myself when I'm off the clock (this weekend, probably). Let me know either way",
        "base": "23d0551569d748460c504af85996451edd685371",
        "env": "23d0551569d748460c504af85996451edd685371",
        "files": [
            "src/marshmallow/base.py",
            "src/marshmallow/fields.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/1702",
        "problem": "RFC: Change the way we store metadata?\nUsers are often bit by the fact that fields store arbitrary keyword arguments as metadata. See https://github.com/marshmallow-code/marshmallow/issues/683.\r\n\r\n> ...The reasons we use **kwargs instead of e.g. `metadata=` are mostly historical. The original decision was that storing kwargs 1) was more concise and 2) saved us from having to come up with an appropriate name... \"metadata\" didn't seem right because there are use cases where the things your storing aren't really metadata. At this point, it's not worth breaking the API.\r\n\r\n> Not the best reasons, but I think it's not terrible. We've discussed adding a [whitelist of metadata keys](https://github.com/marshmallow-code/marshmallow/issues/683#issuecomment-385113845) in the past, but we decided it wasn't worth the added API surface.\r\n\r\n_Originally posted by @sloria in https://github.com/marshmallow-code/marshmallow/issues/779#issuecomment-522283135_\r\n\r\nPossible solutions:\r\n\r\n1. Use `metadata=`.\r\n2. Specify a whitelist of allowed metadata arguments.\r\n\r\nFeedback welcome!\n",
        "hint": "Solution 1. is preferable to 2., I think. That said, there are some use cases where it's awkward to call additional kwargs \"metadata\". `location` in webargs is one that comes to mind.\r\n\r\n```python\r\n# current API\r\n\"some_query_param\": fields.Bool(location=\"query\")\r\n```\r\n\r\nthough we could probably wrap all the fields in webargs to take the additional `location` argument. \ud83e\udd14 \nI wanted to note that even webargs' `location` doesn't necessarily make the case against `metadata=...`. I was surprised/confused at first when I went looking for `location` in marshmallow and found no mention of it. At the cost of a little bit of verbosity, it would make it easier to understand how marshmallow is functioning.\r\n\r\nRelatedly, the plan in https://github.com/marshmallow-code/webargs/issues/419 includes making `location=...` for webargs a thing of the past.\ncc @jtrakk . This was your suggestion in https://github.com/marshmallow-code/marshmallow/issues/779#issuecomment-522282845 . I'm leaning towards this more and more.\n+1 on this, IMHO biggest problem of `self.metadata=kwargs` is not that it's unexpected, but that it's not generating errors on wrong keyword arguments, which is pretty annoying due frequent api changes :-) so - you can find mistakes only later, all your typos in metadata field...\nOne one hand, I think it is better to specify `metadata=`. More explicit.\r\n\r\nOTOH, this will make my models a bit more verbose:\r\n\r\n```py\r\nclass MyModel(ma.Schema:\r\n    some_int = ma.fields.Int(\r\n        required=True,\r\n        validate=ma.validate.OneOf([1, 2, 3]),\r\n        metadata={\"description\": \"This string explains what this is all about\"}\r\n    )\r\n```\r\n\r\nFor such a use case, the shortcut of using extra kwargs as metadata is nice.\r\n\r\nIf we went with solution 2, users would be able to extend the whitelist with their own stuff. Apispec could extend it with the keyword arguments it expects (valid OpenAPI attributes) and we could even catch typos inside metadata, while solution 1 blindly accepts anything in metadata.\r\n\r\nHowever, this would prevent accepting arbitrary attributes in metadata, which sucks. E.g. in apispec, we also accept any `\"x-...\"` attribute. So we'd need to genericize the whitelist to a callable mechanism. And we end up with a gas factory feature while we wanted to make thing simple.\r\n\r\nOverall, perhaps the downside of 1 (model verbosity) is not that bad.\nAgreed. Consider this accepted.\r\n\r\nMy plan is to deprecate `metadata=kwargs` in a later 3.x release. Let's let the dust settle on v3 before bombarding users with DeprecationWarnings \ud83d\ude05 ",
        "base": "fa6c7379468f59d4568e29cbbeb06b797d656215",
        "env": "fa6c7379468f59d4568e29cbbeb06b797d656215",
        "files": [
            "src/marshmallow/fields.py"
        ]
    },
    {
        "pr": "marshmallow-code/marshmallow/1524",
        "problem": "Incorrect Email Validation\nhttps://github.com/marshmallow-code/marshmallow/blob/fbe22eb47db5df64b2c4133f9a5cb6c6920e8dd2/src/marshmallow/validate.py#L136-L151\r\n\r\nThe email validation regex will match `email@domain.com\\n`, `email\\n@domain.com`, and `email\\n@domain.com\\n`.\r\n\r\nThe issue is that `$` is used to match until the end of a string. Instead, `\\Z` should be used. - https://stackoverflow.com/a/48730645\r\n\r\nIt is possible that other validators might suffer from the same bug, so it would be good if other regexes were also checked.\r\n\r\nIt is unclear, but this may lead to a security vulnerability in some projects that use marshmallow (depending on how the validator is used), so a quick fix here might be helpful. In my quick look around I didn't notice anything critical, however, so I figured it would be fine to open this issue.\n",
        "hint": "`^` behaves as expected, but `$` also matches a \"string-ending newline\". This appears to be a holdover from POSIX that most languages have long abandoned as default behavior.\r\n\r\nThanks for reporting this @nbanmp. If you are interesting in contributing a MR, I would be happy to review it, otherwise I can pick this up as soon as I get a chance.\r\n\r\nAlso, can you provide any examples of using an email address in a way that would make this issue a security vulnerability? The best I can come up with is that it will terminate the email headers early and cause an email to be malformed or undeliverable, which would be a standard bug. Since the newline can only be at the very end of the string, this would not allow injecting additional headers into an email. If it does have security implications I can submit a CVE and flag the vulnerable versions on TideLift.\nI don't know if I would call this a vulnerability in marshmallow, but it might - in rare cases - allow for exploitation of certain vulnerabilities in other tools.\r\n\r\nThe most likely place this might result in a vulnerability is when verifying an email for authentication, and allowing multiple accounts with the same, but different emails.\r\n\r\nA minor form of this would be allowing the same email to be used multiple times in sites that want to prevent that.\r\n\r\nA more serious, but unlikely form of this would be something like:\r\n\r\nDepending on how emails are stored / used, whitespace around them might be stripped or not between sending emails, and checking emails. It's not a vulnerability in marshmallow itself, but it might result in inconsistencies in other apps lead to actual vulnerabilities.\r\n\r\n```python\r\nvalidateemail(email)\r\n\r\nif email in database:\r\n    error()\r\nelse:\r\n    saveaccount(email.strip(), password) # This might overwrite an account with the correct email\r\n```\r\n\r\nThis example is actually what I was thinking of, because I was looking through the code for CTFd, which had a similar vulnerability occurring with the username, and, if not for dumb luck (a random lack of .strip() / it being in a different place), the same vulnerability would have also been possible using the email, even though the email was invalid.\r\n",
        "base": "7015fc4333a2f32cd58c3465296e834acd4496ff",
        "env": "7015fc4333a2f32cd58c3465296e834acd4496ff",
        "files": [
            "src/marshmallow/validate.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1160",
        "problem": "ValueError: SingleAxisTracker, Array, and running the model on a tuple/list of weather\n**Describe the bug**\r\nI know a refactoring of the Array with single axis tracking is in the works #1146. In the meantime, a `ValueError` is raised when trying to run a SingleAxisTracker defined with an array and supplying (ghi, dni, dhi) weather as a tuple/list. I would expect calling `run_model([weather])` would work similarly to a modelchain for a fixed system with an array singleton. The error stems from `pvlib.tracking.SingleAxisTracker.get_irradiance`  because most inputs are `pandas.Series`, but ghi, dhi, dni are `Tuple[Series]`.\r\n\r\n**To Reproduce**\r\n```python\r\nimport pandas as pd\r\nfrom pvlib.location import Location\r\nfrom pvlib.pvsystem import Array\r\nfrom pvlib.tracking import SingleAxisTracker\r\nfrom pvlib.modelchain import ModelChain\r\n\r\n\r\narray_params = {\r\n    \"surface_tilt\": None,\r\n    \"surface_azimuth\": None,\r\n    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\r\n    \"albedo\": 0.2,\r\n    \"temperature_model_parameters\": {\r\n        \"u_c\": 29.0,\r\n        \"u_v\": 0.0,\r\n        \"eta_m\": 0.1,\r\n        \"alpha_absorption\": 0.9,\r\n    },\r\n    \"strings\": 5,\r\n    \"modules_per_string\": 7,\r\n    \"module_parameters\": {\r\n        \"alpha_sc\": 0.004539,\r\n        \"gamma_ref\": 1.2,\r\n        \"mu_gamma\": -0.003,\r\n        \"I_L_ref\": 5.11426,\r\n        \"I_o_ref\": 8.10251e-10,\r\n        \"R_sh_ref\": 381.254,\r\n        \"R_sh_0\": 400.0,\r\n        \"R_s\": 1.06602,\r\n        \"cells_in_series\": 96,\r\n        \"R_sh_exp\": 5.5,\r\n        \"EgRef\": 1.121,\r\n    },\r\n}\r\ninverter_parameters = {\r\n    \"Paco\": 250.0,\r\n    \"Pdco\": 259.589,\r\n    \"Vdco\": 40.0,\r\n    \"Pso\": 2.08961,\r\n    \"C0\": -4.1e-05,\r\n    \"C1\": -9.1e-05,\r\n    \"C2\": 0.000494,\r\n    \"C3\": -0.013171,\r\n    \"Pnt\": 0.075,\r\n}\r\n\r\n\r\nlocation = Location(latitude=33.98, longitude=-115.323, altitude=2300)\r\n\r\n\r\ntracking = SingleAxisTracker(\r\n    arrays=[Array(**array_params, name=0)],\r\n    axis_tilt=0,\r\n    axis_azimuth=180,\r\n    gcr=0.1,\r\n    backtrack=True,\r\n    inverter_parameters=inverter_parameters,\r\n)\r\n\r\nweather = pd.DataFrame(\r\n    {\r\n        \"ghi\": [1100.0, 1101.0],\r\n        \"dni\": [1000.0, 1001],\r\n        \"dhi\": [100.0, 100],\r\n        \"module_temperature\": [25.0, 25],\r\n    },\r\n    index=pd.DatetimeIndex(\r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\r\n    ),\r\n)\r\nmc = ModelChain(\r\n    tracking,\r\n    location,\r\n    aoi_model=\"no_loss\",\r\n    spectral_model=\"no_loss\",\r\n)\r\nmc.run_model(weather)  # OK\r\nmc.run_model([weather])  # ValueError\r\n\r\n```\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.0-alpha.2+2.g47654a0\r\n\n",
        "hint": "",
        "base": "47654a073e0eb2b48b2ccdadb5cade9be0484b73",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1738",
        "problem": "`pvlib.soiling.hsu` takes `tilt` instead of `surface_tilt`\n`pvlib.soiling.hsu` takes a `tilt` parameter representing the same thing we normally call `surface_tilt`:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/7a2ec9b4765124463bf0ddd0a49dcfedc4cbcad7/pvlib/soiling.py#L13-L14\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/7a2ec9b4765124463bf0ddd0a49dcfedc4cbcad7/pvlib/soiling.py#L33-L34\r\n\r\nI don't see any good reason for this naming inconsistency (I suspect `tilt` just got copied from the matlab implementation) and suggest we rename the parameter to `surface_tilt` with a deprecation.\r\n\r\nAlso, the docstring parameter type description says it must be `float`, but the model's reference explicitly says time series tilt is allowed: \r\n\r\n> The angle is variable for tracking systems and is taken as the average angle over the time step.\r\n\r\n\n",
        "hint": "On second thought, I'm not seeing how this can be deprecated without reordering the parameters, which doesn't seem worth it to me.  I'm inclined to rename it without deprecation in 0.10.0. ",
        "base": "275e6718caf7486cb5b7dcf29acd59499ad51f7f",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/soiling.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1782",
        "problem": "_golden_sect_DataFrame changes in 0.9.4\n**Describe the bug**\r\n\r\n`0.9.4` introduced the following changes in the `_golden_sect_DataFrame`: We are checking `upper` and `lower` parameters and raise an error if `lower > upper`.\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/81598e4fa8a9bd8fadaa7544136579c44885b3d1/pvlib/tools.py#L344-L345\r\n\r\n`_golden_sect_DataFrame` is used by `_lambertw`:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/81598e4fa8a9bd8fadaa7544136579c44885b3d1/pvlib/singlediode.py#L644-L649\r\n\r\nI often have slightly negative `v_oc` values (really close to 0) when running simulations (second number in the array below):\r\n```\r\narray([ 9.46949758e-16, -8.43546518e-15,  2.61042547e-15,  3.82769773e-15,\r\n        1.01292315e-15,  4.81308106e+01,  5.12484772e+01,  5.22675087e+01,\r\n        5.20708941e+01,  5.16481028e+01,  5.12364071e+01,  5.09209060e+01,\r\n        5.09076598e+01,  5.10187680e+01,  5.11328118e+01,  5.13997628e+01,\r\n        5.15121386e+01,  5.05621451e+01,  4.80488068e+01,  7.18224446e-15,\r\n        1.21386700e-14,  6.40136698e-16,  4.36081007e-16,  6.51236255e-15])\r\n```\r\n\r\nIf we have one negative number in a large timeseries, the simulation will crash which seems too strict.\r\n\r\n**Expected behavior**\r\n\r\nThat would be great to either:\r\n* Have this data check be less strict and allow for slightly negative numbers, which are not going to affect the quality of the results.\r\n* On `_lambertw`: Do not allow negative `v_oc` and set negative values to `np.nan`, so that the error is not triggered. It will be up to the upstream code (user) to manage those `np.nan`.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: >= 0.9.4\r\n - ``pandas.__version__``: 1.5.3\r\n - python: 3.10.11\r\n\nsinglediode error with very low effective_irradiance\n**Describe the bug**\r\n\r\nSince pvlib 0.9.4 release (https://github.com/pvlib/pvlib-python/pull/1606) I get an error while running the single-diode model with some very low effective irradiance values.\r\n\r\n**To Reproduce**\r\n\r\n```python\r\nfrom pvlib import pvsystem\r\n\r\neffective_irradiance=1.341083e-17\r\ntemp_cell=13.7 \r\n\r\ncec_modules = pvsystem.retrieve_sam('CECMod')\r\ncec_module = cec_modules['Trina_Solar_TSM_300DEG5C_07_II_']\r\n\r\nmount = pvsystem.FixedMount()\r\narray = pvsystem.Array(mount=mount,\r\n                       module_parameters=cec_module)\r\n\r\nsystem = pvsystem.PVSystem(arrays=[array])\r\n\r\nparams = system.calcparams_cec(effective_irradiance, \r\n                               temp_cell)\r\n\r\nsystem.singlediode(*params)\r\n```\r\n\r\n```in _golden_sect_DataFrame(params, lower, upper, func, atol)\r\n    303 \"\"\"\r\n    304 Vectorized golden section search for finding maximum of a function of a\r\n    305 single variable.\r\n   (...)\r\n    342 pvlib.singlediode._pwr_optfcn\r\n    343 \"\"\"\r\n    344 if np.any(upper - lower < 0.):\r\n--> 345     raise ValueError('upper >= lower is required')\r\n    347 phim1 = (np.sqrt(5) - 1) / 2\r\n    349 df = params\r\n\r\nValueError: upper >= lower is required\r\n```\r\n\r\n**Expected behavior**\r\nThis complicates the bifacial modeling procedure as `run_model_from_effective_irradiance` can be called with very low irradiance values estimated by pvfactors (at sunrise or sunset for instance). \r\n\r\n**Versions:**\r\n - ``pvlib.__version__``:  0.9.4\r\n - ``pandas.__version__``: 1.5.3\r\n - python: 3.10\r\n\r\n**Additional context**\r\n\r\nv_oc is negative in this case which causes the error. \r\n\r\n```python\r\nfrom pvlib.singlediode import _lambertw_v_from_i\r\nphotocurrent = params[0]\r\nsaturation_current = params[1]\r\nresistance_series = params[2]\r\nresistance_shunt = params[3]\r\nnNsVth = params[4]\r\nv_oc = _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, 0.,\r\n                              saturation_current, photocurrent)\r\n```\r\n\n",
        "hint": "See #1673 \n@cedricleroy can you provide the inputs and function call that produced the negative `v_oc` shown above?\n@echedey-ls Thanks! I thought I checked for related issues, but apparently not enough \ud83d\ude04 \r\n\r\n@cwhanse Sure thing:\r\n\r\nRunning [`_lambertw_v_from_i` in `_lambertw`](https://github.com/pvlib/pvlib-python/blob/v0.9.4/pvlib/singlediode.py#L639-L641) with the following data:\r\n\r\n```\r\n    resistance_shunt  resistance_series    nNsVth  current  saturation_current  photocurrent          v_oc\r\n0        8000.000000              0.178  1.797559      0.0        1.480501e-11      0.000000  8.306577e-16\r\n1        8000.000000              0.178  1.797048      0.0        1.456894e-11      0.000000 -7.399531e-15\r\n2        8000.000000              0.178  1.791427      0.0        1.220053e-11      0.000000  2.289847e-15\r\n3        8000.000000              0.178  1.789892      0.0        1.162201e-11      0.000000  3.357630e-15\r\n4        8000.000000              0.178  1.790915      0.0        1.200467e-11      0.000000  8.885291e-16\r\n5        7384.475098              0.178  1.796786      0.0        1.444902e-11      0.237291  4.222001e+01\r\n6        5023.829590              0.178  1.814643      0.0        2.524836e-11      1.458354  4.495480e+01\r\n7        2817.370605              0.178  1.841772      0.0        5.803733e-11      3.774055  4.584869e+01\r\n8        1943.591919              0.178  1.877364      0.0        1.682954e-10      6.225446  4.567622e+01\r\n9        1609.391479              0.178  1.910984      0.0        4.479085e-10      8.887444  4.530535e+01\r\n10       1504.273193              0.178  1.937034      0.0        9.402419e-10     11.248103  4.494422e+01\r\n11       1482.143799              0.178  1.951216      0.0        1.399556e-09     12.272360  4.466746e+01\r\n12       1485.013794              0.178  1.950762      0.0        1.381967e-09     12.114989  4.465584e+01\r\n13       1506.648315              0.178  1.942643      0.0        1.100982e-09     11.167084  4.475331e+01\r\n14       1580.780029              0.178  1.928508      0.0        7.387948e-10      9.350249  4.485334e+01\r\n15       1832.828735              0.178  1.901971      0.0        3.453772e-10      6.842797  4.508751e+01\r\n16       2604.075684              0.178  1.869294      0.0        1.325485e-10      4.191604  4.518609e+01\r\n17       4594.301270              0.178  1.844949      0.0        6.390201e-11      1.771347  4.435276e+01\r\n18       6976.270996              0.178  1.829467      0.0        3.987927e-11      0.409881  4.214808e+01\r\n19       8000.000000              0.178  1.821491      0.0        3.120619e-11      0.000000  6.300214e-15\r\n20       8000.000000              0.178  1.813868      0.0        2.464867e-11      0.000000  1.064796e-14\r\n21       8000.000000              0.178  1.809796      0.0        2.171752e-11      0.000000  5.615234e-16\r\n22       8000.000000              0.178  1.808778      0.0        2.103918e-11      0.000000  3.825272e-16\r\n23       8000.000000              0.178  1.806231      0.0        1.943143e-11      0.000000  5.712599e-15\r\n```\r\n\r\n[data.csv](https://github.com/pvlib/pvlib-python/files/11807543/data.csv)\r\n\r\n\n> If we have one negative number in a large timeseries, the simulation will crash which seems too strict.\r\n\r\nAgree this is not desirable.\r\n\r\nMy thoughts:\r\n\r\n1. We could insert `v_oc = np.maximum(v_oc, 0)` above this [line](https://github.com/pvlib/pvlib-python/blob/e643dc3f835c29b12b13d7375e33885dcb5d07c7/pvlib/singlediode.py#L649). That would preserve nan.\r\n2. I am reluctant to change `_lambertw_v_from_i`. That function's job is to solve the diode equation, which is valid for negative current. I don't think this function should make decisions about its solution. There will always be some degree of imprecision (currently it's around 10-13 or smaller, I think).\r\n3. I am also reluctant to change `_golden_sect_DataFrame` for similar reasons - the function's job should be to find a minimum using the golden section search. Complying with the `lower < upper` requirement is the job of the code that calls this function.\r\n\r\n\n1/ makes sense to me. I agree with the CONS for 2/ and 3/\r\n\r\nHappy to open a PR with 1. if that helps. \n> Happy to open a PR with 1. if that helps.\r\n\r\nThat is welcome.  Because I'm cautious about junk values with larger magnitude being covered up by 0s, maybe \r\n\r\n```\r\nv_oc[(v_oc < 0) & (v_oc > 1e-12)] = 0.\r\n```\r\n\r\n\nThat's unexpected, thanks for reporting. \r\n\r\nI'll note that the negative Voc results from taking the difference of two very large but nearly equal numbers. It's likely limited to the CEC model, where the shunt resistance is inversely proportional to irradiance, which would be about 1e19 at photocurrent of 1e-17 for this case.\nNow this gets strange: the Voc value is positive with pvlib v0.9.3. The function involved `pvlib.singlediode._lambertw_v_from_i` hasn't changed for many releases. In both pvlib v0.9.3 and v0.9.4, in this calculation of Voc, the lambertw term overflows so the Voc value is computed using only python arithmetic operators and numpy.log.\r\n\r\nI'm starting to think the error depends on python and numpy versions.\nThe difference between 0.9.3 and 0.9.4 here may be due to slightly different values returned by `calcparams_cec`.  Compare the output of `print(list(map(str, params)))`; I get slightly different saturation current values for the given example.  Maybe the changed Boltzmann constant in #1617 is the cause?\n+1 to #1617 as the likely culprit. I get the positive/negative Voc values with the same python and numpy versions but different pvlib versions.\nTo illustrate the challenge, [this line](https://github.com/pvlib/pvlib-python/blob/f4d7c6e1c17b3fddba7cc49d39feed2a6fa0f30e/pvlib/singlediode.py#L566) computes the Voc.\r\n\r\nStripping out the indexing the computation is\r\n\r\n```\r\n    V = (IL + I0 - I) / Gsh - \\\r\n        I * Rs - a * lambertwterm\r\n```\r\nWith pvlib v0.9.4, Io is 7.145289906185543e-12. a is not affected, since a value of the Boltzmann contant is inherent in the a_ref value from the database. (IL + I0 - I) / Gsh is 107825185636.40567, I * Rs is 0, and a * lambertwterm is 107825185636.40569\r\n\r\nWith pvlib v0.9.3, Io is 7.145288699667595e-12.  (IL + I0 - I) / Gsh is 107825167429.58397, I * Rs is 0, and a * lambertwterm is 107825167429.58395\r\n\r\nThe difference defining Voc is in the least significant digit.\r\n\nIncreasing the iterations that solve for lambertwterm doesn't fix this issue.\nThis smells to me like the inevitable error from accumulated round-off. \r\n\r\nFWIW, negative Voc can be achieved in 0.9.3 as well -- try the given example but with `effective_irradiance=1.e-18`.  The difference is that before #1606, it led to nans and warnings instead of raising an error. \r\n\r\n\n@pasquierjb I recommend intercepting the effective irradiance and setting values to 0 which are below a minimum on the order of 1e-9 W/m2. That will propagate to shunt resistance = np.inf, which changes the calculation path in pvlib.singlediode and gives Voc=0.\r\n\r\nI'm not sure we'll be able to extend the numerical solution of the single diode equation to be accurate at very low but non-zero values of photocurrent (and/or enormous but finite values of shunt resistance.)\r\n\r\nI note that `pvlib.pvsystem.calcparams_desoto` doesn't like `effective_irradiance=0.` but is OK with `effective_irradiance=np.array([0.])`.  Has to do with trapping and ignoring division by zero warnings and errors.\nHave you tried setting `method='newton'` instead of `'lambertw'`? https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.pvsystem.singlediode.html#pvlib-pvsystem-singlediode\nSetting `method='newton'` gets a solution to this case. `method` isn't available as a parameter of the `PVSystem.singlediode` method so @pasquierjb would need to change his workflow to use it. Something for us to consider adding.\nMy workaround for this issue was to first filter very low `effective_irradiance` values (`<1e-8`), and then filter `photocurrent` and `saturation_current` parameters when `effective_irradiance=0` and made them `=0`. This assures that you won't get negative `v_oc` values.",
        "base": "0bc5a53dedd8aa9e553c732a31003ce020bc2f54",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/singlediode.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1719",
        "problem": "Match `pvsystem.i_from_v`, `v_from_i` single diode parameters with `singlediode` order.\n**Is your feature request related to a problem? Please describe.**\r\nThe single diode model parameters for `i_from_v`, `v_from_i` in `pvsystem` are expected in a different order than `pvsystem.singlediode`.\r\nThis makes it difficult to pass the parameters to all of these functions using `*args`.\r\n\r\n**Describe the solution you'd like**\r\nGroup and reorder the single diode parameters of `i_from_v`, `v_from_i` to match the order of `singlediode`.\r\n\n",
        "hint": "",
        "base": "30c62e368529df01faa609d6b38456a7b0db9b53",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/ivtools/sdm.py",
            "pvlib/pvsystem.py",
            "pvlib/singlediode.py",
            "pvlib/tools.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1426",
        "problem": "deprecate existing code in forecast.py, possibly replace with solarforecastarbiter shim\n`forecast.py` is a burden to maintain. I haven't used it in years, I don't think any of the other pvlib maintainers are interested in it, and I don't see any users stepping up to volunteer to maintain it. The code is not up to my present standards and I don't see how I'd get it there without a complete rewrite. This leads to difficult to track bugs such as the one recently reported on the [google group](https://groups.google.com/g/pvlib-python/c/b9HdgWV6w6g). It also complicates the pvlib dependencies.\r\n\r\n[solarforecastarbiter](https://github.com/SolarArbiter/solarforecastarbiter-core) includes a [reference_forecasts](https://github.com/SolarArbiter/solarforecastarbiter-core/tree/master/solarforecastarbiter/reference_forecasts) package that is much more robust. See [documentation here](https://solarforecastarbiter-core.readthedocs.io/en/latest/reference-forecasts.html) and [example notebook here](https://github.com/SolarArbiter/workshop/blob/master/reference_forecasts.ipynb) (no promises that this works without modification for the latest version).\r\n\r\nThe main reason to prefer `forecast.py` to `solarforecastarbiter` is the data fetch process. `forecast.py` pulls point data from a Unidata THREDDS server. `solarforecastarbiter.reference_forecasts` assumes you already have gridded data stored in a netcdf file. `solarforecastarbiter.io.nwp` provides functions to fetch that gridded data from NCEP. We have very good reasons for that approach in `solarforecastarbiter`, but I doubt that many `forecast.py` users are interested in configuring that two step process for their application.\r\n\r\nI'm very tempted to stop here, remove `forecast.py` after deprecation, and say \"not my problem anymore\", but it seems to attract a fair number of people to pvlib, so I hesitate to remove it without some kind of replacement. Let's explore a few ideas.\r\n\r\n1. Within `forecast.py`, rewrite code to fetch relevant data from Unidata. Make this function compatible with the expectations for the [`load_forecast`](https://github.com/SolarArbiter/solarforecastarbiter-core/blob/6200ec067bf83bc198a3af59da1d924d4124d4ec/solarforecastarbiter/reference_forecasts/models.py#L16-L19) function passed into `solarforecastarbiter.reference_forecasts.models` functions.\r\n2. Same as 1., except put that code somewhere else. Could be a documentation example, could be in solarforecastarbiter, or could be in a gist.\r\n3. Copy/refactor solarforecastarbiter code into `forecast.py`.\r\n4. Do nothing and let the forecast.py bugs and technical debt pile up. \r\n\r\nOther thoughts?\n",
        "hint": "I used these forecast functions earlier in my career for production forecasting (getting forecasted irradiance data). But that said, everything I used it for can be done with different tools (some already in pvlib). There are some good free/paid weather services out there that return weather forecast data, and pvlib already has functions to determine irradiance from cloud_coverage.  \r\n\r\nWhile it could be tough for some to deprecate `forecast.py`, you have other tools that provide solutions and inputs. I have no problem with it being removed, especially if the amount of work to maintain is greater than the number of users who utilize it.\nProbably not a solution but I recently started a weather data service to more easily access time-series reanalysis (ERA5) and forecast data (GFS) (see example here - https://oikolab.com/documentation). It's currently going through beta testing but I've been thinking about how to offer some of the service to the open-source / academic community. \r\n\r\nIf you have any suggestion, would be more than happy to discuss.\nI've looked at the `forecast.py` code and I'm here to report a bug on it, so yeah, I see the problem.\r\n\r\nI count myself as a newbie user attracted to pvlib in part because of the forecasting. Even so I have no problem with it going away, as long as we have some examples of how to use a different tool for forecasting, if only to be pointed at the existence of the other tool.\r\n\r\nRegarding the ideas posted by @wholmgren. If you have no time to work on it, (4) is your only option. If not, you're clearly not in love with `forecast.py` (\"I have no problem in it going away\"), so I suggest you work toward deprecation and removal. For that, (2) seems the best approach. I also think (2) is the best approach to promote \"separation of concerns\": Getting and formatting the data seems like a useful piece of functionality by itself so it would be good if it existed as such, rather than buried in the `ForecastModel` class.\nI\u2019ve used `forecast.py` and have found it to be the most straightforward way to get data for pv production forecasting.\r\nI played around with `solarforecastarbiter` for around a day.  There appear to be lots of interesting functionality to it but I found the expectation of having externally downloaded data outside of the library\u2019s API stored locally a hurdle to using it. Maybe I didn\u2019t give it the chance it deserved or maybe this is something a documentation example can/does solve, but I\u2019m just echoing your point that part of the allure of `forecast.py` is that it pulls the data for you from an external source, eliminating the need to deal with file management yourself, and allows you to stay within pvlib the whole time.\r\n\r\nA complete and simple forecasting example within pvlib is a powerful use case for the library.  All that said, If the example uses some \u201clean\u201d code from `solarforecastarbiter` that\u2019s probably fine too.\npossibly of interest as an alternative data source: https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-global-forecast-system/\nAfter a year and half of mostly following option 4 (`Do nothing and let the forecast.py bugs and technical debt pile up.`), and another release around the corner, maybe it's time to restart this discussion.  @wholmgren, has the arbiter evolved in a way that changes any of your thoughts at the top of this thread?\r\n\r\nIf the data fetching code needs to live on somewhere, maybe `pvlib.iotools` is as good a place as any.  However I'm not sure that will relieve the maintenance burden much -- of the dozen or so forecast issues opened in the last year, seems like the majority have to do with fetching data.  The dependency complication is for the data fetching as well.\nMy initial reaction to every `forecast.py` issue remains \"remove it.\" Short of that, putting the fetch in `pvlib.iotools` and copying SFA's [`forecast.py`](https://github.com/SolarArbiter/solarforecastarbiter-core/blob/master/solarforecastarbiter/reference_forecasts/forecast.py) (either as a whole module or putting contents in other places like irradiance.py) feels like the best approach. I have no time or interest to maintain the original or any refactored code.\nI also have no interest in maintaining the original, maintaining a refactored version, or doing the refactor itself.  I would be willing to deprecate the contents of `forecast.py` because it's easy and means less maintenance in the long run.\r\n\r\nAny objections to deprecating without replacement?  Any objection to including the deprecation in 0.9.1?\nMy vote would be to move the code to a its own package but I'm with @wholmgren and @kanderso-nrel I don't have the bandwidth to maintain another project.  Maybe we can canvas the user community for volunteers. I support deprecating `forecast.py` perhaps that will encourage someone to come forward to pick it up.",
        "base": "1893b20a7b755004f561037161c242db24e2870c",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/forecast.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/807",
        "problem": "Add Wavelet Variability Model (WVM) for calculating spatial smoothing of irradiance\n> > Should I spin this off to a separate issue, since it might be different (and more compartmented) than the broader downscaling discussion?\r\n> \r\n> Yes. Let's start a new module with this submission, `scaling.py` comes to mind, but I'm not enamored of it. Scope will be functions that operate on irradiance, perhaps other variables, to transform temporal or spatial characteristics.\r\n\r\nSpinoff from [issue #788 ](https://github.com/pvlib/pvlib-python/issues/788). Implementation is a python port of WVM, released as an auxiliary to the Matlab pvlib [here](https://pvpmc.sandia.gov/applications/wavelet-variability-model/). My implementation ports the original model logic, but deviates from the overall package, in that I begin at the point where the user already has a clear sky index to operate on (original starts from GHI and calculates POA clear sky index). I thought this would allow for more flexibility in choice of transposition model, etc, but it does ask a bit more work up front for a user to run the WVM.\r\n\r\nI am close to completion of a draft and will create a pull request when ready. This is my first contribution to the project (or any open source project really), so please accept my apologies in advance if it takes some guidance.\n",
        "hint": "> This is my first contribution to the project (or any open source project really), so please accept my apologies in advance if it takes some guidance.\r\n\r\nWelcome!  Asking for a clear-sky index as input seems appropriate; there's no need to rigidly follow the MATLAB implementation. I'll ask for your patience with the review process, which can involve multiple iterations and reviewers.",
        "base": "e326fa53038f616d949e4f981dab6187d2ca9470",
        "env": "b91d178868d193afd56f8e3b013661a473d699c3",
        "files": [
            "pvlib/scaling.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1138",
        "problem": "document or support modules_per_string strings_per_inverter with pvwatts in modelchain\nHi, \r\n\r\nI am trying to run Modelchain with pvwatt model but it seems that the `modules_per_string` and `strings_per inverter ` doesn't have any affect on the total output. \r\n\r\nI am not sure why is it so. \r\nMay be ModelChain isn't supporting so. If that's the case how can I achieve the desired result?\r\n\r\nHere is my code: \r\n\r\nThanks in advance\r\n```\r\n# built-in python modules\r\nimport os\r\nimport inspect\r\n\r\n# scientific python add-ons\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n# plotting stuff\r\n# first line makes the plots appear in the notebook\r\n%matplotlib inline \r\nimport matplotlib.pyplot as plt\r\nimport matplotlib as mpl\r\n# seaborn makes your plots look better\r\ntry:\r\n    import seaborn as sns\r\n    sns.set(rc={\"figure.figsize\": (12, 6)})\r\n    sns.set_color_codes()\r\nexcept ImportError:\r\n    print('We suggest you install seaborn using conda or pip and rerun this cell')\r\n\r\n# finally, we import the pvlib library\r\nimport pvlib\r\n\r\ntmy = pd.read_csv(\"http://re.jrc.ec.europa.eu/pvgis5/tmy.php?lat=29.74&lon=40.10\")\r\n\r\ntmy.Date = pd.to_datetime(tmy.Date, format='%Y-%d-%m %H:%M:%S')\r\n\r\ntmy.rename(columns={' Ghor':'ghi','Dhor':'dhi','DNI':'dni','Tair':'temp_air',\r\n                        'Ws':'wind_speed'},inplace=True)\t\t\t\t\t\r\n\t\t\t\t\t\t\r\ntmy.set_index(tmy['Date'],inplace=True)\r\n    #Drop unnecessary column\r\n\r\ntmy = tmy.drop('Date', 1)\r\ntmy = tmy.drop('RH', 1)\r\ntmy = tmy.drop('IR', 1)\r\ntmy = tmy.drop(' Wd', 1)\r\ntmy = tmy.drop('Pres', 1)\r\n\r\n#module =Jinko_Solar_JKM320P_72_V\r\n#inverter = ABB__PVS980_58_2000kVA_K__N_A_V__CEC_2018_\r\n\r\nlat = 29.74\r\nlon = 40.10\r\naltitude = 676\r\ntz = 'Etc/GMT+3'  \r\n\r\nloc = pvlib.location.Location(latitude=lat,longitude= lon,tz=tz)\r\n\r\n#model = pvwatts \r\npvwatts_system = pvlib.pvsystem.PVSystem(module_parameters={'pdc0': 320, 'gamma_pdc': -0.0041},inverter_parameters={'pdc' : 3200000, 'pdc0' : 2024292, 'eta_inv_nom':0.988, 'eta_inv_ref':0.986},surface_tilt = 20, surface_azimuth=0,\r\n                    modules_per_string=30,strings_per_inverter=267, albedo = 0.2)\r\n\t\t\t\t\t\r\nmc = pvlib.modelchain.ModelChain(pvwatts_system, loc, transposition_model =\"perez\",aoi_model = 'ashrae',spectral_model='no_loss')\r\nprint(mc)\r\nmc.run_model(times=tmy.index,weather=tmy)\r\na = mc.ac\r\na = pd.Series.to_frame(a)\r\na = a * 530  # 530 = number of inverters in the system \r\n\r\na['month'] = a.index\r\na.month = a.month.dt.month\r\nmonthly = a.groupby('month').sum()\r\n```\r\n\r\n\n",
        "hint": "https://stackoverflow.com/questions/49550656/run-pvlib-modelchain-with-pvwatts-model/50165303#50165303\r\n\r\nThe work around is to scale your ``module_parameters ['pdc0']``. Pull requests welcome for improving the functionality and/or documentation.\nIt seems that the system scaling provided by `PVSystem.scale_voltage_current_power()` is a system-level entity that should be included in `PVSystem.sapm()` and `PVSystem.singlediode()` computations, in addition to adding this to `PVSystem.pvwatts_dc()`. Currently, a higher level `ModelChain` function does this (except for pvwatts, as discussed above). If folks agree to this, then a question arises as to if the corresponding wrapped functions in `pvsystem.py` should still only calculate `singlediode()` for a single module/device instead of the whole system. (ATM, I think that they should.)\n@cwhanse we need this for SPI. Do you have any concern with adding this\r\n\r\n```python\r\n        self.results.dc = self.system.scale_voltage_current_power(\r\n            self.results.dc,\r\n            unwrap=False\r\n        )\r\n```\r\n\r\nto \r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/56971c614e7faea3c24013445f1bf6ffe9943305/pvlib/modelchain.py#L732-L735\r\n\r\n?\r\n\r\nOr do you think we should go ahead with @markcampanelli's suggestion above? I think @markcampanelli's suggestion is better on the merits but it's a much bigger change and I don't know how to do it in a way that wouldn't cause user code to return unexpected answers.\nI don't have a problem patching that into `pvlib.modelchain.ModelChain.pvwatts_dc`. I think it's an oversight that the scaling was left out, since it is included in the `sapm` and `singlediode` methods.\nI think we left it out because it's arguably a departure from the pvwatts model in which you're typically specifying the pdc0 of the entire system. But I don't see a problem with the extension within our data model.\nwant me to open a PR? Or have you got it?\nWould be great if you can do it.",
        "base": "56971c614e7faea3c24013445f1bf6ffe9943305",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1213",
        "problem": "Bug within scaling.py wavelet calculation methodology\n**Describe the bug**\r\nMathematical error within the wavelet computation for the scaling.py WVM implementation. Error arises from the methodology, as opposed to just a software bug. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n```\r\nimport numpy as np\r\nfrom pvlib import scaling\r\ncs = np.random.rand(2**14)\r\nw, ts = scaling._compute_wavelet(cs,1)\r\nprint(np.all( (sum(w)-cs) < 1e-8 ))  # Returns False, expect True\r\n```\r\n\r\n**Expected behavior**\r\nFor a discrete wavelet transform (DWT) the sum of all wavelet modes should equate to the original data. \r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.7.2\r\n - ``pandas.__version__``: 1.2.3\r\n - python: 3.8.8\r\n\r\n**Additional context**\r\nThis bug is also present in the [PV_LIB](https://pvpmc.sandia.gov/applications/wavelet-variability-model/) Matlab version that was used as the basis for this code (I did reach out to them using the PVLIB MATLAB email form, but don't know who actually wrote that code). Essentially, the existing code throws away the highest level of Detail Coefficient in the transform and keeps an extra level of Approximation coefficient. The impact on the calculation is small, but leads to an incorrect DWT and reconstruction. I have a fix that makes the code pass the theoretical test about the DWT proposed under 'To Reproduce' but there may be some question as to whether this should be corrected or left alone to match the MATLAB code it was based on. \r\n\n",
        "hint": "@jranalli thanks for finding and reporting this. Can I ask how you contacted PVLIB MATLAB? Because I maintain that repository and I didn't see the email, so we need to fix something on our end with communications.\n@cwhanse Now that I look again, I think I used the wrong form. It was just a general Questions and Comments link for the PV Performance Modeling Collaborative at the bottom of the page. I didn't see any contact point for the PV_LIB MATLAB library and I also didn't know about the github repo for it, but now I do! \r\n\r\nI do have a fix for the MATLAB code as well, but I don't see that part of the library on github. If you'd like me to open an issue on that repository as well, I'd be happy to do so, but if there's some other pathway or contact point since that's kind of listed as a separate package of the code, please let me know.\r\n\r\nEither way, do you think it's appropriate to fix this, or does there need to be a conversation with the original author of that MATLAB code? If everything is fine to go ahead with it here, I can just put together my fix as a pull request for review.\r\n\r\n\nAnd did my own looking: pvl_WVM is in it's own Matlab archive, separate from PVLIB for Matlab. The WVM code is only available as a download from pvpmc.sandia.gov, whereas PVLIB for Matlab is on [github](https://github.com/sandialabs/MATLAB_PV_LIB).\r\n\r\nI've sent the bug report to Matt Lave, the originator of the WVM algorithm and code. We'll likely welcome the bug fix but I'd like to hear Matt's view first.\nOK sounds good. If he or you want to connect for more detail on the issue, you can get contact info for me at my [Faculty Page](http://personal.psu.edu/jar339/about.html).\nfor the record: bug is confirmed via separate communication with the WVM algorithm author.",
        "base": "40ba4bd5c8b91754aa73e638ed984ab9657847cd",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/scaling.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1707",
        "problem": "regression: iam.physical returns nan for aoi > 90\u00b0 when n = 1\n**Describe the bug**\r\nFor pvlib==0.9.5, when n = 1 (no reflection) and aoi > 90\u00b0, we get nan as result.\r\n\r\n**To Reproduce**\r\n```python\r\nimport pvlib\r\npvlib.iam.physical(aoi=100, n=1)\r\n```\r\nreturns `nan`.\r\n\r\n**Expected behavior**\r\nThe result should be `0`, as it was for pvlib <= 0.9.4.\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: '0.9.5'\r\n - ``pandas.__version__``:  '1.5.3'\r\n - python: 3.10.4\r\n\n",
        "hint": "",
        "base": "40e9e978c170bdde4eeee1547729417665dbc34c",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/iam.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1395",
        "problem": "Add retrieval function for horizon profile from MINES Paris Tech\n<!-- Thank you for your contribution! The following items must be addressed before the code can be merged. Please don't hesitate to ask for help if you're unsure of how to accomplish any of the items. Feel free to remove checklist items that are not relevant to your change. -->\r\n\r\n - [x] I am familiar with the [contributing guidelines](https://pvlib-python.readthedocs.io/en/latest/contributing.html)\r\n - [x] Tests added\r\n - [x] Updates entries to [`docs/sphinx/source/api.rst`](https://github.com/pvlib/pvlib-python/blob/master/docs/sphinx/source/api.rst) for API changes.\r\n - [x] Adds description and name entries in the appropriate \"what's new\" file in [`docs/sphinx/source/whatsnew`](https://github.com/pvlib/pvlib-python/tree/master/docs/sphinx/source/whatsnew) for all changes. Includes link to the GitHub Issue with `` :issue:`num` `` or this Pull Request with `` :pull:`num` ``. Includes contributor name and/or GitHub username (link with `` :ghuser:`user` ``).\r\n - [x] New code is fully documented. Includes [numpydoc](https://numpydoc.readthedocs.io/en/latest/format.html) compliant docstrings, examples, and comments where necessary.\r\n - [x] Pull request is nearly complete and ready for detailed review.\r\n - [x] Maintainer: Appropriate GitHub Labels and Milestone are assigned to the Pull Request and linked Issue.\r\n\r\n<!-- Brief description of the problem and proposed solution (if not already fully described in the issue linked to above): -->\r\n\r\nThe proposed function retrieves the local horizon profile for a specific location (latitude, longitude, and elevation). The returned horizon profile has a resolution of 1 degree in the azimuth direction. The service is provided by MINES ParisTech though I cannot find any official documentation for it.\r\n\r\nThe function added in this PR (``pvlib.iotools.get_mines_horizon``) is very similar to the function added in #1395 (``pvlib.iotools.get_pvgis_horizon``).\n",
        "hint": "@mikofski @cwhanse I saw your discussions in #758 and #1290 and figured I'd share the code I had laying around for downloading the local horizon profile from SRTM. Does this have any interest to you?\nI'm lovin' this! Could we also look into retrieving pvgis horizon data, how do they compare to the SRTM from MINES?",
        "base": "26579bec7e65296223503b9e05da4af914af6777",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/iotools/__init__.py",
            "pvlib/iotools/pvgis.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1216",
        "problem": "`pvsystem.calcparams_cec()` does not propagate parameters\n**Describe the bug**\r\n\r\nThe function calls `calcparams_desoto` with hardcoded reference values.\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/40ba4bd5c8b91754aa73e638ed984ab9657847cd/pvlib/pvsystem.py#L1850-L1855\r\n\r\nThis means the function is silently ignoring its inputs, yielding incorrect results that may go unnoticed.\r\n\r\n\r\n**Expected behavior**\r\n\r\nThe function parameters are propagated into the `calcparams_desoto` call. In particular: `EgRef`, `dEgdT`, `irrad_ref`, `temp_ref`\n",
        "hint": "",
        "base": "01a23e31bcb9e4f844c5877a48cd7681406c6696",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/pvsystem.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1191",
        "problem": "irradiance.aoi can return NaN when module orientation is perfectly aligned with solar position\n**Describe the bug**\r\nI was playing with a dual-axis tracking mount with #1176 and found that when the modules are perfectly aligned with the sun (i.e. AOI should be exactly zero), floating point round-off can result in aoi projection values slightly greater than one, resulting in NaN aoi.  This only happens for some perfectly-aligned inputs (for example tilt=zenith=20, azimuth=180 returns aoi=0 as expected).\r\n\r\n**To Reproduce**\r\n```python\r\nimport pvlib\r\nzenith = 89.26778228223463\r\nazimuth = 60.932028605997004\r\nprint(pvlib.irradiance.aoi_projection(zenith, azimuth, zenith, azimuth))\r\nprint(pvlib.irradiance.aoi(zenith, azimuth, zenith, azimuth))\r\n\r\n# output:\r\n1.0000000000000002\r\nRuntimeWarning: invalid value encountered in arccos:  aoi_value = np.rad2deg(np.arccos(projection))\r\nnan\r\n```\r\n\r\n**Expected behavior**\r\nI expect aoi=0 whenever module orientation and solar position angles are identical.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: `0.9.0-alpha.4+14.g61650e9`\r\n - ``pandas.__version__``: `0.25.1`\r\n - ``numpy.__version__``: `1.17.0`\r\n - python: `3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]`\r\n\r\n**Additional context**\r\nSome ideas for fixes:\r\n1) In `irradiance.aoi_projection`, return a hard-coded `1.0` for inputs within some small tolerance\r\n2) In `irradiance.aoi_projection`, clamp return value to `[-1, +1]`\r\n3) In `irradiance.aoi`, clamp aoi_projection values to `[-1, +1`] before calling `arccos`\r\n4) Rework the `irradiance.aoi_projection` trig equations to not generate impossible values?\n",
        "hint": "",
        "base": "0415365031ca8d0b2867f2a2877e0ad9d7098ffc",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/irradiance.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/823",
        "problem": "backtracking for rare case when sun below tracker improvement\n**Describe the bug**\r\n- related to #656\r\n- in the rare case when the sun rays are below the tracker, then the top of the next row is shaded\r\n- currently tracker backtracks away from sun, back is facing sun instead of front\r\n- this only happens for tilted trackers and very low sun angles, either early morning or late evening when the sun rays are furthest north or south\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create a tilted tracker\r\n```python\r\n# in Brazil so facing north\r\naxis_azimuth = 0.0 \r\naxis_tilt = 20\r\nmax_angle = 75.0\r\ngcr = 0.35\r\n```\r\n2. pick the earliest morning (or latest evening) timestamp\r\n```python\r\nimport pvlib\r\nimport pandas as pd\r\n\r\n# Brazil, timezone is UTC-3[hrs]\r\nstarttime = '2017-01-01T00:30:00-0300'\r\nstoptime = '2017-12-31T23:59:59-0300'\r\nlat, lon = -27.597300, -48.549610\r\ntimes = pd.DatetimeIndex(pd.date_range(\r\n    starttime, stoptime, freq='H'))\r\nsolpos = pvlib.solarposition.get_solarposition(\r\n    times, lat, lon)\r\n# get the early times\r\nts0 = '2017-01-01 05:30:00-03:00'\r\nts1 = '2017-01-01 12:30:00-03:00'\r\napparent_zenith = solpos['apparent_zenith'][ts0:ts1]\r\nazimuth = solpos['azimuth'][ts0:ts1]\r\nsat = pvlib.tracking.singleaxis(\r\n    apparent_zenith, azimuth, axis_tilt, axis_azimuth, max_angle, True, gcr)\r\n```\r\n3. notice that the tracker suddenly jumps from one side facing east to west\r\n```\r\n                           tracker_theta        aoi  surface_azimuth  surface_tilt\r\n2017-01-01 05:30:00-03:00     -21.964540  62.721237       310.299287     29.368272\r\n2017-01-01 06:30:00-03:00      16.231156  69.264752        40.403367     25.546154\r\n2017-01-01 07:30:00-03:00      69.073645  20.433849        82.548858     70.389280\r\n2017-01-01 08:30:00-03:00      54.554616  18.683626        76.316479     56.978562\r\n2017-01-01 09:30:00-03:00      40.131687  17.224233        67.917292     44.072837\r\n2017-01-01 10:30:00-03:00      25.769332  16.144347        54.683567     32.194782\r\n2017-01-01 11:30:00-03:00      11.439675  15.509532        30.610665     22.923644\r\n2017-01-01 12:30:00-03:00      -2.877428  15.358209       351.639727     20.197537\r\n```\r\n\r\n4. AOI is also wrong\r\n\r\n**Expected behavior**\r\nThe tracker should avoid shade. It should not jump from one direction to the other. If the sun ray is below the tracker then it will need to track to it's max rotation or backtrack. If there is shading at it's max rotation then it should track backtrack to zero, or perhaps parallel to the sun rays. Perhaps if bifacial, then it could go backwards, 180 from the correct backtrack position to show it's backside to the sun.\r\n\r\nproposed algorithm (_updated after [this comment](#issuecomment-559154895)_):\r\n```python\r\nif backtracking:\r\n    # cos(R) = L / Lx, R is rotation, L is surface length,\r\n    # Lx is shadow on ground, tracker shades when Lx > x\r\n    # x is row spacing related to GCR, x = L/GCR\r\n    lrot = np.cos(tr_rot_no_lim)  # tracker rotation not limited by max angle\r\n\r\n    # Note: if tr_rot > 90[deg] then lrot < 0 \r\n    # which *can* happen at low angles if axis tilt > 0\r\n    # tracker should never backtrack more than 90[deg], when lrot = 0\r\n    cos_rot = np.minimum(np.abs(lrot) / self.gcr, 1)\r\n\r\n    # so if lrot<0 tracker should backtrack forward\r\n    # backtrack_rot = np.sign(lrot) * np.arccos(cos_rot)\r\n\r\n    # NOTE: updated after comment from @kevinsa5 at Nov 27, 2019, 8:16 AM PST\r\n    # to remove sign()\r\n    backtrack_rot = np.arccos(cos_rot)\r\n```\r\n\r\nalso remove abs from aoi calculation\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/c699575cb6857674f0a96348b77e10c805e741c7/pvlib/tracking.py#L461\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``:  0.6.3\r\n - ``pandas.__version__``:  0.24\r\n - python: 3.7\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\n[STY] remove old comments from single axis tracking\n**Describe the bug**\r\nAfter #823 is merged there may be stale comments in `pvlib.tracking.singleaxis` and commented code that can be removed. This might make the code more readable. It would also resolve some stickler complaints about long lines.\r\n\r\n**To Reproduce**\r\nComments to remove:\r\n1. [L375-L379](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L375-L379) - the tracking algorithm now follows [1] that uses clockwise rotation around z-axis from north\r\n2. [L393-L395](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L393-L395) - ditto\r\n3. [L400-L410](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L400-L410) - ditto\r\n4. [L441-L452](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L441-L452) - pvlib has been using arctan2(x,z) in `pvlib.tracking.singleaxis` for 6 years since 1fb82cc262e43e1d2b55e4b5510a1a5e7e340667, so I believe these comments are unnecessary now\r\n5. [L471-L472](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L471-L472) - this commented code was updated in #823, should we leave it or delete it?\r\n3. [L553-L555](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L553-L555)\r\n\r\netc.\r\n\r\n[1] https://www.nrel.gov/docs/fy20osti/76626.pdf\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: \r\n - ``pandas.__version__``: \r\n - python:\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\n",
        "hint": "\n",
        "base": "a5c24f01e9b420968f5fda0d5c46ce2a4cf2c867",
        "env": "b91d178868d193afd56f8e3b013661a473d699c3",
        "files": [
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1368",
        "problem": "`read_crn` returns -99999 instead of `NaN`\n**Describe the bug**\r\n`read_crn` fails to map -99999 to `NaN`\r\n\r\n**To Reproduce**\r\n```python\r\nfrom pvlib.iotools import read_crn\r\ncrn = read_crn('https://www.ncei.noaa.gov/pub/data/uscrn/products/subhourly01/2021/CRNS0101-05-2021-NY_Millbrook_3_W.txt')\r\ncrn.loc['2021-12-14 0930':'2021-12-14 1130', 'ghi']\r\n2021-12-14 09:30:00+00:00        0.0\r\n2021-12-14 09:35:00+00:00        0.0\r\n2021-12-14 09:40:00+00:00        0.0\r\n2021-12-14 09:45:00+00:00        0.0\r\n2021-12-14 09:50:00+00:00        0.0\r\n2021-12-14 09:55:00+00:00        0.0\r\n2021-12-14 10:00:00+00:00        0.0\r\n2021-12-14 10:05:00+00:00   -99999.0\r\n2021-12-14 10:10:00+00:00   -99999.0\r\n2021-12-14 10:15:00+00:00   -99999.0\r\n2021-12-14 10:20:00+00:00   -99999.0\r\n2021-12-14 10:25:00+00:00   -99999.0\r\n2021-12-14 10:30:00+00:00   -99999.0\r\n2021-12-14 10:35:00+00:00   -99999.0\r\n2021-12-14 10:40:00+00:00   -99999.0\r\n2021-12-14 10:45:00+00:00   -99999.0\r\n2021-12-14 10:50:00+00:00   -99999.0\r\n2021-12-14 10:55:00+00:00   -99999.0\r\n2021-12-14 11:00:00+00:00   -99999.0\r\n2021-12-14 11:05:00+00:00        0.0\r\n2021-12-14 11:10:00+00:00        0.0\r\n2021-12-14 11:15:00+00:00        0.0\r\n2021-12-14 11:20:00+00:00        0.0\r\n2021-12-14 11:25:00+00:00        0.0\r\n2021-12-14 11:30:00+00:00        0.0\r\nName: ghi, dtype: float64\r\n```\r\n\r\n**Expected behavior**\r\nShould return `NaN` instead of -99999\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.0\r\n - ``pandas.__version__``: 1.0.3 (doesn't matter)\r\n - python: 3.7\r\n\r\n**Additional context**\r\n\r\nDocumentation [here](https://www.ncei.noaa.gov/pub/data/uscrn/products/subhourly01/) says\r\n\r\n>          C.  Missing data are indicated by the lowest possible integer for a \r\n>             given column format, such as -9999.0 for 7-character fields with \r\n>             one decimal place or -99.000 for 7-character fields with three\r\n>             decimal places.\r\n\r\nSo we should change \r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/1ab0eb20f9cd9fb9f7a0ddf35f81283f2648e34a/pvlib/iotools/crn.py#L112-L117\r\n\r\nto include -99999 and perhaps -999999. Or do the smarter thing as discussed in the comment.\r\n\r\nalso https://github.com/SolarArbiter/solarforecastarbiter-core/issues/773\n",
        "hint": "",
        "base": "93e84041387c80bf8738fe96409a0cfd5852c29a",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/iotools/crn.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1448",
        "problem": "Output of solarposition.sun_rise_set_transit_ephem depends on installed ephem version\n**Describe the bug**\r\n`pvlib.solarposition.sun_rise_set_transit_ephem` returns a different answer depending on what version of `ephem` is installed. I think the problem is that our wrapper assumes that ephem doesn't pay attention to timezone localization, so it converts the timestamp components to UTC but doesn't bother to change the timestamp's tzinfo:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/8d0f863da92739669e01ac4da4145e4547638b50/pvlib/solarposition.py#L577-L579\r\n\r\nHowever, starting in `ephem==4.1.1` the timezone is no longer ignored ([ref](https://rhodesmill.org/pyephem/CHANGELOG.html#version-4-1-1-2021-november-27)), so the UTC offset is applied twice.  This can shift the timestamp into the next solar period and return the rise/set/transit for the wrong day. \r\n\r\n\r\n**To Reproduce**\r\nSee how the returned sunrise differs by ~24 hours (2019-01-01 vs 2019-01-02) here:\r\n\r\n```python\r\nimport pandas as pd\r\nimport pvlib\r\ntimes = pd.date_range('2019-01-01', freq='h', periods=1, tz='Etc/GMT+8')\r\nout = pvlib.solarposition.sun_rise_set_transit_ephem(times, 40, -120)\r\nprint(out.T)\r\n```\r\n\r\n#### `ephem==4.0.0.1`:\r\n```\r\n\r\n               2019-01-01 00:00:00-08:00\r\nsunrise 2019-01-01 07:21:28.793036-08:00\r\nsunset  2019-01-01 16:45:50.959086-08:00\r\ntransit 2019-01-01 12:03:35.730674-08:00\r\n```\r\n\r\n#### `ephem==4.1.2`:\r\n```\r\n               2019-01-01 00:00:00-08:00\r\nsunrise 2019-01-02 07:21:35.237404-08:00\r\nsunset  2019-01-01 16:45:50.947472-08:00\r\ntransit 2019-01-01 12:03:35.728413-08:00\r\n```\r\n\r\n**Expected behavior**\r\npvlib should give ephem timestamps consistent with its input requirements.  Replacing the above manual utc offset subtraction (which leaves the original tzinfo in place) with `thetime.astimezone(pytz.UTC)` may be suitable for both old and new versions of ephem.  I don't ever use pytz and python datetimes so maybe there's a better alternative.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.1\r\n - python: 3.7\r\n\r\n**Additional context**\r\nThis difference would have popped up back in November when ephem 4.1.1 was released had it not been for #1447.  Here's an example failure: https://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=6027&view=logs&j=e1592cb8-2816-5754-b393-3839a187d454&t=377c4fd6-97bd-5996-bc02-4d072a8786ea&l=2267\r\n\n",
        "hint": "",
        "base": "8d0f863da92739669e01ac4da4145e4547638b50",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/iotools/crn.py",
            "pvlib/solarposition.py",
            "pvlib/tracking.py",
            "setup.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1589",
        "problem": "ZeroDivisionError when gcr is zero\n**Describe the bug**\r\n\r\nThough maybe not intuitive, setting ground coverage ratio to zero is useful when a plant consists of a single shed, e.g. calculating the irradiance on the backside of the panels. However, e.g., `bifacial.infinite_sheds.get_irradiance_poa` fails with `ZeroDivisionError` whenever `gcr=0`.\r\n\r\n**To Reproduce**\r\n\r\n```python\r\nfrom pvlib.bifacial.infinite_sheds import get_irradiance_poa\r\n\r\nget_irradiance_poa(surface_tilt=160, surface_azimuth=180, solar_zenith=20, solar_azimuth=180, gcr=0, height=1, pitch=1000, ghi=200, dhi=200, dni=0, albedo=0.2)\r\n```\r\nreturns:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-7-0cb583b2b311>\", line 3, in <cell line: 3>\r\n    get_irradiance_poa(surface_tilt=160, surface_azimuth=180, solar_zenith=20, solar_azimuth=180, gcr=0, height=1, pitch=1, ghi=200, dhi=200, dni=0, albedo=0.2)\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\bifacial\\infinite_sheds.py\", line 522, in get_irradiance_poa\r\n    vf_shade_sky, vf_noshade_sky = _vf_row_sky_integ(\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\bifacial\\infinite_sheds.py\", line 145, in _vf_row_sky_integ\r\n    psi_t_shaded = masking_angle(surface_tilt, gcr, x)\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\shading.py\", line 56, in masking_angle\r\n    denominator = 1/gcr - (1 - slant_height) * cosd(surface_tilt)\r\nZeroDivisionError: division by zero\r\n```\r\n\r\n**Expected behavior**\r\n\r\nOne can easily solve this `ZeroDivisionError` by multiplying both numerator and denominator with `gcr` inside `shading.masking_angle` and the same inside `bifacial.infinite_sheds._ground_angle`.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: '0.9.3'\r\n - ``pandas.__version__``: '1.4.4'\r\n - python: '3.10.4'\r\n\n",
        "hint": "@kdebrab thanks for investigating this case, which wasn't consider in the implementation. @pvlib/pvlib-core I think we should fix this. \r\n\r\nFor consistency we should also extend `shading.masking_angle_passias` for the limiting case of `gcr=0`. That may be more complicated and could be done in a second PR.\r\n\nI think `infinite_sheds.get_irradiance_poa(...)` converges to `irradiance.get_total_irradiance(..., model='isotropic')` as gcr approaches zero, so that's an option for modeling this situation in the meantime.  \nProbably close enough to be useful, but in theory the rear irradiance would be slightly less than GHI * 1/2 (1 - cos(tilt)), due to the row's shadow.\nWith a more realistic model (e.g. pvfactors) that considers shadows individually that's true, but `infinite_sheds` models ground-reflected irradiance using integrated averages which dilute the effect of the row's own shadow to nothing as gcr approaches zero.  By decreasing `gcr` and increasing `npoints` you can get `infinite_sheds` as close to `get_total_irradiance` as you like:\r\n\r\n![image](https://user-images.githubusercontent.com/57452607/198049857-0c844116-3bc0-48dd-b889-8b658f39b4a0.png)\r\n\r\n<details>\r\n  <summary>Source</summary>\r\n\r\n```python\r\nimport pvlib\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ntimes = pd.date_range('2019-06-01', '2019-06-02', freq='T', tz='Etc/GMT+5')\r\nlocation = pvlib.location.Location(40, -80)\r\nsp = location.get_solarposition(times)\r\ncs = location.get_clearsky(times, solar_position=sp)\r\n\r\nkwargs = dict(\r\n    surface_tilt=20, surface_azimuth=180,\r\n    solar_zenith=sp.apparent_zenith, solar_azimuth=sp.azimuth,\r\n    ghi=cs.ghi, dhi=cs.dhi, dni=cs.dni, albedo=0.2\r\n)\r\n\r\nlimit = pvlib.irradiance.get_total_irradiance(**kwargs, model='isotropic')['poa_global']\r\n\r\nall_stats = []\r\n\r\nfor gcr in [0.3, 0.1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\r\n    stats = dict(gcr=gcr)\r\n    for npoints in [100, 1000, 10000]:\r\n        out = pvlib.bifacial.infinite_sheds.get_irradiance_poa(gcr=gcr, height=1, pitch=1/gcr, npoints=npoints, **kwargs)\r\n        stats[f'npoints={npoints}'] = np.max(np.abs(out['poa_global'] - limit))\r\n\r\n    all_stats.append(stats)\r\n\r\npd.DataFrame(all_stats).set_index('gcr').plot(logx=True, logy=True)\r\nplt.ylabel('Max Absolute Difference [W/m2]')\r\n\r\n```\r\n</details>\n@kanderso-nrel Thanks for pointing this out!\r\n\r\nI think for a `surface_tilt` equal to 20\u00b0 (front side of the panel) this result would be fine, as there is no shadow on the front surface from the own row. However, changing `surface_tilt` to 160\u00b0 (back side of the panel) yields basically the same figure and that is indeed not ideal (and unexpected for me).\r\n\r\nIf I understand well, this is due to the calculation of the ground-reflected irradiance, where the infinite_sheds method uses (1) the **average** 'view factor from the ground to the sky' of the ground between the rows, and (2) the **average** 'fraction of ground between rows that is unshaded', both of which approach 1 as the pitch approaches infinity.\r\n\r\nI think an improved method would not take the average, but some kind of weighted average, considering that the reflection from the ground right in front of the surface is more important than the reflection from the ground further away from the surface. In that case, I would assume that the effect of the row's own shadow would no longer dilute when gcr approaches zero.\n> However, changing surface_tilt to 160\u00b0 (back side of the panel) yields basically the same figure and that is indeed not ideal (and unexpected for me).\r\n\r\nOops, I think I had originally started with rear-side, switched to front-side just to check it as well, and forgot to switch back to rear before posting here.  Good catch.\r\n\r\nFYI pvfactors (`pvlib.bifacial.pvfactors.pvfactors_timeseries`) can model irradiance for single rows (`n_pvrows=1, index_observed_pvrow=0`) and does not suffer from the averaging issues you mention. ",
        "base": "bd86597f62013f576670a452869ea88a47c58c01",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/bifacial/infinite_sheds.py",
            "pvlib/shading.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1031",
        "problem": "expand PVSystem repr\nThe PVSystem repr is\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L239-L243\r\n\r\nThe main issue that I have is that the repr doesn't give me enough information about the temperature model settings. It's relatively important because `temperature_model_params` (not printed) may be inferred from `module_type` (not printed) and `racking_model` (printed). So I'd like to add both `temperature_model_params` and `module_type`.\r\n\r\nWe also don't include `module_parameters`, `inverter_parameters`, and `losses_parameters` in the repr. If I recall correctly, we decided against including these because they can be relatively long. I still think that's reasonable. We could add something like `if len(module_parameters): 'Set. See PVSystem.module_parameters'; else: {}`, but I don't know if that's worth the effort.\n",
        "hint": "",
        "base": "27872b83b0932cc419116f79e442963cced935bb",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/pvsystem.py",
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1349",
        "problem": "`spectrum.spectrl2` calculates negative irradiance for angle of incidence outside +/- 90\u00b0\nWhen using pvlib (but also the spectrl2 implementation provided by NREL), I obtain negative Irradiance for a north-facing panel.\r\nFrom @kevinsa5 's [reply on StackOverflow](https://stackoverflow.com/questions/70172766/pvlib-bird1984-north-facing-element-shows-negative-irradiance/70174010#70174010) I take that this is in fact not intended.\r\n\r\nIn the example code below, the angle of incidence is calculated as values around 115\u00b0, so exceeding a possible (implicitly assumed) +/- 90\u00b0 bound (sun behind panel).\r\n\r\nThis seems to be left open in the original report ([Bird & Riordan, 1984](https://www.nrel.gov/docs/legosti/old/2436.pdf)).\r\n\r\nThe direct irradiance `I_d` (*of a horizontal panel*, Eq 2-1) is obtained by multiplying by cosine of the sun zenith angle. I'd guess that setting that value strictly to zero for angles when cosZ is negative would not be too much of a stretch.\r\n\r\nThen, the direct irradiance `I_d` goes into (Eq 3-18):\r\n\r\n```\r\nI_T(t) = I_d*cos(aoi) + I_s * ( (I_d*cos(aoi) / (H_0*D*cos(Z)) ) + 0.5*(1+cos(t)) * (1 - I_d/(H_0*D)) + 0.5 * I_T0 * r_g * (1-cos(t))\r\n```\r\n\r\nAs such, when you view the angle of incidence `aoi` as the analogue of the sun zenith angle in the prior example, the two first terms of the diffuse irradiation (Eq 3-18) would become zero, which - again - for the direct irradiance would kind of make sense. What remains of (Eq 3-18) would be\r\n\r\n```\r\nI_T(t) = 0 + 0 + 0.5*(1+cos(t))*(1 - 0) + 0.5*I_T0*r_g*(1-cos(t))\r\n```\r\n\r\nI'm not from the field, so I'm very, very wary about the implications of such a work-around suggestion. Can anyone with a proper background comment on this? (Maybe it's the future of air conditioning :-D)\r\n\r\n\r\n**MWE based on the tutorial below**\r\n\r\n```python\r\n## Using PV Lib\r\n\r\nfrom pvlib import spectrum, solarposition, irradiance, atmosphere\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n# assumptions from the technical report:\r\nlat = 49.88\r\nlon = 8.63\r\ntilt = 45\r\nazimuth = 0 # North = 0\r\npressure = 101300  # sea level, roughly\r\nwater_vapor_content = 0.5  # cm\r\ntau500 = 0.1\r\nozone = 0.31  # atm-cm\r\nalbedo = 0.2\r\n\r\ntimes = pd.date_range('2021-11-30 8:00', freq='h', periods=6, tz=\"Europe/Berlin\") # , tz='Etc/GMT+9'\r\nsolpos = solarposition.get_solarposition(times, lat, lon)\r\naoi = irradiance.aoi(tilt, azimuth, solpos.apparent_zenith, solpos.azimuth)\r\n\r\n# The technical report uses the 'kasten1966' airmass model, but later\r\n# versions of SPECTRL2 use 'kastenyoung1989'.  Here we use 'kasten1966'\r\n# for consistency with the technical report.\r\nrelative_airmass = atmosphere.get_relative_airmass(solpos.apparent_zenith,\r\n                                                   model='kasten1966')\r\n\r\nspectra = spectrum.spectrl2(\r\n    apparent_zenith=solpos.apparent_zenith,\r\n    aoi=aoi,\r\n    surface_tilt=tilt,\r\n    ground_albedo=albedo,\r\n    surface_pressure=pressure,\r\n    relative_airmass=relative_airmass,\r\n    precipitable_water=water_vapor_content,\r\n    ozone=ozone,\r\n    aerosol_turbidity_500nm=tau500,\r\n)\r\n\r\nplt.figure()\r\nplt.plot(spectra['wavelength'], spectra['poa_global'])\r\nplt.xlim(200, 2700)\r\n# plt.ylim(0, 1.8)\r\nplt.title(r\"2021-11-30, Darmstadt, $\\tau=0.1$, Wv=0.5 cm\")\r\nplt.ylabel(r\"Irradiance ($W m^{-2} nm^{-1}$)\")\r\nplt.xlabel(r\"Wavelength ($nm$)\")\r\ntime_labels = times.strftime(\"%H:%M %p\")\r\nlabels = [\r\n    \"AM {:0.02f}, Z{:0.02f}, {}\".format(*vals)\r\n    for vals in zip(relative_airmass, solpos.apparent_zenith, time_labels)\r\n]\r\nplt.legend(labels)\r\nplt.show()\r\n```\r\n\r\n![Figure_ne](https://user-images.githubusercontent.com/15192310/144224709-dea899e4-435e-4ff2-a3de-9e9524b28eb8.png)\r\n\r\n\n",
        "hint": "Thanks @cweickhmann!  I want to take a closer look at the technical report to be sure, but on a first glance I think the problem here is the same one solved by the line marked with `# GH 526` in `irradiance.haydavies`:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/aba071f707f9025882e57f3e55cc9e3e90e869b2/pvlib/irradiance.py#L811-L816\r\n\r\nNote that, even though `spectrum.spectrl2` uses `irradiance.haydavies` under the hood, the above branch is not hit because `spectrl2` passes in a pre-calculated `projection_ratio`.  So I think clipping the projection to be non-negative before passing it to `haydavies` would solve the problem.  The `# GH 432` line might be desirable as well, though I don't think it's relevant for this issue.  \r\n\r\nDoes anyone have qualms about us deviating from the reference by implementing that fix and making a note about it in the docstring?  `aoi > 90` is hardly an uncommon occurrence, even for arrays that aren't high-latitude and facing north. \n> deviating from the reference by implementing that fix and making a note about it \r\n\r\nI support that.",
        "base": "aba071f707f9025882e57f3e55cc9e3e90e869b2",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/spectrum/spectrl2.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/980",
        "problem": "pvlib.soiling.hsu model implementation errors\n**Describe the bug**\r\nI ran an example run using the Matlab version of the HSU soiling function and found that the python version did not give anywhere near the same results.  The Matlab results matched the results in the original JPV paper.  As a result of this test, I found two errors in the python implementation, which are listed below:\r\n\r\n1.  depo_veloc = {'2_5': 0.004, '10': 0.0009} has the wrong default values.  They are reversed.\r\nThe proper dictionary should be: {'2_5': 0.0009, '10': 0.004}.  This is confirmed in the JPV paper and the Matlab version of the function.\r\n\r\n2. The horiz_mass_rate is in g/(m^2*hr) but should be in g/(m^2*s).  The line needs to be multiplied by 60x60 or 3600.\r\nThe proper line of code should be: \r\nhoriz_mass_rate = (pm2_5 * depo_veloc['2_5']+ np.maximum(pm10 - pm2_5, 0.) * depo_veloc['10'])*3600\r\n\r\nWhen I made these changes I was able to match the validation dataset from the JPV paper, as shown below.\r\n![image](https://user-images.githubusercontent.com/5392756/82380831-61c43d80-99e6-11ea-9ee3-2368fa71e580.png)\r\n\r\n\n",
        "hint": "nice sleuthing Josh! Is a PR forthcoming? \ud83c\udf89 \nHi Mark,\r\n                Yes, a PR is in the works.  I need to improve the testing first.\r\n\r\n-Josh\r\n\r\nFrom: Mark Mikofski <notifications@github.com>\r\nReply-To: pvlib/pvlib-python <reply@reply.github.com>\r\nDate: Tuesday, May 19, 2020 at 3:51 PM\r\nTo: pvlib/pvlib-python <pvlib-python@noreply.github.com>\r\nCc: Joshua Stein <jsstein@sandia.gov>, Author <author@noreply.github.com>\r\nSubject: [EXTERNAL] Re: [pvlib/pvlib-python] pvlib.soiling.hsu model implementation errors (#970)\r\n\r\n\r\nnice sleuthing Josh! Is a PR forthcoming? \ud83c\udf89\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/pvlib/pvlib-python/issues/970#issuecomment-631102921>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ABJES5C2CRTZFF7ROT2EPOTRSL5ORANCNFSM4NFL4K3Q>.\r\n\nNow I need to go back and figure out where I missed these errors in the review.",
        "base": "75369dcabacb6c6c38790cc23825f33f155ad1a9",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/soiling.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1048",
        "problem": "warnings in test_sdm\nhttps://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=4054&view=logs&j=fc432b8b-e2e3-594e-d8b1-15597b6c1d62&t=309866e1-2cf4-5f00-3d0a-999fc3a0f279&l=209\r\n\r\nthrough\r\n\r\nhttps://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=4054&view=logs&j=fc432b8b-e2e3-594e-d8b1-15597b6c1d62&t=309866e1-2cf4-5f00-3d0a-999fc3a0f279&l=295\r\n\r\nSo almost 100 lines of warnings.\n",
        "hint": "Not bugs per se, the algorithm handles non-convergence when looping through the IV curves by continuing, although the rattling and squeaks are audible. I'd prefer to keep that behavior and silencing divide by 0 and invalid (nan) value warnings in place, if that's OK.\nNo problem with the algorithm, just its rattles and squeaks. Does it need to emit its own more useful warnings?\nIt could, but the causes for non-convergence may not be easy to identify and state. The returned dict includes a Boolean array that is False for IV curves for which it didn't get parameters. It could warn that the array is non-empty, but that seems unnecessary to me.",
        "base": "7fc595a13bcd42e3269c0806f5505ac907af9730",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/ivtools/__init__.py",
            "pvlib/ivtools/sde.py",
            "pvlib/ivtools/sdm.py",
            "pvlib/ivtools/utils.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1623",
        "problem": "Add variable mapping of read_tmy3\n**Is your feature request related to a problem? Please describe.**\r\nThis PR proposes that a `map_variables` parameter be added to the `read_tmy3` function. Additionally, the current `rename_columns` parameter (which removes the units from the column names) should be deprecated. See #714 for a discussion on the topic.\r\n\r\n**Describe the solution you'd like**\r\nA `map_variables` parameter should be added (defaulting to None), and if specified as True then it should override the `rename_columns` parameter and map the column names to standard pvlib names. A deperecation warning should be added stating that the `rename_columns` parameter will be retired starting in pvlib 0.11.0 - the deprecation warning should be silenced if `map_variables` is specified as either True or False.\r\n\n",
        "hint": "",
        "base": "30c62e368529df01faa609d6b38456a7b0db9b53",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "docs/examples/adr-pvarray/plot_simulate_system.py",
            "docs/examples/irradiance-decomposition/plot_diffuse_fraction.py",
            "docs/examples/irradiance-transposition/plot_seasonal_tilt.py",
            "docs/examples/irradiance-transposition/plot_transposition_gain.py",
            "docs/examples/soiling/plot_greensboro_kimber_soiling.py",
            "pvlib/iotools/tmy.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1273",
        "problem": "Incorrect AOI from pvlib.tracking.singleaxis\n`pvlib.tracking.singleaxis` produces an incorrect AOI when the sun is above the earth horizon but behind the module plane.\r\n\r\n**To Reproduce**\r\nModel a fixed tilt system (90 tilt, 180 azimuth) and compare to a vertical single axis tracker with very small rotation limit.\r\n\r\n```\r\n\r\nimport pandas as pd\r\nimport pytz\r\nimport pvlib\r\nfrom matplotlib import pyplot as plt\r\n\r\nloc = pvlib.location.Location(40.1134, -88.3695)\r\n\r\ndr = pd.date_range(start='02-Jun-1998 00:00:00', end='02-Jun-1998 23:55:00',\r\n                   freq='5T')\r\ntz = pytz.timezone('Etc/GMT+6')\r\ndr = dr.tz_localize(tz)\r\nhr = dr.hour + dr.minute/60\r\n\r\nsp = loc.get_solarposition(dr)\r\n\r\ncs = loc.get_clearsky(dr)\r\n\r\ntr = pvlib.tracking.singleaxis(sp['apparent_zenith'], sp['azimuth'],\r\n                               axis_tilt=90, axis_azimuth=180, max_angle=0.01,\r\n                               backtrack=False)\r\n\r\nfixed = pvlib.irradiance.aoi(90, 180, sp['apparent_zenith'], sp['azimuth'])\r\n\r\nplt.plot(hr, fixed)\r\nplt.plot(hr, tr['aoi'])\r\nplt.plot(hr, sp[['apparent_elevation']])\r\nplt.show()\r\n\r\nplt.legend(['aoi - fixed', 'aoi - tracked', 'apparent_elevation'])\r\n```\r\n\r\n**Expected behavior**\r\nThe AOI for the fixed tilt system shows values greater than 90 when the sun is behind the module plane. The AOI from `singleaxis` does not.\r\n\r\nI think the source of the error is the use of `abs` in [this ](https://github.com/pvlib/pvlib-python/blob/ca61503fa83e76631f84fb4237d9e11ae99f3c77/pvlib/tracking.py#L446)line.\r\n\r\n**Screenshots**\r\n![aoi_fixed_vs_tracked](https://user-images.githubusercontent.com/5393711/117505270-01087a80-af41-11eb-9220-10cccf2714e1.png)\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.8.1\r\n\r\nFirst reported by email from Jim Wilzcak (NOAA) for the PVlib Matlab function [pvl_singleaxis.m](https://github.com/sandialabs/MATLAB_PV_LIB/blob/master/pvl_singleaxis.m)\r\n\n",
        "hint": "I agree that `abs` looks like a problem.\nThis is an ancient line first committed April 3, 2015:\r\nhttps://github.com/pvlib/pvlib-python/blob/46f69bf9ae2869d75f74664684b4de3b3b3e9bf2/pvlib/tracking.py#L219\nYes, my initial port of the matlab code was as close to 1:1 as I could make it. I don't recall second guessing the `abs` at the time, but I certainly should have.\nWe (Dan and I) concluded that line is in error, in the matlab code.\nShould we just replace that entire line with a call to `irradiance.aoi`?\n@kanderso-nrel the shortcoming with using `irradiance.aoi` afaict is that it calls [`irradiance.aoi_projection`](https://github.com/pvlib/pvlib-python/blob/0e6fea6219618c0da944e6ed686c10f5b1e244a2/pvlib/irradiance.py#L153) which is redundant because the single axis tracker already calculates the solar vector and rotates it into the tracker reference frame to use the `x` and `z` components to calculate the tracker rotation. \r\n\r\nCOINCIDENTALLY, `irradiance.aoi` is already used in the `SingleAzisTracker` method `get_aoi` which afaict should be a static or class method because it NEVER uses self. I guess that's a separate issue. Anyway, it says this method isn't necessary, b/c `singleaxis` already returns AOI.\r\n\r\nALSO, I was thrown for a bit in `irradiance.aoi_projection` which doesn't have a lot of commentary, because when calculating the dot product of surface normal and solar vector, it shows `z=cos(tilt)*cos(ze)` first and `x=sin(tilt)*sin(ze)*cos(az-surfaz)` last. Whatever\r\n\r\nAnyway, back to this, should we consider adjusting `irradiance.aoi` to allow the user to pass in the AOI projection as an alternate parameter? Seems a bit like a hacky workaround, but something like this:\r\n\r\n```python\r\n# in irradiance.py\r\ndef aoi(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth,\r\n       projection=None):\r\n    if projection is None:\r\n        projection = aoi_projection(\r\n            surface_tilt, surface_azimuth, solar_zenith, solar_azimuth)\r\n    # from here it's the same\r\n    aoi_value = np.rad2deg(np.arccos(projection))\r\n    ...\r\n```\r\n\r\nthen in `singleaxis` we change it to this:\r\n\r\n```python\r\n    # calculate angle-of-incidence on panel\r\n    # aoi = np.degrees(np.arccos(np.abs(np.sum(sun_vec*panel_norm, axis=0))))\r\n    projection = (xp * panel_norm[0]\r\n                  + yp * panel_norm[1]\r\n                  + zp * panel_norm[2])\r\n    # can't use np.dot for 2D matrices\r\n    # expanding arrays is about 1.5x faster than sum\r\n    # can skip sun_vec array formation, but still need panel norm for later\r\n    aoi = irradiance.aoi(None, None, None, None, projection=projection)\r\n```\nor maybe just to get this ball roling we use clip for now and just close it with a #TODO",
        "base": "0e6fea6219618c0da944e6ed686c10f5b1e244a2",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1186",
        "problem": "getter/parser for PVGIS hourly-radiation\n**Is your feature request related to a problem? Please describe.**\r\nRelated to #845 \r\n\r\n**Describe the solution you'd like**\r\nSimilar to `get_pvgis_tmy` retrieve pvgis hourly radiation data from their api\r\n\r\n**Describe alternatives you've considered**\r\nPvgis is becoming a popular resource more and more people are asking me for it, it is nice because it is a global collection of several different radiation databases including nsrdb and others, and different from cams, the data is complete, ie it has air temperature, wind speed as well as all 3 components of irradiance\r\n\r\n**Additional context**\r\nThis would be part of the `iotool` sub-package. There's already a `pvgis.py` module with a getter for tmy be data\n",
        "hint": "",
        "base": "d5d8ffaa9d1201fc943bb0b3e29f7179a65ed995",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/iotools/__init__.py",
            "pvlib/iotools/pvgis.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1822",
        "problem": "singlediode: newton solver fails with Series input of length one\nThe vectorized newton solver doesn't work if parameters are Series of length one.\r\n\r\n```\r\n\r\nimport pandas as pd\r\nimport pvlib\r\n\r\n\r\nargs = (0.001, 1.5, 6., 5e-9, 1000., 0.5)\r\nparams = pvlib.pvsystem.calcparams_desoto(1000., 25, *args)\r\nparams_series = pvlib.pvsystem.calcparams_desoto(pd.Series(data=[1000.]),\r\n                                                 pd.Series([25.]), *args)\r\nparams_series2 = pvlib.pvsystem.calcparams_desoto(pd.Series(data=[1000., 1000.]),\r\n                                                  pd.Series([25., 25.]), *args)\r\n# works with each input as float\r\nresult = pvlib.pvsystem.singlediode(*params, method='newton')\r\n\r\n# works with Series if length > 1\r\nresult_series2 = pvlib.pvsystem.singlediode(*params_series2, method='newton')\r\n\r\n# errors with Series if length is 1\r\nresult_series = pvlib.pvsystem.singlediode(*params_series, method='newton')\r\n```\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.5\r\n\n",
        "hint": "",
        "base": "d2fbfb247979282ba1fba6794dec451c0b1e8d57",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/pvsystem.py",
            "pvlib/singlediode.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1165",
        "problem": "ValueError: ModelChain.run_from_effective_irradiance([weather]) when only providing temp_air and wind_speed\n**Describe the bug**\r\nAccording to the (new) docstring for `ModelChain.run_from_effective_irradiance`, cell temperature can be calculated from temperature_model using `'effective_irradiance'`. This is not the case when using one or more arrays \r\nhttps://github.com/pvlib/pvlib-python/blame/master/pvlib/modelchain.py#L1589-L1606\r\n\r\n**To Reproduce**\r\n```python\r\nfrom copy import deepcopy\r\nimport pandas as pd\r\nfrom pvlib.location import Location\r\nfrom pvlib.pvsystem import Array, PVSystem\r\nfrom pvlib.modelchain import ModelChain\r\n\r\n\r\narray_params = {\r\n    \"surface_tilt\": 32.0,\r\n    \"surface_azimuth\": 180.0,\r\n    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\r\n    \"albedo\": 0.2,\r\n    \"temperature_model_parameters\": {\r\n        \"u_c\": 29.0,\r\n        \"u_v\": 0.0,\r\n        \"eta_m\": 0.1,\r\n        \"alpha_absorption\": 0.9,\r\n    },\r\n    \"strings\": 5,\r\n    \"modules_per_string\": 7,\r\n    \"module_parameters\": {\r\n        \"alpha_sc\": 0.004539,\r\n        \"gamma_ref\": 1.2,\r\n        \"mu_gamma\": -0.003,\r\n        \"I_L_ref\": 5.11426,\r\n        \"I_o_ref\": 8.10251e-10,\r\n        \"R_sh_ref\": 381.254,\r\n        \"R_sh_0\": 400.0,\r\n        \"R_s\": 1.06602,\r\n        \"cells_in_series\": 96,\r\n        \"R_sh_exp\": 5.5,\r\n        \"EgRef\": 1.121,\r\n    },\r\n}\r\ninverter_parameters = {\r\n    \"Paco\": 250.0,\r\n    \"Pdco\": 259.589,\r\n    \"Vdco\": 40.0,\r\n    \"Pso\": 2.08961,\r\n    \"C0\": -4.1e-05,\r\n    \"C1\": -9.1e-05,\r\n    \"C2\": 0.000494,\r\n    \"C3\": -0.013171,\r\n    \"Pnt\": 0.075,\r\n}\r\n\r\n\r\nlocation = Location(latitude=33.98, longitude=-115.323, altitude=2300)\r\n\r\narray_sys = PVSystem(\r\n    arrays=[\r\n        Array(**array_params, name=0),\r\n    ],\r\n    inverter_parameters=inverter_parameters,\r\n)\r\nweather = pd.DataFrame(\r\n    {\r\n        \"effective_irradiance\": [1100.0, 1101.0],\r\n        \"temp_air\": [25.0, 26.0],\r\n        \"wind_speed\": [10.0, 10.0],\r\n    },\r\n    index=pd.DatetimeIndex(\r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\r\n    ),\r\n)\r\nmc0 = ModelChain(\r\n    array_sys,\r\n    location,\r\n    aoi_model=\"no_loss\",\r\n    spectral_model=\"no_loss\",\r\n)\r\nmc1 = deepcopy(mc0)\r\n\r\nmc0.run_model_from_effective_irradiance(weather)\r\nassert isinstance(mc0.results.cell_temperature, pd.Series)\r\n\r\n\r\nmc1.run_model_from_effective_irradiance([weather])  # ValueError\r\n\r\n```\r\n\r\n**Expected behavior**\r\nRunning the model with both `weather` and `[weather]` work\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.0-alpha.2+5.gb40df75\n",
        "hint": "@cwhanse we overlooked updating this in #1129:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/b40df75ddbc467a113b87643c1faef073cc37b3e/pvlib/modelchain.py#L1594-L1598\r\n\r\nOne possible solution is \r\n\r\n```python\r\nif any(p is None for p in poa):\r\n    raise ValueError\r\n```",
        "base": "b40df75ddbc467a113b87643c1faef073cc37b3e",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/modelchain.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1026",
        "problem": "make read_crn accomodate bad files\nA couple of issues with our `read_crn` function. \r\n\r\nFirst, the character sequence '\\x00\\x00\\x00\\x00\\x00\\x00' occasionally shows up and trips up pandas. This can be fixed by adding `na_values=['\\x00\\x00\\x00\\x00\\x00\\x00']` to the reader.\r\n\r\nSecond, we try to set the `CRX_VN` column to dtype int, but it occasionally has floats that cannot be coerced. The [documentation](https://www1.ncdc.noaa.gov/pub/data/uscrn/products/subhourly01/README.txt) says it should be treated like a string.\r\n\r\nExample below shows both issues in `'CRNS0101-05-2020-FL_Titusville_7_E.txt'`\r\n\r\n```\r\n92821 20200706 1145 20200706 0645      3  -80.69   28.62    24.5     0.0    151 0    24.7 C 0    94 0 -99.000 -9999.0   990 0   1.23 0\r\n92821 20200706 1150 20200706 0650      3  -80.69   28.62    24.7     0.0    168 0    25.0 C 0    94 0 -99.000 -9999.0   990 0   1.28 0\r\n92821 20200706 1155 20200706 0655      3  -80.69   28.62    24.9     0.0    173 0    25.3 C 0    93 0 -99.000 -9999.0   990 0   1.48 0\r\n92821 20200706 1200 20200706 0700      3  -80.69   28.62    24.9     0.0    190 0    25.5 C 0    93 0 -99.000 -9999.0   990 0   1.57 0\r\n\\x00\\x00\\x00\\x00\\x00\\x00 repeated\r\n92821 20200706 1305 20200706 0805  2.623  -80.69   28.62    26.8     0.0    409 0    30.0 C 0    87 0 -99.000 -9999.0   988 0   1.44 0\r\n92821 20200706 1310 20200706 0810  2.623  -80.69   28.62    26.9     0.0    430 0    30.2 C 0    87 0 -99.000 -9999.0   989 0   1.64 0\r\n92821 20200706 1315 20200706 0815  2.623  -80.69   28.62    27.0     0.0    445 0    30.4 C 0    86 0 -99.000 -9999.0   989 0   1.94 0\r\n92821 20200706 1320 20200706 0820  2.623  -80.69   28.62    27.3     0.0    463 0    30.8 C 0    86 0 -99.000 -9999.0   988 0   1.50 0\r\n92821 20200706 1325 20200706 0825  2.623  -80.69   28.62    27.6     0.0    478 0    31.1 C 0    85 0 -99.000 -9999.0   988 0   1.54 0\r\n92821 20200706 1330 20200706 0830  2.623  -80.69   28.62    27.6     0.0    496 0    31.5 C 0    84 0 -99.000 -9999.0   988 0   1.48 0\r\n```\r\n\r\nfyi @lboeman \n",
        "hint": "",
        "base": "27872b83b0932cc419116f79e442963cced935bb",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/iotools/crn.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1616",
        "problem": "IAM that supports AR coating like Fresnel\n# Problem\r\nCurrently pvlib supports the DeSoto physical model (similar to normal glass), ASHRAE, Martin & Ruiz, and SAPM polynomial, but it doesn't have a pure Fresnel model that allows additional interfaces like an AR coating. \r\n\r\n* DeSoto physical model is most similar to the Fresnel for normal glass but only has one interface, so is limited to IAM curves below it only, while an AR coating would have a greater &rho; \r\n* Martin & Ruiz could be used to approximate an AR coated glass if the correct `a_r` were known. The default of `a_r=0.16` is slightly above the normal glass Fresnel IAM, but an `a_r=0.14` seems to match an AR coating with index of refraction of 1.2 most closely.\r\n\r\n![pvlib_iam](https://user-images.githubusercontent.com/1385621/180581071-0ff411f1-144a-40b6-a6a9-189ef55f019f.png)\r\n\r\n\r\n# Proposal\r\na new method in `pvl.iam.fresnel_ar(aoi, n_ar=1.2, n_air=1.0, n_glass=1.56)` that implements the [Fresnel equation](https://en.wikipedia.org/wiki/Fresnel_equations)\r\n\r\n# Alternative\r\nSuggest readers to use Martin & Ruiz with `a_r=0.14` instead of default.\r\n\r\n# additional content\r\nPVsyst has switched to Fresnel equations. We can duplicate [their methods](https://www.pvsyst.com/help/iam_loss.htm) ignoring additional reflections and the encapsulant layer:\r\n![Fresnel-v-ASHRAE](https://user-images.githubusercontent.com/1385621/180581112-67f3ed9d-5bd3-4dfe-8180-8b5d173fcdd2.png)\r\n\r\n<details>\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nplt.ion()\r\n\r\n\r\n# constants\r\nn_glass = 1.56\r\nn_air = 1.0\r\ntheta_inc = np.linspace(0, 88, 100)\r\n\r\n\r\ndef snell(theta_1, n1, n2):\r\n    \"\"\"Snell's equation\"\"\"\r\n    sintheta_2 = n1/n2 * np.sin(np.radians(theta_1))\r\n    return sintheta_2, np.degrees(np.arcsin(sintheta_2))\r\n\r\n\r\ndef refl_s(theta_1, theta_2, n1, n2):\r\n    \"\"\"Fresnel's equation\"\"\"\r\n    n1_costheta_1 = n1*np.cos(np.radians(theta_1))\r\n    n2_costheta_2 = n2*np.cos(np.radians(theta_2))\r\n    return np.abs((n1_costheta_1 - n2_costheta_2)/(n1_costheta_1 + n2_costheta_2))**2\r\n\r\n\r\ndef refl_p(theta_1, theta_2, n1, n2):\r\n    \"\"\"Fresnel's equation\"\"\"\r\n    n1_costheta_2 = n1*np.cos(np.radians(theta_2))\r\n    n2_costheta_1 = n2*np.cos(np.radians(theta_1))\r\n    return np.abs((n1_costheta_2 - n2_costheta_1)/(n1_costheta_2 + n2_costheta_1))**2\r\n\r\n\r\ndef refl_eff(rs, rp):\r\n    \"\"\"effective reflectivity\"\"\"\r\n    return (rs+rp)/2\r\n\r\n\r\ndef trans(refl):\r\n    \"\"\"transmissivity\"\"\"\r\n    return 1-refl\r\n\r\n\r\ndef refl0(n1, n2):\r\n    \"\"\"reflectivity at normal incidence\"\"\"\r\n    return np.abs((n1-n2)/(n1+n2))**2\r\n\r\n\r\ndef fresnel(theta_inc, n1=n_air, n2=n_glass):\r\n    \"\"\"calculate IAM using Fresnel's Law\"\"\"\r\n    _, theta_tr = snell(theta_inc, n1, n2)\r\n    rs = refl_s(theta_inc, theta_tr, n1, n2)\r\n    rp = refl_p(theta_inc, theta_tr, n1, n2)\r\n    reff = refl_eff(rs, rp)\r\n    r0 = refl0(n1, n2)\r\n    return trans(reff)/trans(r0)\r\n\r\n\r\ndef ashrae(theta_inc, b0=0.05):\r\n    \"\"\"ASHRAE equation\"\"\"\r\n    return 1 - b0*(1/np.cos(np.radians(theta_inc)) - 1)\r\n\r\n\r\ndef fresnel_ar(theta_inc, n_ar, n1=n_air, n2=n_glass):\r\n    \"\"\"calculate IAM using Fresnel's law with AR\"\"\"\r\n    # use fresnel() for n2=n_ar\r\n    _, theta_ar = snell(theta_inc, n1, n_ar)\r\n    rs_ar1 = refl_s(theta_inc, theta_ar, n1, n_ar)\r\n    rp_ar1 = refl_p(theta_inc, theta_ar, n1, n_ar)\r\n    r0_ar1 = refl0(n1, n_ar)\r\n    # repeat with fresnel() with n1=n_ar\r\n    _, theta_tr = snell(theta_ar, n_ar, n2)\r\n    rs = refl_s(theta_ar, theta_tr, n_ar, n2)\r\n    rp = refl_p(theta_ar, theta_tr, n_ar, n2)\r\n    # note that combined reflectivity is product of transmissivity!\r\n    # so... rho12 = 1 - (1-rho1)(1-rho2) \r\n    reff = refl_eff(1-(1-rs_ar1)*(1-rs), 1-(1-rp_ar1)*(1-rp))\r\n    r0 = 1-(1-refl0(n_ar, n2))*(1-r0_ar1)\r\n    return trans(reff)/trans(r0)\r\n\r\n\r\n# plot Fresnel for normal glass and ASHRAE\r\nplt.plot(theta_inc, fresnel(theta_inc))\r\nplt.plot(theta_inc, ashrae(theta_inc))\r\n\r\n# calculate IAM for AR with n=1.1 and plot\r\niam_ar11 = fresnel_ar(theta_inc, n_ar=1.1)\r\nplt.plot(theta_inc, iam_ar11)\r\n\r\n# repeat for AR with n=1.2\r\niam_ar12 = fresnel_ar(theta_inc, n_ar=1.2)\r\nplt.plot(theta_inc, iam_ar12)\r\n\r\n# make plot pretty\r\nplt.legend(['Fresnel, normal glass', 'ASHRAE, $b_0=0.05$', 'Fresnel $n_{AR}=1.1$', 'Fresnel $n_{AR}=1.2$'])\r\nplt.title(\"IAM correction, Fresnel vs. ASHRAE, using basic eqn's\")\r\nplt.ylabel('IAM')\r\nplt.xlabel(r'incidence angle $\\theta_{inc} [\\degree]$')\r\nplt.grid()\r\nplt.ylim([0.55,1.05])\r\n```\r\n</details>\r\n\r\n\n",
        "hint": "+1. This [reference](https://www.sciencedirect.com/science/article/abs/pii/S0038092X96001375) might be relevant.\nI seem to recall from somewhere that PVsyst actually interpolates from a fixed set of pre-calculated values when simulating.\nPVsyst allows a user specified custom IAM v AOI lookup table in the module PAN file, but that presupposes there exist qualified IAM measurements either from a lab or the manufacturer. Otherwise they use Fresnel as of v6.67. See https://www.pvsyst.com/help/iam_loss.htm\nYes, what I meant is that they use the Fresnel equations to populate the table for interpolation. At least this is my recollection.",
        "base": "25af86599845f23b0ce57dc1cbe743b3a1e68d1a",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/iam.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1740",
        "problem": "Update CAMS/SoDa URL\nSoDa has developed a new load-balancing solution, such that requests are automatically redirected to the fastest server. This means that it might be advisable for us to update the URL in the [``pvlib.iotools.get_cams``](https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.iotools.get_cams.html?highlight=get_cams#pvlib.iotools.get_cams) function. \r\n\r\nEmail from SoDa (March 7th, 2023):\r\n> Our beta load balancing system for SoDa/CAMS API requests is extended to March 13th. All requests made on the beta-api.soda-solardata.com WILL NOT BE COUNTED in your subscription. The beta access will last until then. **From March 14th, the service will be fully operational and you have to use api.soda-solardata.com to process your API (machine to machine) requests.**\r\n\r\nand email from February 22nd, 2023:\r\n> This new functionality will automatically redirect any request to the fastest available SoDa server. As a result, future updates/maintenances won't need any action from your part as server switches will be completely autonomous.\r\n\r\nI will be following up on this issue in a couple of weeks.\r\n\r\n*Edit: email from March 20th, 2023*\r\n> We strongly advise you to switch your automatic commands on the load balancing system (api.soda-solardata.com). In that way, all future updates won't need any actions from your side. \n",
        "hint": "",
        "base": "275e6718caf7486cb5b7dcf29acd59499ad51f7f",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/iotools/sodapro.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1225",
        "problem": "Improve docstring or behavior for irradiance.get_total_irradiance and irradiance.get_sky_diffuse\n`pvlib.irradiance.get_total_irradiance` accepts kwargs `dni_extra` and `airmass`, both default to `None`. However, values for these kwargs are required for several of the irradiance transposition models. \r\n\r\nSee discussion [here](https://groups.google.com/d/msg/pvlib-python/ZPMdpQOD6F4/cs1t23w8AwAJ)\r\n\r\nDocstring should specify when `dni_extra` and `airmass` are required, and which airmass is appropriate for each model.\r\n\r\nCould also test for kwarg values if e.g. `model=='perez'`\n",
        "hint": "Consider also specifying it is relative airmass.\nThis came up again in @mikofski's thread [here](https://groups.google.com/g/pvlib-python/c/AUQ_hsbL_B8/m/0y83YIRrBgAJ). \r\n\r\n@mikofski proposed calculating values if needed and not provided.\r\n\r\n@kanderso-nrel proposed a couple of solutions to provide more informative error messages: 1. hard coding the failure modes in `get_sky_diffuse` and 2. using a decorator to communicate the failure modes.\r\n\r\nAnd repeating my take from the thread:\r\n\r\nI'd rather not introduce the complexity of decorators to the lower level pvlib functions to solve this problem.\r\n\r\nIdeas that I support:\r\n* calculate if not provided (Mark's original idea)\r\n* reraising a more informative message.\r\n* require all of the arguments in get_sky_diffuse and get_total_irradiance regardless of whether or not they're used.\r\n* remove the arguments from get_sky_diffuse and get_total_irradiance and do the calculation if it's needed\r\n* ~remove get_sky_diffuse and get_total_irradiance. I'm not convinced they're a net positive for the library.~ (let's try to fix it before throwing up our hands)\r\n\r\nDoes someone want to tackle this in 0.8.0 or should we kick it down the road?\nI think it's tempting to add it in v0.8, but I'm in favor of freezing features now and pushing out the release sooner with the features we already have queued. It's been a while, and I think we should deploy more often with less features per release. I believe this will make it easier to blame issues and get more testing done on new features and fixes faster.\nI'll volunteer to take this up for v0.8.1, since I was the complainer.\n`PVSystem.get_irradiance` has some relevant shim code:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/04a523fafbd61bc2e49420963b84ed8e2bd1b3cf/pvlib/pvsystem.py#L289-L294",
        "base": "0415365031ca8d0b2867f2a2877e0ad9d7098ffc",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/irradiance.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1093",
        "problem": "ModelChain.prepare_inputs can succeed with missing dhi\nFrom the docstring for `ModelChain.prepare_inputs()` I believe the method should fail if `weather` does not have a `dhi` column.\r\n\r\nThe validation checks for `'ghi'` twice, but not `'dhi`'\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/11c356f9a89fc88b4d3ff368ce1aae170a97ebd7/pvlib/modelchain.py#L1136\n",
        "hint": "",
        "base": "11c356f9a89fc88b4d3ff368ce1aae170a97ebd7",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/modelchain.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1193",
        "problem": "TypeError: running ModelChain with Arrays and module_temperature\n**Describe the bug**\r\nAnother bug using Arrays. This time a TypeError is raised in `pvlib.modelchain._get_cell_temperature` because `self.system.temperature_model_parameters` is zipped with dataframe tuples but is never a tuple itself\r\nhttps://github.com/pvlib/pvlib-python/blob/dc617d0c182bc8eec57898a039cb5115b425645f/pvlib/modelchain.py#L1525\r\n\r\n**To Reproduce**\r\n```python\r\nimport traceback\r\nimport pandas as pd\r\nfrom pvlib.location import Location\r\nfrom pvlib.pvsystem import Array, PVSystem\r\nfrom pvlib.modelchain import ModelChain\r\n\r\n\r\narray_params = {\r\n    \"surface_tilt\": 32.0,\r\n    \"surface_azimuth\": 180.0,\r\n    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\r\n    \"albedo\": 0.2,\r\n    \"temperature_model_parameters\": {\r\n        \"a\": -3.47,\r\n        \"b\": -0.0594,\r\n        \"deltaT\": 3.0,\r\n    },\r\n    \"strings\": 5,\r\n    \"modules_per_string\": 7,\r\n    \"module_parameters\": {\r\n        \"alpha_sc\": 0.004539,\r\n        \"gamma_ref\": 1.2,\r\n        \"mu_gamma\": -0.003,\r\n        \"I_L_ref\": 5.11426,\r\n        \"I_o_ref\": 8.10251e-10,\r\n        \"R_sh_ref\": 381.254,\r\n        \"R_sh_0\": 400.0,\r\n        \"R_s\": 1.06602,\r\n        \"cells_in_series\": 96,\r\n        \"R_sh_exp\": 5.5,\r\n        \"EgRef\": 1.121,\r\n    },\r\n}\r\ninverter_parameters = {\r\n    \"Paco\": 250.0,\r\n    \"Pdco\": 259.589,\r\n    \"Vdco\": 40.0,\r\n    \"Pso\": 2.08961,\r\n    \"C0\": -4.1e-05,\r\n    \"C1\": -9.1e-05,\r\n    \"C2\": 0.000494,\r\n    \"C3\": -0.013171,\r\n    \"Pnt\": 0.075,\r\n}\r\n\r\n\r\nlocation = Location(latitude=33.98, longitude=-115.323, altitude=2300)\r\n\r\narray_sys = PVSystem(\r\n    arrays=[\r\n        Array(**array_params, name=0),\r\n    ],\r\n    inverter_parameters=inverter_parameters,\r\n)\r\npoa = pd.DataFrame(\r\n    {\r\n        \"poa_global\": [1100.0, 1101.0],\r\n        \"poa_direct\": [1000.0, 1001.0],\r\n        \"poa_diffuse\": [100.0, 100.0],\r\n        \"module_temperature\": [35.0, 33.0],\r\n    },\r\n    index=pd.DatetimeIndex(\r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\r\n    ),\r\n)\r\nstandard = poa.copy().rename(\r\n    columns={\"poa_global\": \"ghi\", \"poa_direct\": \"dni\", \"poa_diffuse\": \"dhi\"}\r\n)\r\neffective = poa.copy()[[\"module_temperature\", \"poa_global\"]].rename(\r\n    columns={\"poa_global\": \"effective_irradiance\"}\r\n)\r\nmc = ModelChain(\r\n    array_sys,\r\n    location,\r\n    aoi_model=\"no_loss\",\r\n    spectral_model=\"no_loss\",\r\n)\r\ntry:\r\n    mc.run_model([standard])\r\nexcept TypeError:\r\n    print(traceback.format_exc())\r\nelse:\r\n    raise RuntimeError(\"expected a type error\")\r\ntry:\r\n    mc.run_model_from_poa([poa])\r\nexcept TypeError:\r\n    print(traceback.format_exc())\r\nelse:\r\n    raise RuntimeError(\"expected a type error\")\r\ntry:\r\n    mc.run_model_from_effective_irradiance([effective])\r\nexcept TypeError:\r\n    print(traceback.format_exc())\r\nelse:\r\n    raise RuntimeError(\"expected a type error\")\r\n\r\n```\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``:  master/g684b247\r\n\n",
        "hint": "",
        "base": "684b2478e4e174c5eeb10d49a709bbe947921abc",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/modelchain.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1176",
        "problem": "make Array play nicely with fixed tilt systems and trackers\n#1076 is adding an `Array` class that largely describes a fixed-tilt array. However, the composition logic of `PVSystem: def __init__(arrays,...)` combined with the inheritance logic of `SingleAxisTracker(PVSystem)` makes for an odd combination of `Array` objects within `SingleAxisTrackers`. See, for example, https://github.com/pvlib/pvlib-python/pull/1076#discussion_r539704316. \r\n\r\nIn https://github.com/pvlib/pvlib-python/pull/1076#discussion_r539686448 I proposed roughly:\r\n\r\nSplit the `Array` into `BaseArray`, `FixedTiltArray(BaseArray)`, `SingleAxisTrackingArray(BaseArray)`? Basic idea:\r\n\r\n```python\r\nclass FixedTiltArray(BaseArray)\r\n    \"\"\"\r\n    Parameters\r\n    ----------\r\n    surface_tilt: float or array-like, default 0\r\n        Surface tilt angles in decimal degrees.\r\n        The tilt angle is defined as degrees from horizontal\r\n        (e.g. surface facing up = 0, surface facing horizon = 90)\r\n\r\n    surface_azimuth: float or array-like, default 180\r\n        Azimuth angle of the module surface.\r\n        North=0, East=90, South=180, West=270.\r\n\r\n    **kwargs\r\n        Passed to Array. Or copy remainder of Array doc string to be explicit.\r\n    \"\"\"\r\n\r\n\r\n# could be in pvsystem.py (module is gradually becoming just the objects) or could be in tracking.py\r\nclass SingleAxisTrackerArray(BaseArray)\r\n    \"\"\"\r\n    Parameters\r\n    ----------\r\n    axis_tilt : float, default 0\r\n        The tilt of the axis of rotation (i.e, the y-axis defined by\r\n        axis_azimuth) with respect to horizontal, in decimal degrees.\r\n\r\n    etc.\r\n\r\n    **kwargs\r\n        Passed to Array. Or copy remainder of Array doc string to be explicit.\r\n    \"\"\"\r\n```\r\n\r\nI believe the only major challenge is that the `get_aoi` and `get_irradiance` methods would either need to differ in signature (as they do now, and thus present a challenge to a `PVSystem` wrapper) or in implementation (tracker methods would include a call to `singleaxis`, and thus would be less efficient in some workflows). @wfvining suggests that the consistent signature is more important and I'm inclined to agree.\r\n\r\nWe'd also deprecate the old `SingleAxisTracking` class.\r\n\r\nWe should resolve this issue before releasing the new Array code into the wild in 0.9.\n",
        "hint": "I like the idea of depreciating the `SingleAxisTracking` class and wrapping tracking functionality more directly into `PVSystem` and `Array`. I don't quite picture yet how it would work on the user side. They can directly create a `SingleAxisTrackerArray` and then pass that to a `PVSystem`? Or create a `FixedTiltArray` and pass that? \r\n\r\nI think we should keep in mind though that when you have a tracking system you are probably very likely going to have a uniform system and a single `Array` per `PVsystem`. So if I am going to create a tracking PVSystem, I am likely going to want to create it straight from a `PVSystem` as the most direct route rather than having to create the array first. (Unless the intent is to depreciate that functionality eventually and push always creating an `Array` first).  In that sense, keeping `SingleAxisTracker` as a `PVSystem` class and just having it create a `SingleAxisTrackingArray` instead of a `Array` may be more user friendly. But I do think there is opportunity to come up with a system to wrap everything together better. \r\n\r\nI also like the simplicity of `Array` and `PVsystem`, and worry about now adding different types of `Array`. \r\n\r\nJust throwing this out there, what if `Array` had a `tracking_model` attribute that right now could be either `fixed` or `single_axis`? Depending on what is passed it sets the appropriate `get_iam` and `get_irradiance` methods, and initiates the appropriate default attributes (`surface_tilt`, `surface_azimuth`, `axis_angle`, `max_angle` etc)? \n> They can directly create a SingleAxisTrackerArray and then pass that to a PVSystem? Or create a FixedTiltArray and pass that?\r\n\r\nYes.\r\n\r\n> I think we should keep in mind though that when you have a tracking system you are probably very likely going to have a uniform system and a single Array per PVsystem. \r\n\r\nTrue. The main application that I can see for mixing a `SingleAxisTrackerArray` with something else is for modeling systems with a mix of broken and working trackers, so it would look something like:\r\n\r\n```python\r\nother_params = {}  # module_parameters, etc\r\nPVSystem([\r\n    Array(surface_tilt=45, surface_azimuth=90, **other_params), \r\n    SingleAxisTrackerArray(**other_params)\r\n])\r\n```\r\n\r\n> So if I am going to create a tracking PVSystem, I am likely going to want to create it straight from a PVSystem as the most direct route rather than having to create the array first. (Unless the intent is to depreciate that functionality eventually and push always creating an Array first). In that sense, keeping SingleAxisTracker as a PVSystem class and just having it create a SingleAxisTrackingArray instead of a Array may be more user friendly. \r\n\r\nWe discussed deprecating that functionality but haven't committed to it. \r\n\r\n> Just throwing this out there, what if Array had a tracking_model attribute that right now could be either fixed or single_axis? Depending on what is passed it sets the appropriate get_iam and get_irradiance methods, and initiates the appropriate default attributes (surface_tilt, surface_azimuth, axis_angle, max_angle etc)?\r\n\r\nInteresting idea. I could see this working at the `PVSystem` level so that you can retain the ability to create the system with a single function call despite the removal of `SingleAxisTracker`.",
        "base": "f80068c86b9fc14c9395b52fac39a8e9fae5ef27",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/pvsystem.py",
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1157",
        "problem": "ModelChainResult.cell_temperature is not always a pandas.Series \nFor a `ModelChain` configured as below, the `cell_temperature` when running the model chain with a list of data like `ModelChain.run_model([data])` is a tuple with a single number instead of the expected Series\r\n\r\n**To Reproduce**\r\n```python\r\nimport pandas as pd                                                                                                                                                                                                                                           \r\nfrom pvlib.location import Location                                                                                                                                                                                                                           \r\nfrom pvlib.pvsystem import PVSystem, Array                                                                                                                                                                                                                    \r\nfrom pvlib.modelchain import ModelChain                                                                                                                                                                                                                       \r\n                                                                                                                                                                                                                                                              \r\ndata = pd.DataFrame(                                                                                                                                                                                                                                          \r\n    {                                                                                                                                                                                                                                                         \r\n        \"ghi\": [1100.0, 1101.0],                                                                                                                                                                                                                              \r\n        \"dni\": [1000.0, 1001],                                                                                                                                                                                                                                \r\n        \"dhi\": [100.0, 100],                                                                                                                                                                                                                                  \r\n        \"temp_air\": [25.0, 25],                                                                                                                                                                                                                               \r\n        \"wind_speed\": [10.0, 10],                                                                                                                                                                                                                             \r\n    },                                                                                                                                                                                                                                                        \r\n    index=pd.DatetimeIndex(                                                                                                                                                                                                                                   \r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]                                                                                                                                                                      \r\n    ),                                                                                                                                                                                                                                                        \r\n)                                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                              \r\narray_params = {                                                                                                                                                                                                                                              \r\n    \"name\": None,                                                                                                                                                                                                                                             \r\n    \"surface_tilt\": 20.0,                                                                                                                                                                                                                                     \r\n    \"surface_azimuth\": 180.0,                                                                                                                                                                                                                                 \r\n    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",                                                                                                                                                                                                                \r\n    \"albedo\": 0.2,                                                                                                                                                                                                                                            \r\n    \"temperature_model_parameters\": {                                                                                                                                                                                                                         \r\n        \"u_c\": 29.0,                                                                                                                                                                                                                                          \r\n        \"u_v\": 0.0,                                                                                                                                                                                                                                           \r\n        \"eta_m\": 0.1,                                                                                                                                                                                                                                         \r\n        \"alpha_absorption\": 0.9,                                                                                                                                                                                                                              \r\n    },                                                                                                                                                                                                                                                        \r\n    \"strings\": 5,                                                                                                                                                                                                                                             \r\n    \"modules_per_string\": 7,                                                                                                                                                                                                                                  \r\n    \"module_parameters\": {                                                                                                                                                                                                                                    \r\n        \"alpha_sc\": 0.004539,                                                                                                                                                                                                                                 \r\n        \"gamma_ref\": 1.2,                                                                                                                                                                                                                                     \r\n        \"mu_gamma\": -0.003,                                                                                                                                                                                                                                   \r\n        \"I_L_ref\": 5.11426,                                                                                                                                                                                                                                   \r\n        \"I_o_ref\": 8.10251e-10,                                                                                                                                                                                                                               \r\n        \"R_sh_ref\": 381.254,                                                                                                                                                                                                                                  \r\n        \"R_sh_0\": 400.0,                                                                                                                                                                                                                                      \r\n        \"R_s\": 1.06602,                                                                                                                                                                                                                                       \r\n        \"cells_in_series\": 96,                                                                                                                                                                                                                                \r\n        \"R_sh_exp\": 5.5,                                                                                                                                                                                                                                      \r\n        \"EgRef\": 1.121,                                                                                                                                                                                                                                       \r\n    },                                                                                                                                                                                                                                                        \r\n}\r\ninverter_parameters = {                                                                                                                                                                                                                                       \r\n    \"Paco\": 250.0,                                                                                                                                                                                                                                            \r\n    \"Pdco\": 259.589,                                                                                                                                                                                                                                          \r\n    \"Vdco\": 40.0,                                                                                                                                                                                                                                             \r\n    \"Pso\": 2.08961,                                                                                                                                                                                                                                           \r\n    \"C0\": -4.1e-05,                                                                                                                                                                                                                                           \r\n    \"C1\": -9.1e-05,                                                                                                                                                                                                                                           \r\n    \"C2\": 0.000494,                                                                                                                                                                                                                                           \r\n    \"C3\": -0.013171,                                                                                                                                                                                                                                          \r\n    \"Pnt\": 0.075,                                                                                                                                                                                                                                             \r\n}                                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                                              \r\nlocation = Location(latitude=33.98, longitude=-115.323, altitude=2300)                                                                                                                                                                                        \r\n                                                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                                              \r\narray_sys = PVSystem(                                                                                                                                                                                                                                         \r\n    arrays=[Array(**array_params)], inverter_parameters=inverter_parameters                                                                                                                                                                                   \r\n)                                                                                                                                                                                                                                                             \r\nassert isinstance(                                                                                                                                                                                                                                            \r\n    ModelChain(array_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\")                                                                                                                                                                            \r\n    .run_model(data)                                                                                                                                                                                                                                          \r\n    .results.cell_temperature,                                                                                                                                                                                                                                \r\n    pd.Series,                                                                                                                                                                                                                                                \r\n)                                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                              \r\narray_run = ModelChain(                                                                                                                                                                                                                                       \r\n    array_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\"                                                                                                                                                                                        \r\n).run_model([data])                                                                                                                                                                                                                                           \r\nassert array_run.results.cell_temperature == array_run.cell_temperature                                                                                                                                                                                       \r\nprint(array_run.results.cell_temperature)  # (45.329789874660285,)                                                                                                                                                                                            \r\n                                                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                                              \r\narray_params[\"strings_per_inverter\"] = array_params.pop(\"strings\")                                                                                                                                                                                            \r\nstandard_sys = PVSystem(**array_params, inverter_parameters=inverter_parameters)                                                                                                                                                                              \r\nassert isinstance(                                                                                                                                                                                                                                            \r\n    ModelChain(standard_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\")                                                                                                                                                                         \r\n    .run_model(data)                                                                                                                                                                                                                                          \r\n    .results.cell_temperature,                                                                                                                                                                                                                                \r\n    pd.Series,                                                                                                                                                                                                                                                \r\n)                                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                              \r\nstandard_run = ModelChain(                                                                                                                                                                                                                                    \r\n    standard_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\"                                                                                                                                                                                     \r\n).run_model([data])                                                                                                                                                                                                                                           \r\nassert standard_run.results.cell_temperature == standard_run.cell_temperature                                                                                                                                                                                 \r\nprint(standard_run.results.cell_temperature)  # (45.329789874660285,)                                                                                                                                                                                         \r\nassert not isinstance(standard_run.results.cell_temperature, pd.Series)                                                                                                                                                                                       \r\n                                                                                \r\n```\r\n\r\n**Expected behavior**\r\n`type(ModelChain.run_model([data]).results.cell_temperature) == pd.Series`\r\n__\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``:  0.8.1+4.gba4a199\r\n - ``pandas.__version__``:  1.1.4\r\n - python: 3.8.5\r\n\n",
        "hint": "Confirmed. This is a bug in `pvlib.modelchain.ModelChain._prepare_temperature` not all inputs are tuples and aren't being converted. @wfvining fyi and lmk if you want to fix it.\nDefinitely a bug, but I think the correct behavior is slightly different than you expect. Because you pass a list to `ModelChain.run_model()` the output should be a tuple with a single `pd.Series` element.\nI agree that passing data as a tuple/list should result in all results being tuples of Series/DataFrames. So perhaps this is a separate bug, but ``array_run.results.total_irrad`` (and other results properties) is a singular `pd.DataFrame` instead of `Tuple[pd.DataFrame]`. \nYes, that's the problem. We should make those match the type of `ModelChain.weather`. Not sure about this, but we might be able to pass ~`unwrap=isinstance(self.weather, tuple)`~ `unwrap=not isinstance(self.weather, tuple)` to the `PVSystem` methods to accomplish this. Probably won't be that easy though, since `ModelChain` interacts heavily with `PVSystem` attributes like `temperature_model_params` which can't accept that kwarg.\nPerhaps a related error is raised when running \r\n```python\r\npoa_data = pd.DataFrame(\r\n    {\r\n        \"poa_global\": [1100.0, 1101.0],\r\n        \"poa_direct\": [1000.0, 1001],\r\n        \"poa_diffuse\": [100.0, 100],\r\n        \"module_temperature\": [25.0, 25],\r\n    },\r\n    index=pd.DatetimeIndex(\r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\r\n    ),\r\n)\r\n\r\nModelChain(\r\n    array_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\",\r\n).run_model_from_poa([poa_data]) \r\n```\r\nraises a TypeError here https://github.com/pvlib/pvlib-python/blob/6b92d218653633e366241c31e8836c0072739ece/pvlib/modelchain.py#L1102-L1106\r\nbecause `self.results.aoi_modifier = 1.0` and `self.results.spectral_modifier = 1.0` \r\nhttps://github.com/pvlib/pvlib-python/blob/6b92d218653633e366241c31e8836c0072739ece/pvlib/modelchain.py#L904-L909\nYup, that's the *first* place this error creeps in. That needs to change, as well as the methods that apply loss models from `PVSystem`.  `ModelChain._prep_inputs_fixed()` also needs updating, as well as `_prepate_temperature` itself. I suspect there are one or two other places as well.\r\n\r\nI'm kicking myself a bit for not considering this corner case initially. Very glad you found it early on.\nOne simple solution might be to add some indirection when assigning to `ModelChain.results`. Instead of assigning directly, assignments go through an `_assign_result(field, value)` method that ensures the type of `value` matches `ModelChain.weather`. This might not be the *best* option, but it would not be too difficult to implement.",
        "base": "0b8f24c265d76320067a5ee908a57d475cd1bb24",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1737",
        "problem": "SolarAnywhere File -- pvlib.iotools.read_tmy3 Bug\n@AdamRJensen \r\n\r\nThere's a bug report for reading SolarAnywhere Files with using the pvlib.iotools.read_tmy3 function. This bug is in the TMY3 file (I think?)\r\n\r\n\r\n![TMY3](https://github.com/pvlib/pvlib-python/assets/74630912/1f85b014-a40a-42af-9c07-76e51ccc606e)\r\n\n",
        "hint": "Interesting, did this come up during the 2023 PVPMC workshop? These functions are meant to read the legacy TMY files from the [NSRDB archives](https://nsrdb.nrel.gov/data-sets/archives), but because SolarAnywhere TGY files are similar, you may also be able to read them with `pvlib.iotools.read_tmy3()` but you might need to edit the SolarAnywhere file to match the NSRDB TMY3 file format more closely. Can you paste the exact error message you got when trying to read the SolarAnywhere file?\nThe error appeared when one of the hackathon participants tried to read [this SolarAnywhere](https://raw.githubusercontent.com/PVSC-Python-Tutorials/PVPMC_2023/main/data/SolarAnywhere%20Typical%20DNI%20Year%20Lat_40_75%20Lon_-111_85%20TMY3%20format.csv) file.\r\n\r\nThere was an unknown character in the `meta['USAF']` value that couldn't be converted to an int.\r\n\r\nHowever, I cannot reproduce the error.  I wonder if it was caused by the way the participant saved that file locally.\r\n\nI can't reproduce the error either.\r\n\r\n@PGRenewables what version of pvlib were you using? You can check that by writing ``pvlib.__version__``.\r\nAlso, if you could upload the file you saved locally, that would be great.\nRegardless, encoding problems are nightmarish at times, I wonder if we should dedicate a short troubleshooting section to it as perhaps we've already done for time zones?\n@AdamRJensen \r\n\r\nPVLib Version  0.9.2\r\n\r\n[PVLib Tutorial.zip](https://github.com/pvlib/pvlib-python/files/11456496/PVLib.Tutorial.zip)\r\n\r\nI included the files that I was working with in this zip file.\n@PGRenewables Thanks!  Can you upload the data file itself as well?  Since they can't reproduce the error with the original file, a next step is to check whether there is some difference between the original data file and the one you're using.  \nHi @kandersolar , this is the file that I was using. Thank you so much! \r\n\r\n\r\n[SolarAnywhere Typical DNI Year Lat_40_75 Lon_-111_85_TMY3_format.csv](https://github.com/pvlib/pvlib-python/files/11466930/SolarAnywhere.Typical.DNI.Year.Lat_40_75.Lon_-111_85_TMY3_format.csv)\r\n\n`print(open(filename, 'r').read(5))` shows that the file @PGRenewables just uploaded differs from the original in that it starts with `\u00ef\u00bb\u00bf`, which google says is a [Byte Order Mark](https://en.wikipedia.org/wiki/Byte_order_mark#Byte_order_marks_by_encoding) for UTF-8.  I also notice that the date format inside the CSV data is different in this file (`01/01/2004` versus `1/1/2004`).  I speculate that the data got read into some tool that understands CSV (Excel?) and then re-exported before getting used with pvlib?\r\n\r\nThe internet suggests using `encoding='utf-8-sig'` for files with this BOM.  We could try to handle this case, but I'm nervous that `read_tmy3` will turn into a rat's nest if it keeps trying to handle encoding issues itself.  I still think exposing `encoding` as an optional parameter is worth considering (https://github.com/pvlib/pvlib-python/pull/1494#issuecomment-1194011363).",
        "base": "5119b4281fa9de8a4dc97002b5c10a6d73c25a4f",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/iotools/tmy.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1852",
        "problem": "Add `min_angle` argument to `tracking.singleaxis`\nIn `tracking.singleaxis` the minimum angle of the tracker is assumed to be opposite of the maximum angle, although in some cases the minimum angle could be different. NREL SAM doesn't support that but PVsyst does.\r\n\r\nIn order to support non symmetrical limiting angles, `tracking.singleaxis` should have another, optional, input, `min_angle`. By default, if not supplied (i.e. value is `None`), the current behavior (`min_angle = -max_angle`) would apply.\r\n\r\nCan I propose a PR for this, with modifications to `tracking.singleaxis`, `tracking.SingleAxisTracker` and to `pvsystem.SingleAxisTrackerMount` + corresponding tests?\n",
        "hint": "I'm in favor of pvlib being able to handle asymmetrical rotation limits in principle, but I'm curious what situation has that asymmetry in practice.  @MichalArieli do you have a particular real-world application in mind?\r\n\r\nRather than separate `min_` and `max_` parameters, I think I'd favor a single parameter that accepts a tuple as @cwhanse suggested here: https://github.com/pvlib/pvlib-python/pull/823#issuecomment-561399605.  I'm not sure about renaming `max_angle` to something else though.  `singleaxis(..., max_angle=(-40, 45))` seems okay to me.  And since symmetrical limits is by far the more common case, I think the parameter should continue accepting a single value (in which case symmetry is assumed) in addition to a tuple.\r\n\r\n`tracking.SingleAxisTracker` is being removed anyway (#1771), so no point in making any additions there.  Whatever changes we decide on here should only be made to `tracking.singleaxis` and `pvsystem.SingleAxisTrackerMount` (and tests, of course).\n@kandersolar  Thanks for the quick response! \r\n\r\nRegarding handling asymmetry in rotation limits, let's take the example of a tracker placed at a 90-degree axis azimuth, tracking south-north. If the sun is at azimuth 80 degrees during sunrise, the algorithm will guide the tracker to briefly turn north. To prevent that we can implement a maximum angle limit for northward movement to ensure smooth and continuous motion and taking into account the time needed for such a large angular change.\r\n\r\nI agree its better to have a single parameter that accepts a tuple/ single value. Would you like me to apply these changes and send for a PR? \r\n\n> Would you like me to apply these changes and send for a PR?\r\n\r\nPlease do!",
        "base": "f8b129418025b47ad669ba4802d03348a275e329",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/pvsystem.py",
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1469",
        "problem": "ModelChain should accept albedo in weather dataframe\n**Is your feature request related to a problem? Please describe.**\r\nAlbedo is treated as a scalar constant in pvlib, but it is of course a function of the weather and changes throughout the year.  Albedo is currently set in the PVSystem or Array and cannot be altered using the ModelChain.  Albedo is provided as a timeseries from many weather data services as well as through NREL's NSRBD and it would be useful to provide this data to the ModelChain.\r\n\r\nAdditionally, treating albedo as property of the Array seems to conflict with the [PVSystem Design Philosophy](https://pvlib-python.readthedocs.io/en/stable/pvsystem.html#design-philosophy), which highlights the separation of the PV system and the exogenous variables, such as the weather.\r\n\r\n**Describe the solution you'd like**\r\nModelChain.run_model() should accept albedo in the weather dataframe, like temperature and ghi.\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative we have implemented is calling ModelChain.run_model() on each row of a dataframe and manually updating the albedo of the array in each tilmestep.  This probably has some side effects that we are unaware of.\r\n\n",
        "hint": "I think I agree that it would make more sense to pass around ground albedo with the weather data instead of treating it as a characteristic of the array.  \r\n\r\n> Albedo is treated as a scalar constant in pvlib\r\n\r\n> An alternative we have implemented is calling ModelChain.run_model() on each row of a dataframe and manually updating the albedo of the array in each tilmestep. \r\n\r\nIt is true that the docs for `Array` and `PVSystem` say that `albedo` is a float, but I think it also works to set albedo to a time series that matches the weather you pass to `ModelChain.run_model()`.  At least that saves you from looping w/ scalar albedo values.  \nI agree that albedo can change with e.g., precipitation or season, but it is also a property of the bare ground surface, and it is that perspective that put albedo with the system parameters. One use case for not having albedo in the weather data would be to evaluate the effect of different ground cover on array output.\r\n\r\nI am only away of the NSRDB offering albedo with weather data; are there other sources?\r\n\r\nI'm +1 on allowing `albedo` to be a Series. I'm neutral on bundling albedo with weather data, but I don't see a better option.  We only have two data structures that supply `ModelChain`: the `weather` DataFrame, and the `PVSystem` instance. I don't think it is practical to create a third just for `albedo`, and it isn't any more work to add or modify `albedo` to `weather` than it is to extract `albedo` from downloaded weather data and add it to `PVSystem`.\r\n\r\n\nTo clarify my above message, I think it *already* works to set `PVSystem.albedo` or `Array.albedo` to a Series, despite the docs saying it must be float.  \r\n\r\n> are there other sources?\r\n\r\nA non-exhaustive list of examples: [SolarAnywhere](https://www.solaranywhere.com/support/data-fields/albedo/), [SolarGIS](https://solargis.com/docs/getting-started/data-parameters), [MERRA2](https://disc.gsfc.nasa.gov/datasets/M2TMNXRAD_5.12.4/summary)\nDoes anyone know if including time-specific albedo this has been shown to be even more important with bifacials?\r\n\r\n(I would think yes.)\n> Does anyone know if including time-specific albedo this has been shown to be even more important with bifacials?\r\n> \r\n> (I would think yes.)\r\n\r\nYes, it is more important than for single-sided modules. There are ground surfaces where the albedo depends on the solar elevation and hence time of day.\r\n\r\nOne caution about albedo from satellite-derived irradiance: those values are at least km^2 scale, and are observed from space, whereas a PV model is assuming that the albedo is localized (m^2) and has been determined from the irradiance reaching the ground. [SolarAnywhere ](https://www.solaranywhere.com/support/data-fields/albedo/)provides an informative list of caveats.\r\n\r\nThe good news is that the uncertainty in albedo is typically secondary to uncertainty in other data such as GHI, when considering uncertainty in energy production.\r\n\nWhich is the better course of action?\r\n\r\n1. Leave `albedo` on `PVsystem` or `Array`. Edit the docstrings and add tests to make it explicit that `PVSystem.albedo` or `Array.albedo` can be a Series. Advantages: avoids deprecating and removing `PVSystem.albedo`. Downside: users obtaining albedo from weather data sources have an extra step to perform when using `ModelChain` methods.\r\n2. Add `albedo` as an optional column in `weather`, and have `ModelChain` methods use `weather['albedo']` instead of `PVSystem.albedo` when `weather['albedo']` is present. Advantages: convenient for ModelChain users, and avoids deprecating `PVsystem.albedo`. Disadvatanges: potential for confusion when a user also assigns `PVSystem.albedo`. \r\n3. Move `albedo` from `PVSystem` to `weather`. Same advantages as 2, but requires deprecation, and adds an extra step for users who aren't getting albedo with weather data, and would probably lead to adding code to `ModelChain` similar to the code that infers albedo from `PVSystem.surface_type`.\n@cwhanse I lean toward option 1. However, it seems that for option 1, a user might have to duplicate computations already done in the `ModelChain` in the preliminary step of using the weather to compute the existing `PVSystem.albedo` (as some formulaic combination of ground conditions and weather). Do you know if this is really an issue?\nThanks for the discussion around this feature.  I favor Option 2, noting that PVSystem.albedo is already an optional argument.  Option 1 is prone to bugs and cases need to be handled checking for mismatches in series/dataframe lengths or perhaps same series lengths but different indices.  I would discourage Option 3, as it seems there is both a basis for and utility in retaining albedo as a property of PVSystem.\nFor option 2, I would say raise an exception (or at very least a warning) if two albedos are specified. This could definitely be the source of a subtle computational bug when users don't realize a weather file they pulled is doing something they didn't intend.\n+1 for option 2, and I agree with @campanelli-sunpower that an exception should be raised if two albedos are specified\n+1 for option 2, but I like keeping the old method for the case where you just want to set either a single annual albedo or monthly albedos (tho not sure if this option currently exists). I agree also raise exception if both are specified - we can always change it later.",
        "base": "04e3ffd5ed2b8504e45ee0dc6bc7194d2440012d",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/clearsky.py",
            "pvlib/irradiance.py",
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py",
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1666",
        "problem": "is vf_row_sky correct?\nhttps://github.com/pvlib/pvlib-python/blob/7e88d212c786d0ad334dce6fcafaf29339ff60ab/pvlib/bifacial/infinite_sheds.py#L146\r\n\r\nI think this should be:\r\n\r\n$$\\frac{1 + \\cos \\left( \\text{surface tilt} + \\psi_{t}\\ \\text{shaded} \\right)}{2}$$\r\n\r\nbecause in the reference frame of the module surface the angle pointing along the slant height to the sky is actually zero, $cos(0) = 1$, and the angle above the slant height to a horizontal line would be the `surface_tilt` itself, then the angle from the horizontal to the top of the next row is `psi_t_shaded` so finally this angle from the slant height all the way up to the top of the next row is `surface_tilt + psi_t_shaded`:\r\n\r\n![infinite_sheds](https://user-images.githubusercontent.com/1385621/218985907-7fced67c-ccff-439f-8fc8-0774026b9501.png)\r\n\r\nFor example, this is why if `psi_t_shaded` is zero, then the view factor should collapse to the isotropic view factor $(1+\\cos(\\beta))/2$ as given on the [PVPMC website modeling reference for POA sky diffuse](https://pvpmc.sandia.gov/modeling-steps/1-weather-design-inputs/plane-of-array-poa-irradiance/calculating-poa-irradiance/poa-sky-diffuse/isotropic-sky-diffuse-model/).\r\n\r\nThe actual value difference between the two formulas can be quite small when `psi_t_shaded` is close to zero (_eg_ less than 5&deg;), but it's significant when as the masking angle is larger (_eg_ greater than 5&deg;).\n",
        "hint": "",
        "base": "209376e288fe7c928fa2e0a1178ffa44612112f7",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/bifacial/infinite_sheds.py",
            "pvlib/bifacial/utils.py",
            "pvlib/shading.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1764",
        "problem": "Allow user to set tol and maxiter for singlediode newton method\nThe first few lines of `pvlib.singlediode` set `tol` and `maxiter` for all the solvers using the newton method:\r\n\r\n```\r\nfrom scipy.optimize import brentq, newton\r\nfrom scipy.special import lambertw\r\n\r\n# set keyword arguments for all uses of newton in this module\r\nnewton = partial(newton, tol=1e-6, maxiter=100, fprime2=None)\r\n```\r\n\r\nHowever, I would like to change `tol` and `maxiter` for my application. It would be great if these could be added instead as keyword arguments to the various functions so they can be adjusted by the user. Using a variety of singlediode model params, I have found that by setting tol=0.1 and maxiter=10, I can realize a 1.4x speedup in the `singeldiode.bishop88_mpp` algorithm while incurring a maximum error of 0.007038% and a mean absolute error of  0.000042% in calculated V_mp.\r\n\r\n\n",
        "hint": "At what level would they be exposed?  At the `pvlib.singlediode.bishop88_xxx` functions or at a higher level, e.g., `pvlib.pvsystem.singlediode`?\nAt the level of bishop88_xxx would be good enough for my purposes. \nI support this as long as the interface is common for both the `newton` and `brentq` options.",
        "base": "964dc435ca117d47c74da47345ace976b70a471f",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/singlediode.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1478",
        "problem": "ModelChain should accept albedo in weather dataframe\n**Is your feature request related to a problem? Please describe.**\r\nAlbedo is treated as a scalar constant in pvlib, but it is of course a function of the weather and changes throughout the year.  Albedo is currently set in the PVSystem or Array and cannot be altered using the ModelChain.  Albedo is provided as a timeseries from many weather data services as well as through NREL's NSRBD and it would be useful to provide this data to the ModelChain.\r\n\r\nAdditionally, treating albedo as property of the Array seems to conflict with the [PVSystem Design Philosophy](https://pvlib-python.readthedocs.io/en/stable/pvsystem.html#design-philosophy), which highlights the separation of the PV system and the exogenous variables, such as the weather.\r\n\r\n**Describe the solution you'd like**\r\nModelChain.run_model() should accept albedo in the weather dataframe, like temperature and ghi.\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative we have implemented is calling ModelChain.run_model() on each row of a dataframe and manually updating the albedo of the array in each tilmestep.  This probably has some side effects that we are unaware of.\r\n\n",
        "hint": "I think I agree that it would make more sense to pass around ground albedo with the weather data instead of treating it as a characteristic of the array.  \r\n\r\n> Albedo is treated as a scalar constant in pvlib\r\n\r\n> An alternative we have implemented is calling ModelChain.run_model() on each row of a dataframe and manually updating the albedo of the array in each tilmestep. \r\n\r\nIt is true that the docs for `Array` and `PVSystem` say that `albedo` is a float, but I think it also works to set albedo to a time series that matches the weather you pass to `ModelChain.run_model()`.  At least that saves you from looping w/ scalar albedo values.  \nI agree that albedo can change with e.g., precipitation or season, but it is also a property of the bare ground surface, and it is that perspective that put albedo with the system parameters. One use case for not having albedo in the weather data would be to evaluate the effect of different ground cover on array output.\r\n\r\nI am only away of the NSRDB offering albedo with weather data; are there other sources?\r\n\r\nI'm +1 on allowing `albedo` to be a Series. I'm neutral on bundling albedo with weather data, but I don't see a better option.  We only have two data structures that supply `ModelChain`: the `weather` DataFrame, and the `PVSystem` instance. I don't think it is practical to create a third just for `albedo`, and it isn't any more work to add or modify `albedo` to `weather` than it is to extract `albedo` from downloaded weather data and add it to `PVSystem`.\r\n\r\n\nTo clarify my above message, I think it *already* works to set `PVSystem.albedo` or `Array.albedo` to a Series, despite the docs saying it must be float.  \r\n\r\n> are there other sources?\r\n\r\nA non-exhaustive list of examples: [SolarAnywhere](https://www.solaranywhere.com/support/data-fields/albedo/), [SolarGIS](https://solargis.com/docs/getting-started/data-parameters), [MERRA2](https://disc.gsfc.nasa.gov/datasets/M2TMNXRAD_5.12.4/summary)\nDoes anyone know if including time-specific albedo this has been shown to be even more important with bifacials?\r\n\r\n(I would think yes.)\n> Does anyone know if including time-specific albedo this has been shown to be even more important with bifacials?\r\n> \r\n> (I would think yes.)\r\n\r\nYes, it is more important than for single-sided modules. There are ground surfaces where the albedo depends on the solar elevation and hence time of day.\r\n\r\nOne caution about albedo from satellite-derived irradiance: those values are at least km^2 scale, and are observed from space, whereas a PV model is assuming that the albedo is localized (m^2) and has been determined from the irradiance reaching the ground. [SolarAnywhere ](https://www.solaranywhere.com/support/data-fields/albedo/)provides an informative list of caveats.\r\n\r\nThe good news is that the uncertainty in albedo is typically secondary to uncertainty in other data such as GHI, when considering uncertainty in energy production.\r\n\nWhich is the better course of action?\r\n\r\n1. Leave `albedo` on `PVsystem` or `Array`. Edit the docstrings and add tests to make it explicit that `PVSystem.albedo` or `Array.albedo` can be a Series. Advantages: avoids deprecating and removing `PVSystem.albedo`. Downside: users obtaining albedo from weather data sources have an extra step to perform when using `ModelChain` methods.\r\n2. Add `albedo` as an optional column in `weather`, and have `ModelChain` methods use `weather['albedo']` instead of `PVSystem.albedo` when `weather['albedo']` is present. Advantages: convenient for ModelChain users, and avoids deprecating `PVsystem.albedo`. Disadvatanges: potential for confusion when a user also assigns `PVSystem.albedo`. \r\n3. Move `albedo` from `PVSystem` to `weather`. Same advantages as 2, but requires deprecation, and adds an extra step for users who aren't getting albedo with weather data, and would probably lead to adding code to `ModelChain` similar to the code that infers albedo from `PVSystem.surface_type`.\n@cwhanse I lean toward option 1. However, it seems that for option 1, a user might have to duplicate computations already done in the `ModelChain` in the preliminary step of using the weather to compute the existing `PVSystem.albedo` (as some formulaic combination of ground conditions and weather). Do you know if this is really an issue?\nThanks for the discussion around this feature.  I favor Option 2, noting that PVSystem.albedo is already an optional argument.  Option 1 is prone to bugs and cases need to be handled checking for mismatches in series/dataframe lengths or perhaps same series lengths but different indices.  I would discourage Option 3, as it seems there is both a basis for and utility in retaining albedo as a property of PVSystem.\nFor option 2, I would say raise an exception (or at very least a warning) if two albedos are specified. This could definitely be the source of a subtle computational bug when users don't realize a weather file they pulled is doing something they didn't intend.\n+1 for option 2, and I agree with @campanelli-sunpower that an exception should be raised if two albedos are specified\n+1 for option 2, but I like keeping the old method for the case where you just want to set either a single annual albedo or monthly albedos (tho not sure if this option currently exists). I agree also raise exception if both are specified - we can always change it later.",
        "base": "3f397ed44075b01f4ba535750356859195a51b2d",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/clearsky.py",
            "pvlib/irradiance.py",
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py",
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1072",
        "problem": "temperature.fuentes errors when given tz-aware inputs on pandas>=1.0.0\n**Describe the bug**\r\nWhen the weather timeseries inputs to `temperature.fuentes` have tz-aware index, an internal call to `np.diff(index)` returns an array of `Timedelta` objects instead of an array of nanosecond ints, throwing an error immediately after.  The error only happens when using pandas>=1.0.0; using 0.25.3 runs successfully, but emits the warning:\r\n\r\n```\r\n  /home/kevin/anaconda3/envs/pvlib-dev/lib/python3.7/site-packages/numpy/lib/function_base.py:1243: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\r\n  \tTo accept the future behavior, pass 'dtype=object'.\r\n  \tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\r\n    a = asanyarray(a)\r\n```\r\n\r\n**To Reproduce**\r\n```python\r\nIn [1]: import pvlib\r\n   ...: import pandas as pd\r\n   ...: \r\n   ...: index_naive = pd.date_range('2019-01-01', freq='h', periods=3)\r\n   ...: \r\n   ...: kwargs = {\r\n   ...:     'poa_global': pd.Series(1000, index_naive),\r\n   ...:     'temp_air': pd.Series(20, index_naive),\r\n   ...:     'wind_speed': pd.Series(1, index_naive),\r\n   ...:     'noct_installed': 45\r\n   ...: }\r\n   ...: \r\n\r\nIn [2]: print(pvlib.temperature.fuentes(**kwargs))\r\n2019-01-01 00:00:00    47.85\r\n2019-01-01 01:00:00    50.85\r\n2019-01-01 02:00:00    50.85\r\nFreq: H, Name: tmod, dtype: float64\r\n\r\nIn [3]: kwargs['poa_global'].index = index_naive.tz_localize('UTC')\r\n   ...: print(pvlib.temperature.fuentes(**kwargs))\r\n   ...: \r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-3-ff99badadc91>\", line 2, in <module>\r\n    print(pvlib.temperature.fuentes(**kwargs))\r\n\r\n  File \"/home/kevin/anaconda3/lib/python3.7/site-packages/pvlib/temperature.py\", line 602, in fuentes\r\n    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\r\n\r\nTypeError: float() argument must be a string or a number, not 'Timedelta'\r\n```\r\n\r\n**Expected behavior**\r\n`temperature.fuentes` should work with both tz-naive and tz-aware inputs.\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.8.0\r\n - ``pandas.__version__``: 1.0.0+\r\n - python: 3.7.4 (default, Aug 13 2019, 20:35:49) \\n[GCC 7.3.0]\r\n\r\n\n",
        "hint": "",
        "base": "04a523fafbd61bc2e49420963b84ed8e2bd1b3cf",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/temperature.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1073",
        "problem": "expose pvlib.temperature.fuentes in PVSystem and ModelChain\nFollow up to #1032 and #1037 \n",
        "hint": "",
        "base": "b105021f7c1a47f888363af5585083fc27aefd4c",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1181",
        "problem": "remove ModelChain.orientation_strategy\nI don't like that `ModelChain(system, location, orientation_strategy='flat`|`south_at_latitude_tilt`) modifies the `system` object. It's not something we do anywhere else in pvlib. `orientation_strategy` only supports flat and south_at_latitude_tilt, neither of which are commonly used in the real world in 2020. \r\n\r\nI think we should remove it, maybe even without deprecation, in 0.8.\r\n\r\nI'm ok with keeping the `modelchain.get_orientation` function for now.\n",
        "hint": "",
        "base": "8b98768818ee5ad85d9479877533651a2e9dc2cd",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/modelchain.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1606",
        "problem": "golden-section search fails when upper and lower bounds are equal\n**Describe the bug**\r\nI was using pvlib for sometime now and until now I was always passing a big dataframe containing readings of a long period. Because of some changes in our software architecture, I need to pass the weather readings as a single reading (a dataframe with only one row) and I noticed that for readings that GHI-DHI are zero pvlib fails to calculate the output and returns below error while the same code executes correctly with weather information that has non-zero GHI-DHI:\r\n```python\r\nimport os\r\nimport pathlib\r\nimport time\r\nimport json\r\nfrom datetime import datetime\r\nfrom time import mktime, gmtime\r\n\r\nimport pandas as pd\r\n\r\nfrom pvlib import pvsystem\r\nfrom pvlib import location as pvlocation\r\nfrom pvlib import modelchain\r\nfrom pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS as PARAMS # not used -- to remove\r\nfrom pvlib.bifacial.pvfactors import pvfactors_timeseries\r\nfrom pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\r\n\r\nclass PV:\r\n    def pv_transform_time(self, val):\r\n        # tt = gmtime(val / 1000)\r\n        tt = gmtime(val)\r\n        dd = datetime.fromtimestamp(mktime(tt))\r\n        timestamp = pd.Timestamp(dd)\r\n        return timestamp\r\n\r\n    def __init__(self, model: str, inverter: str, latitude: float, longitude: float, **kwargs):\r\n        # super().__init__(**kwargs)\r\n\r\n        temperature_model_parameters = TEMPERATURE_MODEL_PARAMETERS[\"sapm\"][\r\n            \"open_rack_glass_glass\"\r\n        ]\r\n        # Load the database of CEC module model parameters\r\n        modules = pvsystem.retrieve_sam(\"cecmod\")\r\n        # Load the database of CEC inverter model parameters\r\n        inverters = pvsystem.retrieve_sam(\"cecinverter\")\r\n\r\n\r\n        # A bare bone PV simulator\r\n\r\n        # Load the database of CEC module model parameters\r\n        modules = pvsystem.retrieve_sam('cecmod')\r\n        inverters = pvsystem.retrieve_sam('cecinverter')\r\n        module_parameters = modules[model]\r\n        inverter_parameters = inverters[inverter]\r\n\r\n        location = pvlocation.Location(latitude=latitude, longitude=longitude)\r\n        system = pvsystem.PVSystem(module_parameters=module_parameters, inverter_parameters=inverter_parameters, temperature_model_parameters=temperature_model_parameters)\r\n        self.modelchain = modelchain.ModelChain(system, location, aoi_model='no_loss', spectral_model=\"no_loss\")\r\n\r\n    def process(self, data):\r\n        weather = pd.read_json(data)\r\n        # print(f\"raw_weather: {weather}\")\r\n        weather.drop('time.1', axis=1, inplace=True)\r\n        weather['time'] = pd.to_datetime(weather['time']).map(datetime.timestamp) # --> this works for the new process_weather code and also the old weather file\r\n        weather[\"time\"] = weather[\"time\"].apply(self.pv_transform_time)\r\n        weather.index = weather[\"time\"]\r\n        # print(f\"weather: {weather}\")\r\n        # print(weather.dtypes)\r\n        # print(weather['ghi'][0])\r\n        # print(type(weather['ghi'][0]))\r\n\r\n        # simulate\r\n        self.modelchain.run_model(weather)\r\n        # print(self.modelchain.results.ac.to_frame().to_json())\r\n        print(self.modelchain.results.ac)\r\n\r\n\r\n# good data\r\ngood_data = \"{\\\"time\\\":{\\\"12\\\":\\\"2010-01-01 13:30:00+00:00\\\"},\\\"ghi\\\":{\\\"12\\\":36},\\\"dhi\\\":{\\\"12\\\":36},\\\"dni\\\":{\\\"12\\\":0},\\\"Tamb\\\":{\\\"12\\\":8.0},\\\"WindVel\\\":{\\\"12\\\":5.0},\\\"WindDir\\\":{\\\"12\\\":270},\\\"time.1\\\":{\\\"12\\\":\\\"2010-01-01 13:30:00+00:00\\\"}}\"\r\n\r\n# data that causes error\r\ndata = \"{\\\"time\\\":{\\\"4\\\":\\\"2010-01-01 05:30:00+00:00\\\"},\\\"ghi\\\":{\\\"4\\\":0},\\\"dhi\\\":{\\\"4\\\":0},\\\"dni\\\":{\\\"4\\\":0},\\\"Tamb\\\":{\\\"4\\\":8.0},\\\"WindVel\\\":{\\\"4\\\":4.0},\\\"WindDir\\\":{\\\"4\\\":240},\\\"time.1\\\":{\\\"4\\\":\\\"2010-01-01 05:30:00+00:00\\\"}}\"\r\np1 = PV(model=\"Trina_Solar_TSM_300DEG5C_07_II_\", inverter=\"ABB__MICRO_0_25_I_OUTD_US_208__208V_\", latitude=51.204483, longitude=5.265472)\r\np1.process(good_data)\r\nprint(\"=====\")\r\np1.process(data)\r\n```\r\nError:\r\n```log\r\n$ python3 ./tmp-pv.py \r\ntime\r\n2010-01-01 13:30:00    7.825527\r\ndtype: float64\r\n=====\r\n/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py:340: RuntimeWarning: divide by zero encountered in divide\r\n  np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))\r\nTraceback (most recent call last):\r\n  File \"/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py\", line 88, in <module>\r\n    p1.process(data)\r\n  File \"/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py\", line 75, in process\r\n    self.modelchain.run_model(weather)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 1770, in run_model\r\n    self._run_from_effective_irrad(weather)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 1858, in _run_from_effective_irrad\r\n    self.dc_model()\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 790, in cec\r\n    return self._singlediode(self.system.calcparams_cec)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 772, in _singlediode\r\n    self.results.dc = tuple(itertools.starmap(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py\", line 931, in singlediode\r\n    return singlediode(photocurrent, saturation_current,\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py\", line 2826, in singlediode\r\n    out = _singlediode._lambertw(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/singlediode.py\", line 651, in _lambertw\r\n    p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14,\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py\", line 364, in _golden_sect_DataFrame\r\n    raise Exception(\"Iterations exceeded maximum. Check that func\",\r\nException: ('Iterations exceeded maximum. Check that func', ' is not NaN in (lower, upper)')\r\n```\r\n\r\nI have to mention that for now the workaround that I am using is to pass the weather data as a dataframe with two rows, the first row is a good weather data that pvlib can process and the second row is the incoming weather reading (I can also post that code if you want).\r\n\r\n**Expected behavior**\r\nPVlib should have consistent behavior and regardless of GHI-DHI readings.\r\n\r\n**Versions:**\r\n```python\r\n>>> import pvlib\r\n>>> import pandas\r\n>>> pvlib.__version__\r\n'0.9.1'\r\n>>> pandas.__version__\r\n'1.4.3'\r\n``` \r\n - python: 3.10.6\r\n- OS: Ubuntu 22.04.1 LTS\n",
        "hint": "Confirmed. This appears to be an oversight in `pvlib.tools._golden_section_DataFrame` involving error messaging, likely introduced with #1089 .\r\n\r\nIn this code when processing the content of `data`, photocurrent is 0., hence the shunt resistance is infinite and v_oc is 0. That sets the range for the golden section search to be [0., 0.]. [iterlimit](https://github.com/pvlib/pvlib-python/blob/582b956c63c463e5178fbb7a88fa545fa5b1c257/pvlib/tools.py#L358) is then -infinity, which skips the loop (`iterations <= iterlimit`) but since `iterations > iterlimit` raises the \"Iterations exceeded...\" exception.\r\n",
        "base": "c78b50f4337ecbe536a961336ca91a1176efc0e8",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/tools.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1480",
        "problem": "Consider extracting the surface orientation calculation in pvlib.tracking.singleaxis() to its own function\n**Is your feature request related to a problem? Please describe.**\r\nThe usual workflow for modeling single-axis tracking in pvlib is to treat tracker rotation (`tracker_theta`) as an unknown to be calculated from solar position and array geometry.  However, sometimes a user might have their own tracker rotations but not have the corresponding `surface_tilt` and `surface_azimuth` values.  Here are a few motivating examples:\r\n- Using measured rotation angles\r\n- Post-processing the output of `tracking.singleaxis()` to include wind stow events or tracker stalls\r\n- Other tracking algorithms that determine rotation differently from the astronomical method\r\n\r\nAssuming I have my tracker rotations already in hand, getting the corresponding `surface_tilt` and `surface_azimuth` angles is not as easy as it should be.  For the specific case of horizontal N-S axis the math isn't so bad, but either way it's annoying to have to DIY when pvlib already has code to calculate those angles from tracker rotation.\r\n\r\n**Describe the solution you'd like**\r\nA function `pvlib.tracking.rotation_to_orientation` that implements the same math in `pvlib.tracking.singleaxis` to go from `tracker_theta` to `surface_tilt` and `surface_azimuth`.  Basically extract out the second half of `tracking.singleaxis` into a new function.  Suggestions for the function name are welcome.  To be explicit, this is more or less what I'm imagining:\r\n\r\n```python\r\ndef rotation_to_orientation(tracker_theta, axis_tilt=0, axis_azimuth=0, max_angle=90):\r\n    # insert math from second half of tracking.singleaxis() here\r\n    out = {'tracker_theta': tracker_theta, 'aoi': aoi,\r\n           'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\r\n    return pandas_if_needed(out)\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nContinue suffering\r\n\r\n**Additional context**\r\nThis is one step towards a broader goal I have for `pvlib.tracking` to house other methods to determine tracker rotation in addition to the current astronomical method, the same way we have multiple temperature and transposition models.  These functions would be responsible for determining tracker rotations, and they'd all use this `rotation_to_orientation` function to convert rotation to module orientation.\r\n\r\nSeparately, I wonder if the code could be simplified using the tilt and azimuth equations in Bill's technical report (https://www.nrel.gov/docs/fy13osti/58891.pdf) -- seems like what we're doing is overly complicated, although maybe I've just not studied it closely enough.\r\n\r\ncc @williamhobbs @spaneja \n",
        "hint": "I like this. \r\n\r\nThis is related to an issue submitted for NREL SAM, https://github.com/NREL/SAM/issues/850, and I think @mjprilliman is looking at something related. \n@kanderso-nrel Nice meeting you at PVSC the other day.  I've been working on this a bit using the tilt and azimuth equations in the technical report by Bill Marion mentioned.  I would like to use this for the custom backtracking schedules Nevados generates for the terrain following trackers.  \r\n\r\nI have surface tilt working.  It requires a dataframe `df` of axis tilt angles. I use the dataframe because Nevados has a different axis tilt for each bay in our tracker.  stdf (surface tilt dataframe) starts as a dataframe of rotation angles (theta) indexed by timestep and is transformed into surface tilts.  col.name is used to match the tracker and bay's rotation angle to their corresponding axis tilt.\r\n\r\n```\r\n        def calc_surface_tilt(col):\r\n            axis_tilt = df.at[col.name, 'axis_tilt']\r\n            surface_tilt = np.rad2deg(\r\n                np.arccos(\r\n                    np.cos(np.deg2rad(col)) * np.cos(np.deg2rad(axis_tilt))\r\n                )\r\n            )\r\n            return surface_tilt\r\n\r\n        stdf = stdf.apply(calc_surface_tilt, axis=0)\r\n```\r\n\r\nUnfortunately I can't seem to get surface azimuth working correctly.  sadf (surface angle data frame) is almost equal to surface azimuth as calculated by pvlib, but not quite in the middle of the day.  `ts2` is the output of `pvlib.tracking.singleaxis`\r\n\r\n\r\n```\r\n    sadf = np.rad2deg(\r\n        np.deg2rad(180) +\r\n        np.arcsin(\r\n            (\r\n                np.sin(np.deg2rad(ts2['tracker_theta'])) /\r\n                np.sin(np.deg2rad(ts2['surface_tilt']))\r\n            ).clip(upper=1, lower=-1)\r\n        )\r\n    )\r\n```\r\n![image](https://user-images.githubusercontent.com/33131958/174192086-124a53c6-78c3-445c-a50d-3a70f6944adb.png)\r\n\r\n![image](https://user-images.githubusercontent.com/33131958/174192355-62437cb6-9f23-403f-ab26-21924d79dab7.png)\r\n\r\n\r\n\nThanks @kurt-rhee for this investigation.  Trying some simple examples on my end, things seem to line up.  Here's a complete copy/pasteable example where I get negligible difference between the current pvlib approach and your code.  Note that I did replace the hard-coded axis_azimuth of 180 in the surface_azimuth calculation. \r\n\r\n<details>\r\n  <summary>Click to expand!</summary>\r\n\r\n```python\r\nimport pvlib\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\naxis_tilt = 20\r\naxis_azimuth = 230\r\nloc = pvlib.location.Location(40, -80)\r\ntimes = pd.date_range('2019-06-01', '2019-06-02', freq='5T', tz='Etc/GMT+5')\r\nsp = loc.get_solarposition(times)\r\ntr = pvlib.tracking.singleaxis(sp.apparent_zenith, sp.azimuth,\r\n                               axis_tilt=axis_tilt, axis_azimuth=axis_azimuth)\r\n\r\n\r\ndef rotation_to_orientation(tracker_theta, axis_tilt=0, axis_azimuth=0, max_angle=90):\r\n    surface_tilt = np.rad2deg(\r\n        np.arccos(\r\n            np.cos(np.deg2rad(tracker_theta)) * np.cos(np.deg2rad(axis_tilt))\r\n        )\r\n    )\r\n    surface_azimuth = np.rad2deg(\r\n        np.deg2rad(axis_azimuth) +\r\n        np.arcsin(\r\n            (\r\n                np.sin(np.deg2rad(tracker_theta)) /\r\n                np.sin(np.deg2rad(surface_tilt))\r\n            ).clip(upper=1, lower=-1)\r\n        )\r\n    )\r\n    return pd.DataFrame({\r\n        'tracker_theta': tracker_theta,\r\n        'surface_tilt': surface_tilt,\r\n        'surface_azimuth': surface_azimuth,\r\n    })\r\n\r\ntr2 = rotation_to_orientation(tr.tracker_theta, axis_tilt=axis_tilt, axis_azimuth=axis_azimuth)\r\n```\r\n\r\n</details>\r\n\r\n```python\r\nIn [53]: (tr[['surface_tilt', 'surface_azimuth']] - tr2[['surface_tilt', 'surface_azimuth']]).describe()\r\nOut[53]: \r\n       surface_tilt  surface_azimuth\r\ncount  1.780000e+02     1.780000e+02\r\nmean  -6.586492e-16     3.193450e-15\r\nstd    8.916369e-15     2.864187e-14\r\nmin   -2.842171e-14    -5.684342e-14\r\n25%   -7.105427e-15     0.000000e+00\r\n50%    0.000000e+00     0.000000e+00\r\n75%    3.552714e-15     2.842171e-14\r\nmax    2.131628e-14     5.684342e-14\r\n```\nNot that there was much doubt, but I've convinced myself that Bill's surface orientation equations are mathematically equivalent to the approach pvlib takes.  Here are some notes if anyone is interested: https://gist.github.com/kanderso-nrel/ac3051de41261df317180c794144d6a9\r\n\r\nIf we do switch to Bill's equations we should be sure to preserve the handling of NaN and edge cases of the current implementation.\nI realize that my small error metric is due to a small timeshift that I had in my data that changed my answer when resampling / averaging. \r\n\r\nCheers",
        "base": "35af84e9a7bede8dfe86d6d6c73002393544ab5a",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/tools.py",
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1682",
        "problem": "Infinite sheds perf improvement: vectorize over surface_tilt\nInfinite sheds is quite a bit slower than the modelchain POA modeling we use for frontside (as expected). I see a TODO comment in the code for _vf_ground_sky_integ (`_TODO: vectorize over surface_tilt_`) that could potentially result in some perf improvement for Infinite sheds calls with tracking systems.\n",
        "hint": "I haven't profiled it but I suspect you are right: [this line](https://github.com/pvlib/pvlib-python/blob/7bb30ad6e9d599d10510c7b37d95d56f14e846b4/pvlib/bifacial/infinite_sheds.py#L61) is the likely bottleneck.\n@wholmgren in #1627 you mentioned looking into optimizations for infinite sheds.  I wonder if there is anything in the works?\r\n\r\nAlso here's a quick and dirty profile, for reference:\r\n\r\n![image](https://user-images.githubusercontent.com/57452607/222270556-923deda6-6b9a-4006-b729-6270f0235d90.png)\r\n\r\n```python\r\nimport pvlib\r\nimport pandas as pd\r\n\r\ntimes = pd.date_range('2019-01-01', '2019-02-01', freq='5T', tz='Etc/GMT+5')\r\nlocation = pvlib.location.Location(40, -80)\r\nsp = location.get_solarposition(times)\r\ncs = location.get_clearsky(times, solar_position=sp)\r\ntr = pvlib.tracking.singleaxis(sp.zenith, sp.azimuth)\r\n\r\ngcr = 0.5\r\nheight = 1.5\r\npitch = 3.0\r\nalbedo = 0.2\r\n\r\nresult = pvlib.bifacial.infinite_sheds.get_irradiance(\r\n    surface_tilt=tr.surface_tilt, surface_azimuth=tr.surface_azimuth,\r\n    solar_zenith=sp.zenith, solar_azimuth=sp.azimuth,\r\n    gcr=0.5, height=1.5, pitch=3.0,\r\n    ghi=cs.ghi, dhi=cs.dhi, dni=cs.dni, albedo=0.2\r\n)\r\n```\nI identified a few places with repeated calculations and started thinking about larger changes to mostly private functions that would avoid more repeated calculations. So all of that is largely complementary to vectorization.\nThis is a great conversation, thanks all for responding so quickly. Is there a timeline by which we could expect some of these perf changes to go in? Excited to use a faster Infinite Sheds. :)\nI tried out vectorizing over `surface_tilt` in `utils._vf_ground_sky_2d` and got a ~15-20% speed improvement for `infinite_sheds._vf_ground_sky_integ`.  After those changes (and some minor unrelated optimizations), 80% of remaining runtime in `_vf_ground_sky_2d` is in the arctan and cosine calculations, so there might not be much room for additional improvement without changing how we do the math.  I'll open a PR with some more formal benchmark results. \r\n\r\n@aturabi, separately from these code optimizations, you might take a look at reducing the `npoints` parameter.  That parameter offers a trade-off between model resolution and speed, and I don't think we've done a real sensitivity analysis but I bet the default of 100 is overkill a lot of the time and you could get essentially the same simulation results faster with a lower (perhaps much lower) number.  ",
        "base": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "benchmarks/benchmarks/infinite_sheds.py",
            "pvlib/bifacial/infinite_sheds.py",
            "pvlib/bifacial/utils.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1832",
        "problem": "Add interp method for modelchain aoi model.\nI would like to simulate the effect of different IAM functions on performance. Pvlib already has an `interp` method for the iam_loss function. However, it is not possible to use `interp` within model chain. Can we add this feature?\n",
        "hint": "",
        "base": "80edabe7b37ac798a9202b1424777c0cefbac0fe",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/iam.py",
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1089",
        "problem": "Apparent numerical instability in I_mp calculation using PVsyst model\n**Describe the bug**\r\n\r\nI used these parameters in `pvlib.pvsystem.calcparams_pvsyst()` in order to calculate `I_mp` vs. `T` using `pvlib.pvsystem.singlediode()` with `effective_irradiance` fixed at 1000 W/m^2 and `temp_cell` having 1001 values ranging from 15 to 50 degC:\r\n\r\n`{'alpha_sc': 0.006, 'gamma_ref': 1.009, 'mu_gamma': -0.0005, 'I_L_ref': 13.429, 'I_o_ref': 3.719506010004821e-11, 'R_sh_ref': 800.0, 'R_sh_0': 3200.0, 'R_s': 0.187, 'cells_in_series': 72, 'R_sh_exp': 5.5, 'EgRef': 1.121, 'irrad_ref': 1000, 'temp_ref': 25}`\r\n\r\nMy purpose was to investigate the temperature coefficient of `I_mp`, and I got the following result, which appears to suffer from a numeric instability:\r\n\r\n![image](https://user-images.githubusercontent.com/1125363/98264917-ab2d2880-1f45-11eb-83a2-e146774abf44.png)\r\n\r\nFor comparison, the corresponding `V_mp` vs. `T` plot:\r\n\r\n![image](https://user-images.githubusercontent.com/1125363/98264984-bc763500-1f45-11eb-9012-7c29efa25e1e.png)\r\n\r\n**To Reproduce**\r\n\r\nRun the above calculations using the parameters provided.\r\n\r\n**Expected behavior**\r\n\r\nBetter numerical stability in `I_mp` vs. `T`.\r\n\r\n**Screenshots**\r\n\r\nSee above.\r\n\r\n**Versions:**\r\n\r\n - ``pvlib.__version__``: 0.8.0\r\n - ``numpy.__version__``: 1.19.2\r\n - ``scipy.__version__``: 1.5.2\r\n - ``pandas.__version__``: 1.1.3\r\n - python: 3.8.5\r\n\r\n**Additional context**\r\n\r\nI was going to attempt a numerical computation of the temperature coefficient of `I_mp` for a model translation to the SAPM. I have seen reports from CFV in which this coefficient is actually negative, and I have computed it alternately using the `P_mp` and `V_mp` temperature coefficients, and gotten a negative value for this particular PV module. Despite the apparent numerical instability in the above plot, it still suggests that the coefficient should be positive, not negative. Perhaps I am missing something here?\r\n\r\nAlso, I have not dug deep enough to figure out if the underlying issue is in `pvlib.pvsystem.singlediode()`.\n",
        "hint": "Confirmed. Looks like it's a lack of convergence in `pvlib.singlediode._lambertw`.  Some time ago a similar issue was reported for PVLib Matlab, I think we bumped up the iteration count to fix it.\r\n\r\nIf you set `pvlib.singlediode(..., method='brentq',...)` the noise smooths away.\n@cwhanse Thanks. `method='brentq'` fixed the issue for me too. :)\r\n\r\nSince you're from Sandia, do have any insight about a \"typical\" sign of the T-coef for `I_mp` in the SAPM? It seems like the sign is positive for the PVsyst model and negative for the SAPM (~both~ in SOME CFV reports ~and by taking the derivative of `P_mp` = `I_mp` * `V_mp` w.r.t. `T` and solving for `dI_mp/ dT`~).\r\n\r\nUPDATE: With the `I_mp` calculation correction, it appears that I get a consistent + sign for the `I_mp` temperature coefficient in both computational methods. However, I do see this reported as a negative value in some places.\nI don't know that theory predicts a sign for this temperature coefficient.\r\n\r\nFor SAPM, when Sandia fits this model using outdoor measurements, a value is determined by fitting a line to Imp vs. cell temperature (backed out of Isc, Voc values). I've argued that given the resolution of measurements, the technique for determining cell temperature and maximum power point, and the regression method, that the value is usually statistically indistinguishable from zero.\r\n\r\nFor CFV-produced Pvsyst parameters, my guess is that CFV is determining alpha_imp by fitting a line to the Imp vs. cell temperature data (IEC61853 matrix test results using an indoor flasher with temperature variation control), which is the same method used by Sandia for the SAPM. Happy to connect you with CFV and you can ask them.\nThanks for sharing your understanding @cwhanse. I think I will have a convo with Daniel C. Zirzow at CFV at some point in the near future.\r\n\r\nShould I close this issue, or should it become a PR to change defaults to better ensure convergence?\nLeave it open, there's a pvlib issue to be addressed.\r\n\r\nI tracked down the source of the oscillatory behavior to [this line](https://github.com/pvlib/pvlib-python/blob/3e25627e34bfd5aadea041da85a30626322b3a99/pvlib/singlediode.py#L611). \r\n\r\n```\r\n        argW = Rs[idx_p] * I0[idx_p] / (\r\n                    a[idx_p] * (Rs[idx_p] * Gsh[idx_p] + 1.)) * \\\r\n               np.exp((Rs[idx_p] * (IL[idx_p] + I0[idx_p]) + V[idx_p]) /\r\n                      (a[idx_p] * (Rs[idx_p] * Gsh[idx_p] + 1.)))\r\n```\r\n\r\nIt's a product of two terms:\r\nconstant factor : Rs * I0 / (a (Rs Gsh + 1))  which is very small (~1E-10) and\r\nexponential factor: np.exp( (Rs * (IL + I0) + V) / (a * (Rs * Gsh + 1)) ) which is quite large (~1E+10), since the argument is ~20.\r\n\r\nThe constant factor increases smoothly with cell temperature, as expected. \r\n\r\nThe argument of the exponential term decreases with temperature but not smoothly. The cause seems to be that Vmp (`V` in that line of code) decreases but not smoothly. Vmp is calculated using `pvlib.tools._golden_sect_DataFrame`. I suspect the convergence of this root finder is the culprit.\r\n\n@cwhanse An incidental colleague of mine (Ken Roberts) figured out a way around the numerical convergence issues with using the LambertW function for solving the single diode equation. I have never had the time to do it full justice in code (I use simple newton in PVfit), but this \"Log Wright Omega\" function might be worth looking into for pvlib. Here are some references he shared with me. He was so pleasant to work with and I found his exposition very approachable.\r\n\r\n- https://arxiv.org/pdf/1504.01964.pdf\r\n- https://www.researchgate.net/publication/305991463",
        "base": "d5d1d66aae4913f0e23b9a79c655efa1bdafe5f4",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/ivtools/sdm.py",
            "pvlib/tools.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1033",
        "problem": "PVSystem.temperature_model_parameters requirement\nThe `temperature_model_parameters` handling code below suggests to me that in 0.8 we're going to \r\n\r\n1. set default values `module_type=None` and `racking_model=None`.\r\n2. require user to specify either `temperature_model_parameters` or both `module_type` and `racking_model`.\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L208-L221\r\n\r\n@cwhanse is that correct?\r\n\r\nThe problem is that the only way to see this warning is to supply an invalid `module_type` or `racking_model`. That's because `PVSystem._infer_temperature_model` is called before the code above, and it looks up the default `module_type` and `racking_model` and successfully finds temperature coefficients.\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L201-L203\r\n\r\nSo I'm guessing that this warning has been seen by only a small fraction of people that need to see it. I'm ok moving forward with the removal in 0.8 or pushing to 0.9. \nremove deprecated functions in 0.8\n`pvsystem`:\r\n* `sapm_celltemp`\r\n* `pvsyst_celltemp`\r\n* `ashraeiam`\r\n* `physicaliam`\r\n* `sapm_aoi_loss`\r\n* `PVSystem.ashraeiam`\r\n* `PVSystem.physicaliam`\r\n* `PVSystem.sapm_aoi_loss`\r\n* inference of `PVSystem.temperature_model_parameters`\r\n\r\n`modelchain.ModelChain`:\r\n* remove `times` from `complete_irradiance`, `prepare_inputs`, `run_model`\r\n* remove `temp_model` kwarg\n",
        "hint": "> The `temperature_model_parameters` handling code below suggests to me that in 0.8 we're going to\r\n> \r\n> 1. set default values `module_type=None` and `racking_model=None`.\r\n> 2. require user to specify either `temperature_model_parameters` or both `module_type` and `racking_model`.\r\n> \r\n> @cwhanse is that correct?\r\n\r\nYes, that is the intent.\r\n\r\n> So I'm guessing that this warning has been seen by only a small fraction of people that need to see it. I'm ok moving forward with the removal in 0.8 or pushing to 0.9.\r\n\r\nThe warning should have been raised whenever condition #2 above wasn't met; it looks to me that has been the case. If that hasn't been the case I would prefer to fix the warning and push the deprecation out to v0.9. pvlib-python has had that unadvertised default temperature model assignment for a long time.\r\n\n> The problem is that the only way to see this warning is to supply an invalid `module_type` or `racking_model`. That's because `PVSystem._infer_temperature_model` is called before the code above, and it looks up the default `module_type` and `racking_model` and successfully finds temperature coefficients.\r\n\r\nI don't follow here - it looks to me that the warning should be raised if 1) `temperature_model_parameters` isn't specified, or 2) either `module_type` or `racking_model` are invalid. Maybe we're saying the same thing. `_infer_temperature_model` doesn't assign the default temperature model, that is done in the block of code that raises the warning. \r\n\r\n\nThe `module_type` and `racking_model` defaults prevent the warning from showing up in many use cases. If we change the defaults to `None` then the warning will be triggered. If we simultaneously remove the warning then code will break without users having ever seen the warning.\nWhat is the expected behavior for `PVSystem()`?\n> The `module_type` and `racking_model` defaults prevent the warning from showing up in many use cases. If we change the defaults to `None` then the warning will be triggered. If we simultaneously remove the warning then code will break without users having ever seen the warning.\r\n\r\nAha. I see the problem now, thanks.  Perhaps remove the defaults in v0.8 and leave the warning until v0.9?\nOk, I'll work on that.\n",
        "base": "a7edb8b582174ed45f2d3859f29261908f5e0ab5",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/718",
        "problem": "CEC 6-parameter coefficient generation\nSAM is able to extract the CEC parameters required for calcparams_desoto.  This is done through the 'CEC Performance Model with User Entered Specifications' module model, and coefficients are automatically extracted given nameplate parameters Voc, Isc, Imp, Vmp and TempCoeff.  The method is based on Aron Dobos' \"An Improved Coefficient Calculator for the California Energy Commission 6 Parameter Photovoltaic Module Model \", 2012\r\n\r\nIdeally we should be able to work with the SAM open source code, extract the bit that does the coefficient generation, and put it into a PVLib function that would allow users to run calcparams_desoto with any arbitrary module type.  At the moment we are dependent on PV modules loaded into the SAM or CEC database.\r\n\r\nThank you!\r\n\n",
        "hint": "SAM solution routine is located at https://github.com/NREL/ssc/blob/develop/shared/6par_solve.h , function \"solve_with_sanity_and_heuristics\" . Additional dependencies on other files 6par_newton.h, 6par_search.h, 6par_jacobian.h, 6par_lu.h, 6par_gamma.h, all located in https://github.com/NREL/ssc/tree/develop/shared . \nI'd like to try to take this up sometime soon, maybe by the end of the year? Is anyone else working on it? I haven't seen a PR. Does anyone have any strong opinions? Is seems like there's a SAM implementation, is there a PVLIB-MATLAB version already? thx\nI'm not working on it. If it is practical, it would be great to wrap the SAM method. NREL put a lot of effort into the heuristics for initial values and updating.  We have a function in PVLib for MATLAB but it's a very different algorithm. For consistency with the parameter databases it would be better to use the SAM algorithm.\nDo I read between the lines that the two algorithms do not converge on the same parameters?  Then it would be of great interest to have python implementations of each!  Or perhaps even more important: documents describing the full algorithms along with every heuristic, assumption and constraint they implement...\n@adriesse yes. Each model fitting algorithm I'm aware of, will produce different parameter values from the same data although the predicted IV curves may be similar.\nFWIW I am trying to establish a common/fair metric to compare different model fits in PVfit. In addition to choosing, say, which residual(s) to analyze, the effect of model discrepancy significantly affects such comparisons. PVfit\u2019s orthogonal distance regression (ODR) fits to the single-diode model (SDM) often are worse than other algorithms\u2019 fits in terms of terminal current residuals, but I have observed consistently that this is likely due to model discrepancy. One might claim that ODR is being more \u201chonest\u201d here, but I think it really means that the model should be improved. Note that PVfit\u2019s ODR alternatively optimizes the residual of the sum of currents at the high-voltage diode node in the equivalent-circuit model, and it doesn\u2019t make unfounded assumptions about error-free measurements in all data channels except the terminal current (which also has implications for parameter non-identifiability). I have seen plenty of evidence that ODR fits are \u201cgood\u201d in the absence of significant model discrepancy, such as for some (but not all!) double-diode model (DDM) fits across various material systems. \n> If it is practical, it would be great to wrap the SAM method. \r\n\r\nTwo kinds of practicalities here:\r\n\r\n1. Technical. I don't know how to do it but I am sure it's possible. I am ok with it so long as pvlib remains straightforward to install from source.\r\n2. Legal. We would need assurance from NREL that it's ok for pvlib to distribute this SAM code under the terms of a MIT license rather than SAM's mixed MIT/GPL license.\r\n\nIn the next 6-8 months, the NREL team will be updating our python wrapper for the SAM software development kit so that you can pip install the sam-sdk and call its routines in a much more native pythonic fashion. That might be a quick way to implement the CEC parameter generation to solve both the technical and legal practicalities Will mentions above, as well as the many file dependencies involved with that routine. The SDK is licensed separately from the SAM open source code under an MIT type license, so that distribution with pvlib would be ok, and once we have it set up as a python package, that should make it a non-issue to include with pvlib installation from source. \r\n\r\nTo @adriesse 's point, the publication associated with the method implemented in SAM is here: http://solarenergyengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1458865 \nThanks @janinefreeman that is great news!\n@janinefreeman Does NREL's web site no longer provide free preprints of Aron's paper?\n@thunderfish24  I'm not finding a copy of it in NREL's pubs database, but it looks like you may be able to get it here (didn't create an account to give it a try): https://www.osti.gov/biblio/1043759 \n@thunderfish24 I sent you a preprint.",
        "base": "3c84edd644fa4db54955e2225a183fa3e0405eb0",
        "env": "84818c6d950142927359ffe308c728a0c080ddce",
        "files": [
            "pvlib/__init__.py",
            "pvlib/ivtools.py",
            "setup.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1653",
        "problem": "Corrections to Townsend snow model\nPrivate communications with the model's author have turned up some issues with the pvlib implementation. Chief among the issues is  this part of the calculation:\r\n\r\n```\r\n    lower_edge_height_clipped = np.maximum(lower_edge_height, 0.01)\r\n    gamma = (\r\n        slant_height\r\n        * effective_snow_weighted_m\r\n        * cosd(surface_tilt)\r\n        / (lower_edge_height_clipped**2 - effective_snow_weighted_m**2)\r\n        * 2\r\n        * tand(angle_of_repose)\r\n    )\r\n\r\n    ground_interference_term = 1 - C2 * np.exp(-gamma)\r\n```\r\n\r\nWhen `lower_edge_height_clipped` < `effective_snow_weighted_m`, `gamma` < 0 and the `ground_interference_term` can become negative. In contrast, the author's intent is that C2 < `ground_interference_terms` < 1. The author recommends clipping the squared difference (lower bound being worked out but will be something like 0.01.).\r\n\r\nOther issues appear to arise from the unit conversions. The published model uses inches for distance and snow depth. The pvlib code uses cm for snow depth (convenience for working with external snow data) and m for distances (for consistency with the rest of pvlib). After several steps, including the `ground_interference_term` calculation, the code converts from cm or m to inches to apply the final formula for loss (since the formula involves some coefficients determined by a regression). It would be easier to trace the pvlib code back to the paper if the internal unit conversions (from cm / m to inches) were done earlier.\r\n\n",
        "hint": "Interestingly, clamping the difference of squares at 0.01 was part of the original PR but got lost along the way: https://github.com/pvlib/pvlib-python/pull/1251#discussion_r830258000\r\n\r\nIt would be great if the communication with the author results in improved tests as well as improved code.\nAfter communications with the author, the pvlib code is missing two items:\r\n\r\n- a lower bound on `lower_edge_height_clipped**2 - effective_snow_weighted_m**2` which the author specifies should be 0.1 in^2.\r\n- a factor that multiplies the monthly loss fraction, to represent the potential for a string of modules at the top of the slanted array to generate power while strings at lower positions are still affected by snow.  This factor was brought up in #1625 \r\n\r\nNeither is documented in the 2011 paper but should be added to 1) prevent unreasonably low loss values (item 1) and to better represent the loss for systems with multiple, horizontally-oriented strings.\r\n\r\nAlso, the author recommends advising users to enter 1/2 the total module width as the slant_height for single-axis tracked systems, which makes sense to me, as snow could slide off either surface depending on its rotation. ",
        "base": "faf27fee1fcbae3b7056bd02f9e98b7d6e5cb42d",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/snow.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1854",
        "problem": "PVSystem with single Array generates an error\n**Is your feature request related to a problem? Please describe.**\r\n\r\nWhen a PVSystem has a single Array, you can't assign just the Array instance when constructing the PVSystem.\r\n\r\n```\r\nmount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\narray = pvlib.pvsystem.Array(mount=mount)\r\npv = pvlib.pvsystem.PVSystem(arrays=array)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-f5424e3db16a> in <module>\r\n      3 mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\n      4 array = pvlib.pvsystem.Array(mount=mount)\r\n----> 5 pv = pvlib.pvsystem.PVSystem(arrays=array)\r\n\r\n~\\anaconda3\\lib\\site-packages\\pvlib\\pvsystem.py in __init__(self, arrays, surface_tilt, surface_azimuth, albedo, surface_type, module, module_type, module_parameters, temperature_model_parameters, modules_per_string, strings_per_inverter, inverter, inverter_parameters, racking_model, losses_parameters, name)\r\n    251                 array_losses_parameters,\r\n    252             ),)\r\n--> 253         elif len(arrays) == 0:\r\n    254             raise ValueError(\"PVSystem must have at least one Array. \"\r\n    255                              \"If you want to create a PVSystem instance \"\r\n\r\nTypeError: object of type 'Array' has no len()\r\n\r\n```\r\n\r\nNot a bug per se, since the PVSystem docstring requests that `arrays` be iterable. Still, a bit inconvenient to have to do this\r\n\r\n```\r\nmount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\narray = pvlib.pvsystem.Array(mount=mount)\r\npv = pvlib.pvsystem.PVSystem(arrays=[array])\r\n```\r\n\r\n**Describe the solution you'd like**\r\nHandle `arrays=array` where `array` is an instance of `Array`\r\n\r\n**Describe alternatives you've considered**\r\nStatus quo - either make the single Array into a list, or use the PVSystem kwargs.\r\n\n",
        "hint": "",
        "base": "27a3a07ebc84b11014d3753e4923902adf9a38c0",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/pvsystem.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1140",
        "problem": "Inconsistent default settings for _prep_inputs_solar_pos in prepare_inputs and prepare_inputs_from_poa\nHi there,\r\n\r\nI find that `_prep_inputs_solar_pos` method has been both called in [`prepare_inputs`](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/modelchain.html#ModelChain.prepare_inputs) and [`prepare_inputs_from_poa`](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/modelchain.html#ModelChain.prepare_inputs_from_poa). However, the former takes an additional argument, press_temp that contains temperature pulled from the weather data provided outside. For the default `nrel_numpy` algorithm, I further checked its input requirement is [avg. yearly air temperature in degrees C](https://pvlib-python.readthedocs.io/en/stable/generated/pvlib.solarposition.spa_python.html#pvlib.solarposition.spa_python) rather than the instantaneous temperature provided in weather. Hence I would like to ask if the following codes in `prepare_inputs` are redundant at least for the default 'nrel_numpy' algorithm?\r\n```\r\n        # build kwargs for solar position calculation\r\n        try:\r\n            press_temp = _build_kwargs(['pressure', 'temp_air'], weather)\r\n            press_temp['temperature'] = press_temp.pop('temp_air')\r\n        except KeyError:\r\n            pass\r\n```\r\nAnd thereby we change `self._prep_inputs_solar_pos(press_temp)` to `self._prep_inputs_solar_pos()` in `prepare_inputs`?\r\n\r\nMeanwhile, does the temperature really matter? How much uncertainty will it cause in the calculation of the sun's position? Should we provide avg. local temperature data if for a global modelling purpose?\r\n\r\nAny help would be appreciated!\r\n\r\n\n",
        "hint": "> Meanwhile, does the temperature really matter? How much uncertainty will it cause in the calculation of the sun's position? Should we provide avg. local temperature data if for a global modelling purpose?\r\n\r\nTaking [A simple ModelChain example](https://pvlib-python.readthedocs.io/en/stable/modelchain.html#a-simple-modelchain-example) as an illustration, I find the difference caused by temperature argument is very small:\r\n\r\n`mc.solar_position` produced in the tutorial above:\r\n![image](https://user-images.githubusercontent.com/43779303/94040505-f2f65700-fdc0-11ea-84f9-bbf842c76fb4.png)\r\n\r\nwith default temperature=12 `mc.location.get_solarposition(mc.weather.index, method=mc.solar_position_method)`:\r\n![image](https://user-images.githubusercontent.com/43779303/94040659-1de0ab00-fdc1-11ea-936c-fdca38d8e47f.png)\r\n\r\nOnly slight difference in apparent_zenith and thereby apparent_elevation come in being.\r\n\r\nThat said, I would still think we'd better not to use instantaneous temp_air pulled from weather provided outside to calculate solar position? Besides this [documentation](https://pvlib-python.readthedocs.io/en/stable/generated/pvlib.solarposition.spa_python.html#pvlib.solarposition.spa_python), I find the [original paper](https://www.sciencedirect.com/science/article/pii/S0038092X0300450X) also states annual local temperature.\r\n\r\nAny discussion would be appreciated!\r\n\nThanks @FeiYao-Edinburgh. \r\n\r\nThe two `_prep` functions should certainly be consistent. \r\n\r\nI agree that the documentation and original paper state that the appropriate quantity is the average yearly temperature. I don't know why we accepted #936/#893/#523 without discussing this.\r\n\r\nI'm disinclined to expose the average temperature or pressure through `ModelChain` - it would be an awkward API for no practical benefit.\n> I'm disinclined to expose the average temperature or pressure through `ModelChain` - it would be an awkward API for no practical benefit.\r\n\r\nI think so, too. This is simply because avg. yearly temperature or pressure is somewhat a time-invariant variable. Thus if it is pulled from weather data outside, it will cause weather data somewhat redundant containing many repeated values throughout time.\r\n\r\nAlternatively, maybe we can add `temperature` as a new attribute to [Location](https://pvlib-python.readthedocs.io/en/stable/generated/pvlib.location.Location.html) class. Then we can treat temperature as what we do for altitude. More specifically:\r\n\r\n1. Make changes outlined in my initial comment.\r\n2. In [Location.get_solarposition](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/location.html#Location.get_solarposition), remove temperature argument:\r\n```\r\n    def get_solarposition(self, times, pressure=None,\r\n                          **kwargs):\r\n```\r\nand use self.temperature instead:\r\n```\r\n        return solarposition.get_solarposition(times, latitude=self.latitude,\r\n                                               longitude=self.longitude,\r\n                                               altitude=self.altitude,\r\n                                               pressure=pressure,\r\n                                               temperature=self.temperature,\r\n                                               **kwargs)\r\n```\r\nThen in [pvlib.solarposition.get_solarposition](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/solarposition.html#get_solarposition), make the default temperature as None:\r\n```\r\ndef get_solarposition(time, latitude, longitude,\r\n                      altitude=None, pressure=None,\r\n                      method='nrel_numpy',\r\n                      temperature=None, **kwargs):\r\n```\r\nand determine if temperature is None or not just like we do for altitude/pressure:\r\n```\r\nif temperature is None:\r\n    temperature=12\r\n# Otherwise use the self.temperature\r\n```\r\nI am not sure whether this small issue is important enough to re-structure the Location class but happy to discuss.\r\n\r\n\nI exchanged email with Ibrahim Reda, the author of the SPA algorithm. His point is that refraction is being corrected by the atmospheric content between the observer and the sun, which cannot be known. They chose to use annual average temperature and pressure (rather than moment in time air temperature and pressure near earth's surface) by consensus, because the refraction correction is small (in terms of sun position), and either temperature gets the refraction close enough. I also suspect that annual data was easier to obtain.\r\n\r\nIMO using the time-dependent air temperature is justified in pvlib, but it is a departure from the SPA algorithm as published. \r\n\r\nI see two options:\r\n\r\n1. leave the calculation as is, change docstrings and add comment about the variation from the SPA publication. Users can still supply annual average temperature as `temp_air` if they want to strictly adhere to the publication.\r\n2. find and include global grids of average annual temperature and pressure, and wire these data into the SPA algorithm in the same way that the Linke turbidity data serve the Ineichen function.\r\n\r\nI favor the first.\r\n\r\n@adriesse would this have affected the comparisons you did a few years back between various PV-related software?\r\n\n> 1. leave the calculation as is, change docstrings and add comment about the variation from the SPA publication. Users can still supply annual average temperature as `temp_air` if they want to strictly adhere to the publication.\r\n\r\nFor this, I think it might be good to make the following changes in [`prepare_inputs_from_poa`](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/modelchain.html#ModelChain.prepare_inputs_from_poa) to enable consistent calculations.\r\n```\r\nself._prep_inputs_solar_pos()\r\n```\r\nto\r\n```\r\n        # build kwargs for solar position calculation\r\n        try:\r\n            press_temp = _build_kwargs(['pressure', 'temp_air'], self.weather) # weather => self.weather (TBC)\r\n            press_temp['temperature'] = press_temp.pop('temp_air')\r\n        except KeyError:\r\n            pass\r\n        self._prep_inputs_solar_pos(kwargs=press_temp)\r\n```\nThanks for checking on this @cwhanse.\r\n\r\nThe `solarposition.spa_python` temperature/pressure documentation is clear and consistent with the reference, so I think we should leave it alone. We should double check the expectations for the other algorithms and update their documentation if needed. The `solarposition.get_solarposition` documentation will need to be updated. \r\n\r\nUsers are free to obtain their own average annual temperature and pressure data and provide them to `spa_python`. I'm -1 on distributing gridded average temperature/pressure data with pvlib. I suspect the altitude pressure conversion that we already support is plenty accurate for this purpose, so users only need to find their own temperature if they care that much about it.\r\n\r\nWhen it comes to ModelChain, I'm ok with consistently using the temperature time series or removing the correction entirely. @FeiYao-Edinburgh's idea of a `Location.average_temperature` attribute is interesting, but I think I'd want to see more applications of this attribute before considering it worth the API complication.\nI agree that the `prepare_inputs` methods should be consistent whichever course we take on using air temperature for solar position calculations.\n@cwhanse thanks for inviting me to this discussion.\r\n\r\nEarlier this year or last I read the Wikipedia article about atmospheric refraction (highly recommended), did some refraction calculations myself, and started to wonder about the point of SPA.  The effect of refraction is huge compared to \"SPA is well within the stated uncertainty of \u00b1 0.0003 deg.\"   \r\n\r\nIt is nice for pvlib to provide SPA code corresponding to its published form, but for PV simulations consistency is much more useful than super high accuracy.  Using fixed values for temperature and pressure promotes consistency.\r\n\r\nIf it were my code I would separate the sun position and refraction code because they model entirely different things.\r\n\nWhy not implement or use [solpos](https://www.nrel.gov/grid/solar-resource/solpos.html) instead. It's much simpler and faster than spa, includes refraction, and is accurate enough considering uncertainty in irradiance & other parameters. Also it solves @kanderso-nrel  issue with implementing spectr2\nMight be worth synchronizing with SAM at least on the default for the sun position calculation. \r\n\nIt looks like [SAM ](https://github.com/NREL/ssc/blob/c6e41d7d946723e3cc4111262abcab081c151621/solarpilot/solpos.cpp) uses that solpos C function. For the refraction calculation (which appears to be different than in SPA) pressure and temperature can be time-dependent, or fixed at defaults (1013mb and 15C, the docstring says 10C but that looks to be a typo).\r\n\r\nI don't have a preference for any of the solar position algorithms. A drawback of SOLPOS is that it does not appear to be published other than as code.\nI had this document in my archives, but it similar to what's already online\r\n[SOLPOS Documentation.pdf](https://github.com/pvlib/pvlib-python/files/5290012/SOLPOS.Documentation.pdf)\r\n\nMore background to my earlier comment:\r\n\r\nLast year I compared simulation output from SAM, PlantPredict, Cassys, PVsyst and pvlib.  This was made more difficult because _four_ different sun position algorithms were being used, and this is why I advocated for harmonization.  I recommended SPA because I thought it more likely that others would switch to that, as opposed to the other way around.  So perhaps we could make a pitch to SAM to adopt SPA instead.  But we also need consistency on refraction.\r\n\r\n\n> I had this document in my archives, but it similar to what's already online\r\n\r\nYes. I was hoping for something to cite besides the webpage, if we implement SOLPOS.\r\n\nI have no qualms about adding SOLPOS with only a citation to a webpage. It sounds like it could be a more standard and better documented version of the [`ephemeris`](https://github.com/pvlib/pvlib-python/blob/04a523fafbd61bc2e49420963b84ed8e2bd1b3cf/pvlib/solarposition.py#L683) function. We have a robust set of solar position tests and so it should be easy to have confidence in its results. \nLooks to me like Michalsky 1988 is the one to cite.\nLooks like SAM is changing to SPA? https://github.com/NREL/ssc/pull/450\nGreat, perhaps we can get the same default behavior for refraction too... \n> Looks like SAM is changing to SPA? [NREL/ssc#450](https://github.com/NREL/ssc/pull/450)\r\n\r\nSAM has (in it's [development branch)](https://github.com/NREL/ssc/pull/450#event-3921449470), using time-dependent temperature rather than annual average.",
        "base": "6b92d218653633e366241c31e8836c0072739ece",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/modelchain.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1296",
        "problem": "Add a model for spectral corrections\n**Additional context**\r\nFirst of all, I introduce myself, my name is Jose Antonio Caballero, and I have recently finished my PhD in photovoltaic engineering at the University of Ja\u00e9n, Spain.\r\n\r\nI have developed a python script to apply spectral corrections as a function of AM, AOD, PW based on this work (https://doi.org/10.1109/jphotov.2017.2787019).\r\n\r\nWe have found that in pvlib there is already a similar methodology developed by First solar, in which the spectral corrections are based only on the AM and PW parameters, so we intend to include our proposed method in pvlib in a similar way.\r\n\r\nAs an example, I attach the code developed in python (.zip file) to estimate the spectral effects related to different flat photovoltaic technologies from the AM, AOD and PW parameters included in a .csv file.\r\n[PV-MM-AM_AOD_PW_data.csv](https://github.com/pvlib/pvlib-python/files/6970716/PV-MM-AM_AOD_PW_data.csv)\r\n[PV_Spectral_Corrections.zip](https://github.com/pvlib/pvlib-python/files/6970727/PV_Spectral_Corrections.zip)\r\n\r\nKind regards\n",
        "hint": "Hello @Jacc0027 and welcome to this community.\r\n\r\nI would welcome submission of a function implementing that model. It differs from the existing functions by including parameters for perovskite cells.\r\n\r\nIt would need to be coded as a function similar to [first_solar_spectral_correction](https://github.com/pvlib/pvlib-python/blob/f318c1c1527f69d9bf9aed6167ca1f6ce9e9d764/pvlib/atmosphere.py#L324) using the same parameter names, where possible.\r\n\r\nThe script isn't in a function format. The time series in the .csv file appear to be related to the SMARTS simulations used to set up this model, is that correct? If so, then these data don't need to come into pvlib.\nMy pleasure @cwhanse , thank you very much for the welcome.\r\n\r\nNo problem, I can adapt the code in a similar way to the one you attached, and assuming the same variables.\r\n\r\nYes, you are right, the script I attached is not made as a function, it was just a sample to verify the results. And again you are right, the time series in the .csv file are obtained through SMARTS simulations\r\n\r\nTherefore, I will attach in this thread the modified code as we have just agreed.\r\n\r\n\nHi @cwhanse , please find attached the updated script, in which we have tried to meet the requested requirements.\r\n\r\nI look forward to any comments you may have.\r\n\r\n[AM_AOD_PW_spectral_correction.zip](https://github.com/pvlib/pvlib-python/files/6984688/AM_AOD_PW_spectral_correction.zip)\r\n\nHi Jacc0027, I have looked at the code in the attachment. Can you make a pull request? When there are some tests, I think we can start the review process.",
        "base": "bc0f0ff4e8580797af6672ff7c590caa1c78d6ed",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/spectrum/__init__.py",
            "pvlib/spectrum/mismatch.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1739",
        "problem": "`pvlib.iotools.get_pvgis_hourly`'s `surface_azimuth` parameter doesn't use pvlib's azimuth convention\nNearly everything in pvlib represents azimuth angles as values in [0, 360) clockwise from north, except `pvlib.iotools.get_pvgis_hourly`:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/3def7e3375002ee3a5492b7bc609d3fb63a8edb1/pvlib/iotools/pvgis.py#L79-L81\r\n\r\nThis inconsistency is a shame.  However, I don't see any way to switch it to pvlib's convention without a hard break, which is also a shame.  I wonder how others view the cost/benefit analysis here.\r\n\r\nSee also https://github.com/pvlib/pvlib-python/pull/1395#discussion_r1181853794\r\n\r\n\n",
        "hint": "Since `get_pvgis_hourly` is only using `surface_azimuth` to pass a value to a pvgis keyword `aspect`, we could add `aspect` with the South=0 convention and deprecate `surface_azimuth` as is. Then later, we can deprecate `aspect` and add `surface_azimuth` back with its pvlib meaning. Now that I write that out, seems better just to make the breaking change.\r\n",
        "base": "909f86dc67eddc88154c9e7bff73fd9d6bfe2e4d",
        "env": "6072e0982c3c0236f532ddfa48fbf461180d834e",
        "files": [
            "pvlib/iotools/pvgis.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1224",
        "problem": "It should be impossible to instantiate a PVSystem with no Arrays\n**Describe the bug**\r\nIt should be impossible to instantiate a `PVSystem` with no `Arrays`. Currently this is possible via `PVSystem(arrays=[])`.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n```python\r\nfrom pvlib import pvsystem\r\npvsystem.PVSystem(arrays=[])\r\n```\r\nresults in this PVSystem:\r\n```\r\nPVSystem:\r\n  name: None\r\n  inverter: None\r\n```\r\n**Expected behavior**\r\nA `ValueError` should be raised indicating that a PVSystem must have at least one `Array` and suggesting that a system with an arbitrary default array can be constructed by passing `arrays=None` or not passing the `arrays` parameter at all.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.8.1+\r\n\r\n\n",
        "hint": "",
        "base": "50dcc7fe412d9e27fe06670b8057e3d8e9ce5b19",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/pvsystem.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/763",
        "problem": "Add recombination current params to all bishop88 functions\nThe changes made in #163 incorporate recombination current into the `bishop88()` function.  Functions that build on the `bishop88()` function should likewise accept these parameters.\r\n\n",
        "hint": "",
        "base": "0a14b273517a082603cc157faa88a5ab0ac4cac9",
        "env": "b91d178868d193afd56f8e3b013661a473d699c3",
        "files": [
            "pvlib/pvsystem.py",
            "pvlib/singlediode.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/907",
        "problem": "access private _parse_pvgis_tmy_csv() function as read_pvgis_tmy_csv()\n**Is your feature request related to a problem? Please describe.**\r\nsomeone sent me a csv file they downloaded from pvgis, and I needed to parse it, so I had to call the private methods like this:\r\n\r\n```python\r\n>>> from pvlib.iotools.pvgis import _parse_pvgis_tmy_csv\r\n>>> with (path_to_folder / 'pvgis_tmy_lat_lon_years.csv').open('rb') as f:\r\n        pvgis_data = _parse_pvgis_tmy_csv(f)\r\n```\r\n\r\n**Describe the solution you'd like**\r\nIf I need this, others may also. I think a public method that takes either a string or a buffer could be useful? Something called `read_pvgis_tmy_csv()`\r\n\r\n**Describe alternatives you've considered**\r\nI was able to do it by just calling the private function and it worked, so that's an alternative also\r\n\r\n**Additional context**\r\nrelated to #845 and #849 \r\n\naccess private _parse_pvgis_tmy_csv() function as read_pvgis_tmy_csv()\n**Is your feature request related to a problem? Please describe.**\r\nsomeone sent me a csv file they downloaded from pvgis, and I needed to parse it, so I had to call the private methods like this:\r\n\r\n```python\r\n>>> from pvlib.iotools.pvgis import _parse_pvgis_tmy_csv\r\n>>> with (path_to_folder / 'pvgis_tmy_lat_lon_years.csv').open('rb') as f:\r\n        pvgis_data = _parse_pvgis_tmy_csv(f)\r\n```\r\n\r\n**Describe the solution you'd like**\r\nIf I need this, others may also. I think a public method that takes either a string or a buffer could be useful? Something called `read_pvgis_tmy_csv()`\r\n\r\n**Describe alternatives you've considered**\r\nI was able to do it by just calling the private function and it worked, so that's an alternative also\r\n\r\n**Additional context**\r\nrelated to #845 and #849 \r\n\n",
        "hint": "OK with me to refactor to the same pattern as #842 in particular the get / read / parse [trio](https://github.com/pvlib/pvlib-python/pull/842#issuecomment-570309454).\nI just started in this, but didn't push it yet, sorry. Kinda important now to have a static/remoteless test that checks parsing, since all of the tests are decorated\nOK with me to refactor to the same pattern as #842 in particular the get / read / parse [trio](https://github.com/pvlib/pvlib-python/pull/842#issuecomment-570309454).\nI just started in this, but didn't push it yet, sorry. Kinda important now to have a static/remoteless test that checks parsing, since all of the tests are decorated",
        "base": "113917b780d4c7a249b09b197c16568f6212c119",
        "env": "b91d178868d193afd56f8e3b013661a473d699c3",
        "files": [
            "pvlib/iotools/__init__.py",
            "pvlib/iotools/pvgis.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1053",
        "problem": "remove **kwargs from PVSystem, ModelChain, Location\nThese objects accept arbitrary kwargs so that users can be lazy about splatting dictionaries into the object constructors. I guess this is nice in some situations. But it also leads to bugs when users mistype a parameter name because python doesn't raise an exception. I ran into this when working on #1022 and #1027. \r\n\r\nI propose that we remove the kwargs without deprecation in 0.8.\n",
        "hint": "",
        "base": "bc3a9c8efcedd4b686f60ed0e3d3d7548189bb27",
        "env": "6e5148f59c5050e8f7a0084b7ae39e93b80f72e6",
        "files": [
            "pvlib/location.py",
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py",
            "pvlib/tracking.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1518",
        "problem": "Altitude lookup table\nCurrently, altitude for `pvlib.location` based algorithms defaults to zero, but if we include a low-resolution altitude lookup, we can provide better results when altitude is not specified.\r\nWe can make this altitude lookup the same format as [LinkeTurbidities.h5](https://github.com/pvlib/pvlib-python/blob/master/pvlib/data/LinkeTurbidities.h5), so it wouldn't require that much new code or any new dependencies.\r\nI was able to build an altitude map using [open data aggregated by tilezen](https://github.com/tilezen/joerd/blob/master/docs/data-sources.md). My test H5 file is currently `13 mb` using `4320x2160` resolution, `uint16` altitude, and `gzip` compression. We are free to distribute this data, but we do need to do is add [this attribution](https://github.com/tilezen/joerd/blob/master/docs/attribution.md) somewhere in the documentation.\r\nWould you guys be interested in this feature? Should I make a pull request?\r\n\r\nHere is a plot of my sample\r\n![altitude](https://user-images.githubusercontent.com/17040442/182914007-aedbdd53-5f74-4657-b0cb-60158b6aa26d.png)\r\n:\n",
        "hint": "Is there an api we can use instead?\nCool idea. I can't really comment on the cost (if any) of distributing another relatively large data file.\r\n\r\nSome thoughts: the technical improvement to modeling would be minor: the only use of 'altitude' is to estimate air pressure for a clear-sky model or for adjusting geometric solar position to apparent position. But there's an intangible benefit from not having to look up a value, or having to explain why the default 0m is good enough. To me the intangible benefit is likely greater than the improvement in model results.\r\n\r\nAs alternatives, several of the weather data sources (PSM3, others), but not all, return altitude. \nCould file size be reduced by omitting the ocean bathymetry data?  I'm not really familiar with h5 files.  If accuracy is not so important, maybe cutting the values down to 8 bits would be acceptable. \r\n\r\n> Cool idea\r\n\r\nYes!  Makes me wonder if there are other such datasets we should consider. \n> Accuracy is not so important\r\n\r\nGood point. Altitude rounded to 100m increments is probably good enough.\nI'm unsure about packaging an elevation map with pvlib. But a higher resolution data set could be useful for shading. I thought there were some PR discussions along those lines.\nI think getting altitude is probably something we can outsource to an API like the Google Maps maybe, ESRI, or MapQuest? Seems a lot easier than adding 10 mb to the package maybe?\nFound these:\r\n* [Google map elevation API](https://developers.google.com/maps/documentation/elevation/start), [guide](https://developers.google.com/maps/documentation/elevation/overview)\r\n* [open elevation api](https://open-elevation.com/)\r\n* [Air Map API](https://developers.airmap.com/docs/elevation-api)\r\n* [open topo data api](https://www.opentopodata.org/)\r\n* [DEM.NET](https://elevationapi.com/faq) EG\r\n\r\n    > This URL will give you (quickly!) the elevation near Notre-Dame de Paris (39 meters), using SRTM GL3 dataset : \r\n[`https://api.elevationapi.com/api/Elevation?lat=48.854624&lon=2.3436793&dataSet=SRTM_GL3`](https://api.elevationapi.com/api/Elevation?lat=48.854624&lon=2.3436793&dataSet=SRTM_GL3)\r\n\r\nAnd more\u2026 I googled \u201celevation api lookup\u201d\nI tried the suggestions to reduce the size of the altitude lookup map.\r\nIf I set the oceans data to zero, it compresses much better and goes down to `3.9 mb`.\r\nI also tried encoding the remaining land data as `uint8` in `35m` increments.\r\nThis `uint8` version is `1.2 mb` after gzip compression.\r\n\r\nI agree that an API would be good for high-resolution use cases like shading. \r\nBut I think having a fallback built-in is still valid.\r\nAn external API has more friction; You need API keys, there can be costs or limits, and it adds more latency.\nAlso, where I got the original data can be helpful in high-resolution use-cases. It is hosted for free by aws in their [open data initiative](https://registry.opendata.aws/terrain-tiles/).\r\nYou can fetch terrain tiles using [slippy tilenames](https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames) from this URL template\r\n`https://elevation-tiles-prod.s3.amazonaws.com/v2/terrarium/{zoom}/{x}/{y}.png`.\r\nAnd [here](https://github.com/tilezen/joerd/blob/master/docs/formats.md) are the docs for how to convert RGB -> meters\nI'm -1 to having `Location.get_solarposition()` make web requests, if that's being proposed.  I'm cautiously open to bundling a local dataset if it's only a ~1MB expense (our [current dist](https://pypi.org/project/pvlib/#files) is ~30 MB).\r\n\r\nI'm also open to `iotools` functions for elevation data, although I'm not sure I see a lot of value in it if the relevant API calls are as simple as `requests.get(\"url?lon=X&lat=Y\").json()['elevation']`.  I think I'd rather see a gallery example showing how to DIY in that case. \nThis discussion suggests there are two, perhaps complementary, enhancements:\r\n- provide a low-resolution, low-accuracy altitude to supplement the current default of 0. for clearsky and solar position calculations.\r\n- via iotools, provide access to an API for high-resolution elevation data for detailed modeling of shading, tracker steering, and (in future pvlib) spatially varying module orientation.\r\n\r\nAccess to an API for detailed elevation data will not relieve those who want a simple alternative to the current default. Similarly, a low-resolution file included with pvlib will not satisfy those looking for high accuracy elevation.\r\n\r\nI'm not opposed to having all three (current default, low-resolution internal lookup, high resolution via API) in pvlib. \nI think it'd be great to have the option of something like this: \r\n\r\n    pvlib.location.Location(latitude=55, longitude=10, altitude='SRTM')\r\n\r\nand then have the location object call an API (using a function from iotools) that retrieved the elevation.\nIt looks like the 1.2 mb version might be acceptable. I will make a pull request early next week and you can make the final decision then. I also will play with the encoding a bit, there might be ways to increase accuracy while staying ~1 mb.\nI wonder if folks will start to cannibalize  pvlib for elevation data, and then send issues complaining that it's only accurate to 35m :rofl:\n> I wonder if folks will start to cannibalize pvlib for elevation data, and then send issues complaining that it's only accurate to 35m \ud83e\udd23\r\n\r\nAssuredly, yes.",
        "base": "6a94e35ffae279468d59577a1d2fbefdcbf768d9",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/clearsky.py",
            "pvlib/location.py",
            "pvlib/tools.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1239",
        "problem": "The Perez diffuse model should not be forcing the horizon coefficient up to zero\nThe perez model in irradiance.py forces F2, and thus the horizon component of diffuse, to be non-negative.  This restriction should not happen.  F2 and the horizon coefficient should be allowed to be negative and to reduce overall diffuse.\r\n\r\nAccording to the original paper at https://www.osti.gov/servlets/purl/7024029\r\nSection III.2 states this explicitly for the horizon component:\r\n\"(2) The horizon brightening coefficient, F2, is negative for overcast and low E occurrences -- indicative of brightening of the zenithal region of the sky for these conditions. This becomes positive past intermediate conditions and increases substantially with clearness.\"\r\n\r\nWe observed a higher than expected POAI, coming from poa diffuse, on cloudy days at certain sites.\r\nExpected:\r\nHorizon (burgundy) can be less than zero and sky diffuse (green) is less than isotropic (blue)\r\n![image](https://user-images.githubusercontent.com/81724637/119172295-9ebc7900-ba1a-11eb-8e1a-3a170e1f995a.png)\r\n\r\nObserved from PVLib:\r\nHorizon is prevented from being negative and sky diffuse ends up higher than isotropic.\r\n![image](https://user-images.githubusercontent.com/81724637/119172410-c4498280-ba1a-11eb-8e91-540db0ddc609.png)\r\n\r\nRepro'd on PVLib 0.8.1\r\n\r\nSee added test case in the PR for this repro case.\r\n\n",
        "hint": "",
        "base": "276a30de72a392ff59f798b8850229e9527ff03e",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/irradiance.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1458",
        "problem": "Update get_cams protocol to https\nAccording to an email sent out by Transvalor on May 12th 2022, the SoDa websites and services will switch from using \"HTTP\" to only \"HTTPS\". \r\n\r\nThe existing HTTP endpoints will redirect to the correct HTTPS sites, hence without any changes the [`get_cams`](https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.iotools.get_cams.html) function will continue to work correctly (as the request package handles redirects automatically). However, for good practice and to avoid unnecessary redirects, updating the existing URLs and endpoints to HTTPS is surely a good idea:\r\nhttps://github.com/pvlib/pvlib-python/blob/a0812b12584cfd5e662fa5aeb8972090763a671f/pvlib/iotools/sodapro.py#L188\r\n\r\n<br>\r\nFor reference, here's a screen-shot of Transvalor email:\r\n\r\n![image](https://user-images.githubusercontent.com/39184289/168595497-095c17c1-3fec-43c9-b6fd-49c928b51d78.png)\r\n\r\nFYI: this is a good first issue to tackle to get familiar with contribution to pvlib :) \n",
        "hint": "",
        "base": "a0812b12584cfd5e662fa5aeb8972090763a671f",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/iotools/sodapro.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1218",
        "problem": "change eta_m to module_efficiency\n`temperature.noct_sam` uses `eta_m_ref` to describe the module efficiency at reference conditions and `temperature.pvsyst_cell` uses `eta_m` to describe the module efficiency generically.\r\n\r\nJust calling both of these `module_efficiency` would make the function signatures easily understandable by many more people. I'd be ok with `module_efficiency_ref` but I don't think that precision is very important.\r\n\r\nI skimmed [pvterms](https://duramat.github.io/pv-terms/) and didn't see a suggestion for this quantity.\r\n\r\n`temperature.noct_sam` has not yet been released and it's just a positional argument, so changing the name is trivial. `temperature.pvsyst_cell` would need a deprecation cycle.\r\n\r\nOriginally discussed in https://github.com/pvlib/pvlib-python/pull/1177#discussion_r589081257\r\n\r\nAssignment of milestone indicates that we will act on this or close it forever before 0.9 is released.\r\n\n",
        "hint": "I support this change.",
        "base": "40ba4bd5c8b91754aa73e638ed984ab9657847cd",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/modelchain.py",
            "pvlib/pvsystem.py",
            "pvlib/temperature.py"
        ]
    },
    {
        "pr": "pvlib/pvlib-python/1154",
        "problem": "pvlib.irradiance.reindl() model generates NaNs when GHI = 0\n**Describe the bug**\r\nThe reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to \"term3\" having a quotient that divides by GHI.  \r\n\r\n**Expected behavior**\r\nThe reindl function should result in zero sky diffuse when GHI is zero.\r\n\r\n\npvlib.irradiance.reindl() model generates NaNs when GHI = 0\n**Describe the bug**\r\nThe reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to \"term3\" having a quotient that divides by GHI.  \r\n\r\n**Expected behavior**\r\nThe reindl function should result in zero sky diffuse when GHI is zero.\r\n\r\n\n",
        "hint": "Verified. Looks like an easy fix.\nVerified. Looks like an easy fix.",
        "base": "0b8f24c265d76320067a5ee908a57d475cd1bb24",
        "env": "ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91",
        "files": [
            "pvlib/irradiance.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1741",
        "problem": "Consider creating a ``UninferableType`` or ``_Uninferable`` class\nI opened https://github.com/microsoft/pyright/issues/3641 as I wondered why `pyright` didn't recognise how we type `Uninferable`. Normally they are a little bit more up to date than `mypy` so I wondered if this was intentional.\r\n\r\nTurns out it is. According to them, the way we currently handle the typing of `Uninferable` is incorrect and should ideally be refactored.\r\nAs we're stille early days into the typing of `astroid` I think there is still chance to do this.\r\n\r\nTheir suggestion is to create a private class (`_Uninferable`) which `Uninferable` can then instantiate. One of the issues with this is that we tend to require `Uninferable` as a type in `pylint` as well and so we would need to import that private class as well.\r\nWe could also create a public class, perhaps suffixed with `Type`, which we document as only being useful for typing.\r\n\r\nLet me know if you guys thinks this is something we should do and what approach is best.\r\n\r\n/CC @cdce8p As you're likely interested in this.\n",
        "hint": "@jacobtylerwalls Do you have a preference for either of the two solutions?\nI think `UninferableType` is more clear. I wish [PEP 661](https://peps.python.org/pep-0661/) had an implementation.",
        "base": "bcaecce5634a30313e574deae101ee017ffeff17",
        "env": "29b42e5e9745b172d5980511d14efeac745a5a82",
        "files": [
            "astroid/arguments.py",
            "astroid/bases.py",
            "astroid/brain/brain_builtin_inference.py",
            "astroid/brain/brain_dataclasses.py",
            "astroid/brain/brain_functools.py",
            "astroid/brain/brain_namedtuple_enum.py",
            "astroid/brain/brain_typing.py",
            "astroid/builder.py",
            "astroid/constraint.py",
            "astroid/helpers.py",
            "astroid/inference.py",
            "astroid/inference_tip.py",
            "astroid/interpreter/objectmodel.py",
            "astroid/nodes/node_classes.py",
            "astroid/nodes/scoped_nodes/scoped_nodes.py",
            "astroid/protocols.py",
            "astroid/typing.py",
            "astroid/util.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1616",
        "problem": "Infer calls to str.format() on names\nFuture enhancement could infer this value instead of giving an empty string:\r\n\r\n```python\r\nfrom astroid import extract_node\r\ncall = extract_node(\"\"\"\r\nx = 'python is {}'\r\nx.format('helpful sometimes')\r\n\"\"\")\r\ncall.inferred()[0].value  # gives \"\"\r\n```\r\n\r\n_Originally posted by @jacobtylerwalls in https://github.com/PyCQA/astroid/pull/1602#discussion_r893423433_\n",
        "hint": "",
        "base": "6280a758733434cba32b719519908314a5c2955b",
        "env": "52f6d2d7722db383af035be929f18af5e9fe8cd5",
        "files": [
            "astroid/brain/brain_builtin_inference.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1978",
        "problem": "Deprecation warnings from numpy\n### Steps to reproduce\r\n\r\n1. Run pylint over the following test case:\r\n\r\n```\r\n\"\"\"Test case\"\"\"\r\n\r\nimport numpy as np\r\nvalue = np.random.seed(1234)\r\n```\r\n\r\n### Current behavior\r\n```\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n```\r\n\r\n### Expected behavior\r\nThere should be no future warnings.\r\n\r\n### python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\" output\r\n2.12.13\n",
        "hint": "This seems very similar to https://github.com/PyCQA/astroid/pull/1514 that was fixed in 2.12.0.\nI'm running 2.12.13 (> 2.12.0), so the fix isn't working in this case?\nI don't know why #1514 did not fix this, I think we were capturing both stdout and stderr, so this will need some investigation. My guess would be that there's somewhere else to apply the same method to.\nHello, \r\nI see the same error with pylint on our tool [demcompare](https://github.com/CNES/demcompare). Pylint version:\r\n```\r\npylint --version\r\npylint 2.15.9\r\nastroid 2.12.13\r\nPython 3.8.10 (default, Nov 14 2022, 12:59:47) \r\n[GCC 9.4.0]\r\n```\r\nI confirm the weird astroid lower warning and I don't know how to bypass it with pylint checking. \r\n\r\n```\r\npylint demcompare \r\n/home/duboise/work/src/demcompare/venv/lib/python3.8/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n... (four times)\r\n```\r\n\r\nThanks in advance if there is a solution\r\nCordially\r\n\n> Thanks in advance if there is a solution\r\n\r\nwhile annoying the warning does not make pylint fail. Just ignore it. In a CI you can just check pylint return code. It will return 0 as expected\nI agree, even if annoying because it feels our code as a problem somewhere, the CI with pylint doesn't fail indeed. Thanks for the answer that confirm to not bother for now. \nThat might be fine in a CI environment, but for users, ultimately, ignoring warnings becomes difficult when there are too many such warnings. I would like to see this fixed.\nOh, it was not an argument in favour of not fixing it. It was just to point out that it is not a breaking problem. It is \"just\" a lot of quite annoying warnings. I am following the issue because it annoys me too. So I am in the same \"I hope they will fix it\" boat\n> I don't know why https://github.com/PyCQA/astroid/pull/1514 did not fix this, I think we were capturing both stdout and stderr, so this will need some investigation. My guess would be that there's somewhere else to apply the same method to.\r\n\r\nThat PR only addressed import-time. This `FutureWarning` is emitted by numpy's package-level `__getattr__` method, not during import.",
        "base": "0c9ab0fe56703fa83c73e514a1020d398d23fa7f",
        "env": "0c9ab0fe56703fa83c73e514a1020d398d23fa7f",
        "files": [
            "astroid/raw_building.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/934",
        "problem": "error during inference of class inheriting from another with `mod.Type` format\nConsider package a `level` with a class `Model` defined in `level`'s `__init__.py` file.\r\n\r\n```\r\nclass Model:\r\n    data: int = 1\r\n```\r\n\r\nIf a class `Test` inherits from `Model` as `class Test(Model)`, and `Model` comes from `from level import Model`,  then inferring `Test.data` works fine (below, A is an alias for astroid).\r\n\r\n<img width=\"248\" alt=\"Screen Shot 2021-02-19 at 09 41 09\" src=\"https://user-images.githubusercontent.com/2905588/108505730-9b3c1900-7296-11eb-8bb8-5b66b7253cf4.png\">\r\n\r\nHowever, if a `Test` inherits from `Model` as `class Test(level.Model)` and `level` comes from `import level`, then inference triggers an exception.\r\n\r\n<img width=\"784\" alt=\"Screen Shot 2021-02-19 at 09 42 09\" src=\"https://user-images.githubusercontent.com/2905588/108505815-beff5f00-7296-11eb-92a2-641be827e1f0.png\">\r\n\r\n\r\n\r\n\r\n\n",
        "hint": "",
        "base": "4cfd9b6d1003b9912ab94538e1dfa5d734f55251",
        "env": "d2ae3940a1a374253cc95d8fe83cb96f7a57e843",
        "files": [
            "astroid/inference.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1962",
        "problem": "v2.13.x regression: Crash when inspecting `PyQt5.QtWidgets` due to `RuntimeError` during `hasattr`\n### Steps to reproduce\r\n\r\nInstall PyQt5, run `pylint --extension-pkg-whitelist=PyQt5 x.py` over a file containing `from PyQt5 import QtWidgets`\r\n\r\n### Current behavior\r\n\r\nWith astroid 2.12.13 and pylint 2.15.10, this works fine. With astroid 2.13.2, this happens:\r\n\r\n```pytb\r\nException on node <ImportFrom l.1 at 0x7fc5a3c47d00> in file '/home/florian/tmp/pylintbug/x.py'\r\nTraceback (most recent call last):\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\r\n    self._check_module_attrs(node, module, name.split(\".\"))\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\r\n    module = next(module.getattr(name)[0].infer())\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\r\n    result = [self.import_module(name, relative_only=True)]\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\r\n    return self.ast_from_module(named_module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\r\n    elif hasattr(member, \"__all__\"):\r\nRuntimeError: wrapped C/C++ object of type QApplication has been deleted\r\nx.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-11-06-17.txt'. (astroid-error)\r\n```\r\n\r\nIt looks like it happens when `member` is `QtWidgets.qApp`, which is a kind of \"magic\" object referring to the QApplication singleton. Since none exists, it looks like PyQt doesn't like trying to access an attribute on that.\r\n\r\nBisected to:\r\n\r\n- #1885 \r\n\r\nIt looks like 974f26f75eb3eccb4bcd8ea143901baf60a685ff is the exact culprit.\r\n\r\ncc @nickdrozd \r\n\r\n(took the freedom to add appropriate labels already, hope that's fine)\r\n\n",
        "hint": "Thank you for the investigation @The-Compiler ! I'm going to revert the offending commit seeing it's only a cleanup. Let's do something more elegant later on if required.\nThe commit you linked is from a fork, so I tried to reproduce locally to bisect on pylint main repo but I cannot\r\n```\r\npip3 install PyQt5\r\nCollecting PyQt5\r\n Collecting PyQt5-sip<13,>=12.11\r\nCollecting PyQt5-Qt5>=5.15.0\r\nInstalling collected packages: PyQt5-Qt5, PyQt5-sip, PyQt5\r\nSuccessfully installed PyQt5-5.15.7 PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.0\r\n(venv) fix-crash-regression-2.13.2: pylint --extension-pkg-whitelist=PyQt5 x.py \r\n************* Module x\r\nx.py:1:0: W0611: Unused QtWidgets imported from PyQt5 (unused-import)\r\n---------------------------------------------------------------------\r\n\r\nYour code has been rated at 0.00/10 (previous run: -50.00/10, +50.00)\r\n```\r\nCould you provide your other dependencies maybe ?\r\n\n> The commit you linked is from a fork, so I tried to reproduce locally to bisect on pylint main repo but I cannot\r\n\r\nThe main astroid repo bisects to the merge of #1885 (f26dbe419ac15a87ed65e9b55ed15d3d8100b608) - that was a squash merge, 974f26f75eb3eccb4bcd8ea143901baf60a685ff is my guess at what in the PR is the problem.\r\n\r\n> Could you provide your other dependencies maybe ?\r\n\r\nNothing else really:\r\n\r\n```\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ python3 --version\r\nPython 3.10.8\r\n\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ python3 -m venv .venv\r\npython3 -m venv .venv  2.87s user 0.26s system 98% cpu 3.170 total\r\n\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ .venv/bin/pip install pylint PyQt5\r\nCollecting pylint\r\n  Using cached pylint-2.15.10-py3-none-any.whl (509 kB)\r\nCollecting PyQt5\r\n  Using cached PyQt5-5.15.7-cp37-abi3-manylinux1_x86_64.whl (8.4 MB)\r\nCollecting tomli>=1.1.0\r\n  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\r\nCollecting tomlkit>=0.10.1\r\n  Using cached tomlkit-0.11.6-py3-none-any.whl (35 kB)\r\nCollecting platformdirs>=2.2.0\r\n  Using cached platformdirs-2.6.2-py3-none-any.whl (14 kB)\r\nCollecting mccabe<0.8,>=0.6\r\n  Using cached mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\r\nCollecting dill>=0.2\r\n  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\r\nCollecting astroid<=2.14.0-dev0,>=2.12.13\r\n  Using cached astroid-2.13.2-py3-none-any.whl (272 kB)\r\nCollecting isort<6,>=4.2.5\r\n  Using cached isort-5.11.4-py3-none-any.whl (104 kB)\r\nCollecting PyQt5-Qt5>=5.15.0\r\n  Using cached PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\r\nCollecting PyQt5-sip<13,>=12.11\r\n  Using cached PyQt5_sip-12.11.0-cp310-cp310-manylinux1_x86_64.whl (359 kB)\r\nCollecting lazy-object-proxy>=1.4.0\r\n  Using cached lazy_object_proxy-1.9.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63 kB)\r\nCollecting wrapt<2,>=1.11\r\n  Using cached wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\r\nCollecting typing-extensions>=4.0.0\r\n  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\r\nInstalling collected packages: PyQt5-Qt5, wrapt, typing-extensions, tomlkit, tomli, PyQt5-sip, platformdirs, mccabe, lazy-object-proxy, isort, dill, PyQt5, astroid, pylint\r\nSuccessfully installed PyQt5-5.15.7 PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.0 astroid-2.13.2 dill-0.3.6 isort-5.11.4 lazy-object-proxy-1.9.0 mccabe-0.7.0 platformdirs-2.6.2 pylint-2.15.10 tomli-2.0.1 tomlkit-0.11.6 typing-extensions-4.4.0 wrapt-1.14.1\r\n\r\n[notice] A new release of pip available: 22.2.2 -> 22.3.1\r\n[notice] To update, run: python3 -m pip install --upgrade pip\r\n.venv/bin/pip install pylint PyQt5  3.18s user 0.57s system 74% cpu 5.004 total\r\n\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ cat x.py\r\nfrom PyQt5 import QtWidgets\r\n\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ .venv/bin/pylint --extension-pkg-whitelist=PyQt5 x.py \r\n************* Module x\r\nx.py:1:0: C0114: Missing module docstring (missing-module-docstring)\r\nException on node <ImportFrom l.1 at 0x7fa1f6e37d90> in file '/home/florian/tmp/pylintbug/x.py'\r\nTraceback (most recent call last):\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\r\n    self._check_module_attrs(node, module, name.split(\".\"))\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\r\n    module = next(module.getattr(name)[0].infer())\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\r\n    result = [self.import_module(name, relative_only=True)]\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\r\n    return self.ast_from_module(named_module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\r\n    elif hasattr(member, \"__all__\"):\r\nRuntimeError: wrapped C/C++ object of type QApplication has been deleted\r\nx.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-12-36-04.txt'. (astroid-error)\r\n\r\n------------------------------------------------------------------\r\nYour code has been rated at 0.00/10 (previous run: 0.00/10, +0.00)\r\n```\r\n",
        "base": "fe058bff95745371df5796286d33677c21137847",
        "env": "fe058bff95745371df5796286d33677c21137847",
        "files": [
            "astroid/raw_building.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/2240",
        "problem": "`.arguments` property ignores keyword-only args, *args, and **kwargs\n```python\r\n>>> from astroid import extract_node\r\n>>> node = extract_node(\"\"\"def a(*args, b=None, c=None, **kwargs): ...\"\"\")\r\n>>> node.args.arguments\r\n[]\r\n```\r\n\r\nExpected to find all the arguments from the function signature.\r\n\r\nThe wanted data can be found here:\r\n\r\n```python\r\n>>> node.args.vararg\r\n'args'\r\n>>> node.args.kwarg\r\n'kwargs'\r\n>>> node.args.kwonlyargs\r\n[<AssignName.b l.1 at 0x1048189b0>, <AssignName.c l.1 at 0x104818830>]\r\n```\r\n\r\nDiscussed at https://github.com/pylint-dev/pylint/pull/7577#discussion_r989000829.\r\n\r\nNotice that positional-only args are found for some reason \ud83e\udd37 \n",
        "hint": "Should the definition be changed as well? It states `args.arguments` returns required arguments, and AFAIK in the example none are required (I can call `a` without supplying any arguments).\r\n\r\nI tried running the following:\r\n\r\n```\r\n>>> node = extract_node(\"\"\"def a(kiwi, apple, *args, b=None, c=None, **kwargs): ...\"\"\")\r\n>>> node.args.arguments\r\n[<AssignName.kiwi l.1 at 0x7f5c55986b90>, <AssignName.apple l.1 at 0x7f5c55985a50>]\r\n```\r\n\r\nAnd it seems correct to me :thinking: \nhttps://github.com/pylint-dev/astroid/blob/fef38f2dd474b0dacd1dda3f15abbf61eb0e9a71/astroid/nodes/node_classes.py#L685-L688\r\n\r\nThe docstring seems to be correct?\nDepends on how you parse the language. \"positional and keyword\" could describe the argument `kiwi` and exclude keyword-only arguments.\r\n\r\nEssentially, the crux of this is whether we should\r\n- leave the function as is, and audit everywhere that uses it (given that we keep finding bugs)\r\n- change the function\r\n\r\n@crazybolillo have you happened to sample the places that use this function to be able to offer a view on that? I'd be eager to hear it!\nI think I got confused about the documentation :sob:. I was reading the docstring for `args` (`node.args.args` in the example):\r\n\r\nhttps://github.com/pylint-dev/astroid/blob/d4f4452fe089f600bf9144ffdcd8e698816df3c2/astroid/nodes/node_classes.py#L554-L555\r\n\r\n but we are dealing with `arguments` (`node.args.arguments`). I will review the code further to see if I can come up with something\r\n\r\n",
        "base": "514991036806e9cda2b12cef8ab3184ac373bd6c",
        "env": "1113d490ec4a94cdc1b35f45abfdaca9f19fa31e",
        "files": [
            "astroid/arguments.py",
            "astroid/nodes/node_classes.py",
            "astroid/nodes/scoped_nodes/scoped_nodes.py",
            "astroid/protocols.py",
            "astroid/rebuilder.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/941",
        "problem": "@property members defined in metaclasses of a base class are not correctly inferred\nRef https://github.com/PyCQA/astroid/issues/927#issuecomment-817244963\r\n\r\nInference works on the parent class but not the child in the following example:\r\n\r\n```python\r\nclass BaseMeta(type):\r\n    @property\r\n    def __members__(cls):\r\n        return ['a', 'property']\r\nclass Parent(metaclass=BaseMeta):\r\n    pass\r\nclass Derived(Parent):\r\n    pass\r\nParent.__members__  # [<Set.set l.10 at 0x...>]\r\nDerived.__members__  # [<Property.__members__ l.8 at 0x...>]\r\n```\n",
        "hint": "Looks like this is caused by https://github.com/PyCQA/astroid/blob/f2b197a4f8af0ceeddf435747a5c937c8632872a/astroid/scoped_nodes.py#L2590-L2603. When we are inferring an attribute on a derived class then `class_context` is `True` but `metaclass` is `False` so the property itself is returned.",
        "base": "962becc0ae86c16f7b33140f43cd6ed8f1e8a045",
        "env": "d2ae3940a1a374253cc95d8fe83cb96f7a57e843",
        "files": [
            "astroid/brain/brain_namedtuple_enum.py",
            "astroid/scoped_nodes.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1417",
        "problem": "Replace `cachedproperty` with `functools.cached_property` (>= 3.8)\nI thought about this PR recently again. Typing `cachedproperty` might not work, but it can be replaced with `functools.cached_property`. We only need to `sys` guard it for `< 3.8`. This should work\r\n```py\r\nif sys.version_info >= (3, 8):\r\n    from functools import cached_property\r\nelse:\r\n    from astroid.decorators import cachedproperty as cached_property\r\n```\r\n\r\nAdditionally, the deprecation warning can be limited to `>= 3.8`.\r\n\r\n_Originally posted by @cdce8p in https://github.com/PyCQA/astroid/issues/1243#issuecomment-1052834322_\n",
        "hint": "@cdce8p Just thinking out loud: can we also use a type guard to define `cached_property`? Would `mypy` pick up on that? \n> @cdce8p Just thinking out loud: can we also use a type guard to define `cached_property`? Would `mypy` pick up on that?\r\n\r\nNot completely sure what you want to do with that.\r\n\r\nOn other thing, I just saw that we don't set the `python-version` for mypy. If we do that, we probably need to do some more workarounds to tell mypy `cachedproperty` is equal to `cached_property`. Adding `TYPE_CHECKING` could work\r\n```py\r\nif sys.version_info >= (3, 8) or TYPE_CHECKING:\r\n    from functools import cached_property\r\nelse:\r\n    from astroid.decorators import cachedproperty as cached_property\r\n```",
        "base": "da745538c7236028a22cdf0405f6829fcf6886bc",
        "env": "da745538c7236028a22cdf0405f6829fcf6886bc",
        "files": [
            "astroid/decorators.py",
            "astroid/mixins.py",
            "astroid/nodes/node_classes.py",
            "astroid/nodes/node_ng.py",
            "astroid/nodes/scoped_nodes/scoped_nodes.py",
            "astroid/objects.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1364",
        "problem": "MRO failure on Python 3.7 with typing_extensions\n### Steps to reproduce\r\n\r\nRun the following script on Python 3.7:\r\n\r\n```python\r\nfrom astroid import parse\r\nmodule = parse(\"\"\"\r\nimport abc\r\nimport typing\r\nimport dataclasses\r\n\r\nimport typing_extensions\r\n\r\nT = typing.TypeVar(\"T\")\r\n\r\nclass MyProtocol(typing_extensions.Protocol): pass\r\nclass EarlyBase(typing.Generic[T], MyProtocol): pass\r\nclass Base(EarlyBase[T], abc.ABC): pass\r\nclass Final(Base[object]): pass\r\n\"\"\")\r\n\r\n#                    typing.Protocol\r\n#                          |\r\n# typing.Generic[T]    MyProtocol\r\n#              \\       /\r\n#              EarlyBase     abc.ABC\r\n#                       \\    /\r\n#                        Base\r\n#                         |\r\n#                        Final\r\n\r\nfinal_def = module.body[-1]\r\nfinal_def.mro()\r\n```\r\n\r\n### Current behavior\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"xxx.py\", line 31, in <module>\r\n    print(\"mro:\", final_def.mro())\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 3009, in mro\r\n    return self._compute_mro(context=context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 2985, in _compute_mro\r\n    mro = base._compute_mro(context=context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 2999, in _compute_mro\r\n    return _c3_merge(unmerged_mro, self, context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 103, in _c3_merge\r\n    context=context,\r\nastroid.exceptions.InconsistentMroError: Cannot create a consistent method resolution order for MROs (tuple, object), (EarlyBase, tuple, Generic, object, MyProtocol), (ABC, object), (tuple, EarlyBase, ABC) of class <ClassDef.Base l.1347 at 0x7fa0efd52590>.\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo MRO error is raised; Python 3.7 doesn't raise an error.\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.6.7-dev0; the test case fails in pylint 2.9.6 and on the main branch at commit 6e8699cef0888631bd827b096533fc6e894d2fb2.\n",
        "hint": "",
        "base": "9363c34934f94124f4867caf1bdf8f6755201ccd",
        "env": "da745538c7236028a22cdf0405f6829fcf6886bc",
        "files": [
            "astroid/const.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/2023",
        "problem": "Invalid variable lookup when walrus operator is used\n### Steps to reproduce\r\n1. Consider following code in `loop_error.py`:\r\n\t```\r\n    \"\"\"Test module\"\"\"\r\n\r\n\r\n\tdef walrus_in_comprehension_test_2(some_path, module_namespace):\r\n\t    \"\"\"Suspected error\"\"\"\r\n\t    for mod in some_path.iterdir():\r\n\t        print(mod)\r\n\t\r\n\t    for org_mod in some_path.iterdir():\r\n\t        if org_mod.is_dir():\r\n\t            if mod := module_namespace.get_mod_from_alias(org_mod.name):\r\n\t                new_name = mod.name\r\n\t            else:\r\n\t                new_name = org_mod.name\r\n\t\r\n\t            print(new_name)\r\n\t```\r\n2. Run `pylint ./loop_error.py`\r\n\r\n### Current behavior\r\nA warning appears: ```W0631: Using possibly undefined loop variable 'mod' (undefined-loop-variable)```\r\n\r\n### Expected behavior\r\nNo warning, because the variable `mod` is always defined.\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n- 2.14.1\r\n- 2.15.0-dev0 on 56a65daf1ba391cc85d1a32a8802cfd0c7b7b2ab with Python 3.10.6\n",
        "hint": "## Investigation\r\nI tried to understand the possible reason for this warning. \r\n\r\n### Minimum reproducible code\r\nIt seems ultra-rare case, because we cannot:\r\n- remove first loop (`for mod in some_path.iterdir():`),\r\n- remove first condition (`if org_mod.is_dir():`),\r\n- rewrite walrus operator to standard assignment (`mod :=`),\r\n- remove `else` with body,\r\n\r\notherwise, there is no error.\r\n\r\n### Tracked calls\r\n- [`pylint/checkers/variables.py:_loopvar_name:419`](https://github.com/PyCQA/pylint/blob/bd22f2822f9344487357c90e18a8505705c60a29/pylint/checkers/variables.py#L2419) - in this function we got only 1 statement in `astmts` - should be 2,\r\n- I looked at `scope_lookup` functions. Until [`astroid/filter_statements.py:_filter_stmts:201`](https://github.com/PyCQA/astroid/blob/56a65daf1ba391cc85d1a32a8802cfd0c7b7b2ab/astroid/filter_statements.py#L201) there are always 2 statements - that is good.\r\n- So the problem is in [`astroid.are_exclusive`](https://github.com/PyCQA/astroid/blob/56a65daf1ba391cc85d1a32a8802cfd0c7b7b2ab/astroid/nodes/node_classes.py#L140). Probably there is an invalid (old, before Python 3.8) assumption that `ast.IfExp` branches are searched only in `IfExp.body` and `IfExp.orelse` and therefore they must be exclusive. But it turns out that if assignment is in `IfExp.test` (i.e. walrus operator), branches are not exclusive - actually there are always inclusive.\r\n\r\n### Simpler minimum reproducible code\r\nIt seems that this code is a minimum to reproduce error:\r\n```\r\nfrom astroid import nodes, extract_node\r\n\r\nnode_if, node_body, node_or_else = extract_node(\"\"\"\r\n    if val := True:  #@\r\n        print(val)  #@  \r\n    else:\r\n        print(val)  #@\r\n    \"\"\")\r\nnode_if: nodes.If\r\nnode_walrus = next(node_if.nodes_of_class(nodes.NamedExpr))\r\n\r\nassert not nodes.are_exclusive(node_walrus, node_body)\r\n```\r\n\r\n I will try to fix it and add test cases.",
        "base": "2108ae51b516458243c249cf67301cb387e33afa",
        "env": "29b42e5e9745b172d5980511d14efeac745a5a82",
        "files": [
            "astroid/nodes/node_classes.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1719",
        "problem": "Cython module with import triggers deep introspection for pandas, raises unhandled FutureWarning\nThis is a somewhat complicated situation to reproduce, but basically `pandas` throws `FutureWarning`s for certain attributes, and when you import it into a Cython module (triggering astroid's deep module inspection), these future warnings are not handled by astroid and bubble up as `AstroidError`s through to pylint. Here is a full repro:\r\n\r\n\r\n### Cython module `pyx.pyx`\r\n\r\n```python\r\n# distutils: language = c++\r\nimport pandas as pd\r\n\r\ncdef class Test:\r\n    def __cinit__(self):\r\n        ...\r\n```\r\n\r\n\r\n### Python module `test.py`\r\n\r\n```python\r\nimport pyx\r\n\r\npyx.Test()\r\n```\r\n\r\n\r\n\r\n### Commands\r\n```\r\ncythonize -a -i pyx.pyx\r\npylint --extension-pkg-allow-list=pyx,pandas test.py\r\n```\r\n\r\n\r\n### Exception\r\n```\r\nException on node <Import l.1 at 0x106b23ca0> in file '/Users/timkpaine/Programs/projects/other/astroid/test.py'\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 765, in _get_imported_module\r\n    return importnode.do_import_module(modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/mixins.py\", line 102, in do_import_module\r\n    return mymodule.import_module(\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(absmodname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/manager.py\", line 168, in ast_from_module_name\r\n    return self.ast_from_module(module, modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/manager.py\", line 265, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/builder.py\", line 91, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 311, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 367, in object_build\r\n    self.object_build(module, member)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 325, in object_build\r\n    member = getattr(obj, name)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pandas/__init__.py\", line 198, in __getattr__\r\n    warnings.warn(\r\nFutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 472, in visit_import\r\n    imported_module = self._get_imported_module(node, name)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 788, in _get_imported_module\r\n    raise astroid.AstroidError from e\r\nastroid.exceptions.AstroidError\r\n************* Module test\r\ntest.py:1:0: F0002: test.py: Fatal error while checking 'test.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/Users/timkpaine/Library/Caches/pylint/pylint-crash-2022-07-19-17.txt'. (astroid-error)\r\n```\r\n\r\n\r\n\r\n\r\n### Standalone (Non Cython) repro for convenience\r\n\r\n```python\r\nimport types\r\nimport pandas as pd\r\nfrom astroid.builder import AstroidBuilder\r\n\r\n\r\nm = types.ModuleType(\"test\")\r\nm.pd = pd\r\n\r\nAstroidBuilder().module_build(m, \"test\")\r\n```\r\n\r\n\r\nxref: https://github.com/PyCQA/pylint/issues/7205\r\nxref: https://github.com/PyCQA/astroid/pull/1719\n",
        "hint": "",
        "base": "5bb3ddef43b35c07485a84f90b6a453fc649e31d",
        "env": "fe058bff95745371df5796286d33677c21137847",
        "files": [
            "astroid/raw_building.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1614",
        "problem": "Crash when inferring `str.format` call involving unpacking kwargs\nWhen parsing the following file:\r\n\r\n<!--\r\n If sharing the code is not an option, please state so,\r\n but providing only the stacktrace would still be helpful.\r\n -->\r\n\r\n```python\r\nclass A:\r\n    def render(self, audit_log_entry: AuditLogEntry):\r\n        return \"joined team {team_slug}\".format(**audit_log_entry.data)\r\n\r\n\r\n\r\n```\r\n\r\npylint crashed with a ``AstroidError`` and with the following stacktrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/.../astroid/astroid/inference_tip.py\", line 38, in _inference_tip_cached\r\n    result = _cache[func, node]\r\nKeyError: (<function _infer_str_format_call at 0x1064a96c0>, <Call l.3 at 0x106c452d0>)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 731, in _check_file\r\n    check_astroid_module(ast_node)\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 950, in check_astroid_module\r\n    retval = self._check_astroid_module(\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 1000, in _check_astroid_module\r\n    walker.walk(node)\r\n  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 93, in walk\r\n    self.walk(child)\r\n  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 93, in walk\r\n    self.walk(child)\r\n  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/Users/.../pylint/pylint/checkers/classes/special_methods_checker.py\", line 170, in visit_functiondef\r\n    inferred = _safe_infer_call_result(node, node)\r\n  File \"/Users/.../pylint/pylint/checkers/classes/special_methods_checker.py\", line 31, in _safe_infer_call_result\r\n    value = next(inferit)\r\n  File \"/Users/.../astroid/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1752, in infer_call_result\r\n    yield from returnnode.value.infer(context)\r\n  File \"/Users/.../astroid/astroid/nodes/node_ng.py\", line 159, in infer\r\n    results = list(self._explicit_inference(self, context, **kwargs))\r\n  File \"/Users/.../astroid/astroid/inference_tip.py\", line 45, in _inference_tip_cached\r\n    result = _cache[func, node] = list(func(*args, **kwargs))\r\n  File \"/Users/.../astroid/astroid/brain/brain_builtin_inference.py\", line 948, in _infer_str_format_call\r\n    formatted_string = format_template.format(*pos_values, **keyword_values)\r\nKeyError: 'team_slug'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 688, in _check_files\r\n    self._check_file(get_ast, check_astroid_module, file)\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 733, in _check_file\r\n    raise astroid.AstroidError from e\r\nastroid.exceptions.AstroidError\r\n```\r\n***\r\ncc @DanielNoord in #1602 \r\nfound by pylint primer \ud83d\ude80 \n",
        "hint": "Thanks! Going to add `KeyError` to the caught exceptions.",
        "base": "aa5a0d92e640ee5f3fa9a8ba3ba058a7b594ca44",
        "env": "52f6d2d7722db383af035be929f18af5e9fe8cd5",
        "files": [
            "astroid/brain/brain_builtin_inference.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/984",
        "problem": "Pyreverse regression after #857 (astroid 2.5)\n### Steps to reproduce\r\n1. Checkout pylint's source (which contains pyreverse)\r\n1. cd `<pylint checkout>` \r\n2. Run `source .tox/py39/bin/activate` or similar (you may need to run a tox session first)\r\n3. Ensure you have `astroid` ac2b173bc8acd2d08f6b6ffe29dd8cda0b2c8814 or later\r\n4. Ensure you have installed `astroid` (`python3 -m pip install -e <path-to-astroid>`) as dependencies may be different\r\n4. Run `pyreverse --output png --project test tests/data`\r\n\r\n### Current behaviour\r\nA `ModuleNotFoundError` exception is raised.\r\n\r\n```\r\n$ pyreverse --output png --project test tests/data\r\nparsing tests/data/__init__.py...\r\nparsing /opt/contrib/pylint/pylint/tests/data/suppliermodule_test.py...\r\nparsing /opt/contrib/pylint/pylint/tests/data/__init__.py...\r\nparsing /opt/contrib/pylint/pylint/tests/data/clientmodule_test.py...\r\nTraceback (most recent call last):\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/bin/pyreverse\", line 8, in <module>\r\n    sys.exit(run_pyreverse())\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/__init__.py\", line 39, in run_pyreverse\r\n    PyreverseRun(sys.argv[1:])\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/main.py\", line 201, in __init__\r\n    sys.exit(self.run(args))\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/main.py\", line 219, in run\r\n    diadefs = handler.get_diadefs(project, linker)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/diadefslib.py\", line 236, in get_diadefs\r\n    diagrams = DefaultDiadefGenerator(linker, self).visit(project)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 210, in visit\r\n    self.visit(local_node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 207, in visit\r\n    methods[0](node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/diadefslib.py\", line 162, in visit_module\r\n    self.linker.visit(node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 210, in visit\r\n    self.visit(local_node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 207, in visit\r\n    methods[0](node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/inspector.py\", line 257, in visit_importfrom\r\n    relative = astroid.modutils.is_relative(basename, context_file)\r\n  File \"/opt/contrib/pylint/astroid/astroid/modutils.py\", line 581, in is_relative\r\n    parent_spec = importlib.util.find_spec(name, from_file)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.2_4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/util.py\", line 94, in find_spec\r\n    parent = __import__(parent_name, fromlist=['__path__'])\r\nModuleNotFoundError: No module named 'pylint.tests'\r\n```\r\n\r\n### Expected behaviour\r\nNo exception should be raised. Prior to #857 no exception was raised.\r\n\r\n```\r\n$ pyreverse --output png --project test tests/data\r\nparsing tests/data/__init__.py...\r\nparsing /opt/contributing/pylint/tests/data/suppliermodule_test.py...\r\nparsing /opt/contributing/pylint/tests/data/__init__.py...\r\nparsing /opt/contributing/pylint/tests/data/clientmodule_test.py...\r\n```\r\n\r\n### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\r\n`2.6.0-dev0` (cab9b08737ed7aad2a08ce90718c67155fa5c4a0)\r\n\n",
        "hint": "Some notes:\r\n\r\n- The `ModuleNotFoundError` exception is thrown by `importlib.util.find_spec`. Before python 3.7 this was an `AttributeError`\r\n- `modutils.is_relative()` is the call-site for the `find_spec` call\r\n- It seems that `is_ralative()` is trying to find a parent mod-spec for the anchor `from_file` parameter so that it can check to see if the `modname` is contained by it\r\n- Neither `is_relative` nor its tests, explicitly handle on-disk paths (that is, paths for which `os.path.exists()` returns `True`) vs virtual-paths (`exists() == False`). The seems to be important to `find_spec` which raises if the package's `__path__` attribute isn't found (which it won't be for virtual paths).\r\n- The existing unit tests for `is_realtive()` use a range of modules to check against a set of module paths via `__path__[0]`. Somehow these pass and it is not clear why/how\r\n\r\nOne workaround fix, that feels like a hack until I understand the problem better, is for `is_relative()` to handle the `ModuleNotFoundError`/`AttributeError` exceptions and them as signifying that the `parent_spec` is being not-yet found. However, that requires that the loop-logic is fixed for linux-like systems, otherwise you end up in an infinite loop with `len(file_path)>0` always being true for `/` and `file_path = os.path.dirname('/')` always returning `/`.\r\n\r\nI have written some new unit tests for this issue and extended the existing ones, but there is some fundamental logic underpinning the use of `importlib.util.find_spec` that I am not smart enough to understand. For example, why do the existing unit-tests not all use the same `modname` parameter? Is it to work around the import caching in `sys.modules`? Should we take that into account?\r\n\r\nMy unit tests look at both virtual and ondisk paths (because of the `if not os.path.isdir(from_file):` line in `is_realtive()`, and the docs for `find_spec`). They also look at both absolute and relative paths explicitly. Finally, they also use system modules and assert that they are already in the import cache.\r\n\r\n.. and I thought this fix was going to be easy.\n@doublethefish thanks for your report and investigation.\r\nI can reproduce it and i confirm that #857 is the origin of the issue.\nIt seems like this happens if an import inside a package which is at least two levels deep is processed.\r\nFor example, trying to run ``pyreverse`` on ``pylint`` itself will crash as soon as the import statements in checker modules in ``pylint.checkers.refactoring`` are processed.\r\n``importlib.util.find_spec(name, from_file)`` is called with ``name`` = ``checkers.refactoring``. \r\n``find_spec`` will then split this up and try to import ``checkers``, which fails because the path to _inside_ the ``pylint`` package (i.e. ``pylint/pylint`` from the repo root) is normally not in the Pythonpath.",
        "base": "d2394a3e24236106355418e102b1bb0f1bef879c",
        "env": "d2ae3940a1a374253cc95d8fe83cb96f7a57e843",
        "files": [
            "astroid/interpreter/_import/spec.py",
            "astroid/manager.py",
            "astroid/modutils.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1164",
        "problem": "ImportError: cannot import name 'Statement' from 'astroid.node_classes' \n### Steps to reproduce\r\n\r\n1. run pylint <some_file>\r\n\r\n\r\n### Current behavior\r\n\r\n```python\r\nexception: Traceback (most recent call last):\r\n  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/__main__.py\", line 9, in <module>\r\n    pylint.run_pylint()\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/__init__.py\", line 24, in run_pylint\r\n    PylintRun(sys.argv[1:])\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/lint/run.py\", line 331, in __init__\r\n    linter.load_plugin_modules(plugins)\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/lint/pylinter.py\", line 551, in load_plugin_modules\r\n    module = astroid.modutils.load_module_from_name(modname)\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/astroid/modutils.py\", line 218, in load_module_from_name\r\n    return importlib.import_module(dotted_name)\r\n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 855, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"/home/user/folder/check_mk/tests/testlib/pylint_checker_cmk_module_layers.py\", line 14, in <module>\r\n    from astroid.node_classes import Import, ImportFrom, Statement  # type: ignore[import]\r\nImportError: cannot import name 'Statement' from 'astroid.node_classes' (/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/astroid/node_classes.py)\r\n```\r\n\r\n### Expected behavior\r\nNo exception\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.7.3\r\npylint 2.10.2\r\nastroid 2.7.3\r\nPython 3.9.5 (default, May 11 2021, 08:20:37) \n",
        "hint": "This is caused by our local plugin.\r\n\r\nHas probably nothing to do with upstream. \nThis is caused by a bad refactor from us, we deprecated `astroid.node_classes` and `astroid.scoped_nodes` in favor of `astroid.nodes` but nothing should break before astroid 3.0.\n@Pierre-Sassoulas I see.\r\nAlso Statement is not available in astroid.nodes it is in astroid.nodes.node_classes\r\n\r\nWas the Statement also deprecated? Or called something else now?\nIt seems we're not using it ourselves or not by using `astroid.nodes` API so we did not realize it was not importable easily. But it should, I'm going to add it.",
        "base": "40ea1a3b8e52bbfed43deb1725cde461f4bd8a96",
        "env": "40ea1a3b8e52bbfed43deb1725cde461f4bd8a96",
        "files": [
            "astroid/nodes/__init__.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1030",
        "problem": "astroid has an undeclared dependency on setuptools.\nThe dependency is here: https://github.com/PyCQA/astroid/blob/1342591e2beb955a377e4486e5595478f79789e8/astroid/__pkginfo__.py#L29\n\nThe lack of declaration is here: https://github.com/PyCQA/astroid/blob/1342591e2beb955a377e4486e5595478f79789e8/setup.cfg#L37-L41\n",
        "hint": "Hmm, thank you for opening a report for this. There is the same problem in pylint btw. I thought `pkg_resources` was a builtin. Do you know a way to remove this dependency for python < 3.8 ? We can use ` from importlib.metadata import version` for python > 3.8. If there is no way to do that for 3.6 and 3.7 we could add a conditional requirements for those interpreters.\nYou can use https://pypi.org/project/importlib-metadata/ for 3.6 and 3.7 if I understand correctly.\nHo that's still a dependency to add but that's better, thank you for the advise, will do :) \nI'd send up an MR but this is all from my phone. On a road trip today and won't be back to a keyboard for a few days. Happy to review the fix though. Thanks for looking into this.\nI have a feeling that astroid do not need to calculate it's version like this so we could remove the dependency altogether. But pylint will need it because we broke a lot of pylint related tool when we removed the numversion in 2.8.0. Having to add a dependency for such a small benefit makes me rethink using ``setuptools_scm``. Having proper github actions would probably be better than that. There is such a release here: https://github.com/python/importlib_metadata/blob/main/.github/workflows/main.yml#L61. Maybe we just have to create a small script that will update the version in two places and be done with it. Thoughts @cdce8p ?\n> Maybe we just have to create a small script that will update the version in two places and be done with it.\r\n\r\nI believe there are already tools for it. `tbump` certainly looks promising although I haven't used it yet.\r\nhttps://github.com/dmerejkowsky/tbump",
        "base": "d2ae3940a1a374253cc95d8fe83cb96f7a57e843",
        "env": "d2ae3940a1a374253cc95d8fe83cb96f7a57e843",
        "files": [
            "astroid/__pkginfo__.py",
            "astroid/helpers.py",
            "astroid/node_classes.py",
            "astroid/transforms.py",
            "script/bump_changelog.py",
            "setup.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1333",
        "problem": "astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py\n### Steps to reproduce\r\n> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04\r\n\r\n> Update 2022-01-04: Corrected repro steps and added more environment details\r\n\r\n1. Set up simple repo with following structure (all files can be empty):\r\n```\r\nroot_dir/\r\n|--src/\r\n|----project/ # Notice the missing __init__.py\r\n|------file.py # It can be empty, but I added `import os` at the top\r\n|----__init__.py\r\n```\r\n2. Open a command prompt\r\n3. `cd root_dir`\r\n4. `python -m venv venv`\r\n5. `venv/Scripts/activate`\r\n6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2\r\n7. `pylint src/project` # Updated from `pylint src`\r\n8. Observe failure:\r\n```\r\nsrc\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:\r\n```\r\n\r\n### Current behavior\r\nFails with `src\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:`\r\n\r\n### Expected behavior\r\nDoes not fail with error.\r\n> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.9.1\r\n\r\n`python 3.9.1`\r\n`pylint 2.12.2 `\r\n\r\n\r\n\r\nThis issue has been observed with astroid `2.9.1` and `2.9.2`\n",
        "hint": "I can't seem to reproduce this in my `virtualenv`. This might be specific to `venv`? Needs some further investigation.\n@interifter Which version of `pylint` are you using?\nRight, ``pip install pylint astroid==2.9.0``, will keep the local version if you already have one, so I thought it was ``2.12.2`` but that could be false. In fact it probably isn't 2.12.2. For the record, you're not supposed to set the version of ``astroid`` yourself, pylint does, and bad thing will happen if you try to set the version of an incompatible astroid. We might want to update the issue's template to have this information next.\nMy apologies... I updated the repro steps with a critical missed detail: `pylint src/project`, instead of `pylint src`\r\n\r\nBut I verified that either with, or without, `venv`, the issue is reproduced.\r\n\r\nAlso, I never have specified the `astroid` version, before. \r\n\r\nHowever, this isn't the first time the issue has been observed.\r\nBack in early 2019, a [similar issue](https://stackoverflow.com/questions/48024049/pylint-raises-error-if-directory-doesnt-contain-init-py-file) was observed with either `astroid 2.2.0` or `isort 4.3.5`, which led me to try pinning `astroid==2.9.0`, which worked.\n> @interifter Which version of `pylint` are you using?\r\n\r\n`2.12.2`\r\n\r\nFull env info:\r\n\r\n```\r\nPackage           Version\r\n----------------- -------\r\nastroid           2.9.2\r\ncolorama          0.4.4\r\nisort             5.10.1\r\nlazy-object-proxy 1.7.1\r\nmccabe            0.6.1\r\npip               20.2.3\r\nplatformdirs      2.4.1\r\npylint            2.12.2\r\nsetuptools        49.2.1\r\ntoml              0.10.2\r\ntyping-extensions 4.0.1\r\nwrapt             1.13.3\r\n```\r\n\nI confirm the bug and i'm able to reproduce it with `python 3.9.1`. \r\n```\r\n$> pip freeze\r\nastroid==2.9.2\r\nisort==5.10.1\r\nlazy-object-proxy==1.7.1\r\nmccabe==0.6.1\r\nplatformdirs==2.4.1\r\npylint==2.12.2\r\ntoml==0.10.2\r\ntyping-extensions==4.0.1\r\nwrapt==1.13.3\r\n```\nBisected and this is the faulty commit:\r\nhttps://github.com/PyCQA/astroid/commit/2ee20ccdf62450db611acc4a1a7e42f407ce8a14\nFix in #1333, no time to write tests yet so if somebody has any good ideas: please let me know!",
        "base": "d2a5b3c7b1e203fec3c7ca73c30eb1785d3d4d0a",
        "env": "da745538c7236028a22cdf0405f6829fcf6886bc",
        "files": [
            "astroid/modutils.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1903",
        "problem": "Unhandled AttributeError during str.format template evaluation\n### Steps to reproduce\r\n\r\n1. Use `astroid` to parse code that provides arguments to a `str.format` template that attempts to access non-existent attributes\r\n\r\n```py\r\ndaniel_age = 12\r\n\"My name is {0.name}\".format(daniel_age)  # int literal has no 'name' attribute\r\n```\r\n\r\n### Current behavior\r\n\r\n1. unhandled `AttributeError` when it attempts to [evaluate the templated string](https://github.com/PyCQA/astroid/blob/8bdec591f228e7db6a0be66b6ca814227ff50001/astroid/brain/brain_builtin_inference.py#L956)\r\n\r\n### Expected behavior\r\n\r\n1. could raise an `AstroidTypeError` to indicate that the template formatting is invalid\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n`2.13.0-dev0`\r\n\r\nRelates to pycqa/pylint#7939.\nUnhandled AttributeError during str.format template evaluation\n### Steps to reproduce\r\n\r\n1. Use `astroid` to parse code that provides arguments to a `str.format` template that attempts to access non-existent attributes\r\n\r\n```py\r\ndaniel_age = 12\r\n\"My name is {0.name}\".format(daniel_age)  # int literal has no 'name' attribute\r\n```\r\n\r\n### Current behavior\r\n\r\n1. unhandled `AttributeError` when it attempts to [evaluate the templated string](https://github.com/PyCQA/astroid/blob/8bdec591f228e7db6a0be66b6ca814227ff50001/astroid/brain/brain_builtin_inference.py#L956)\r\n\r\n### Expected behavior\r\n\r\n1. could raise an `AstroidTypeError` to indicate that the template formatting is invalid\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n`2.13.0-dev0`\r\n\r\nRelates to pycqa/pylint#7939.\n",
        "hint": "\n",
        "base": "344454c8ee25d41a4ce12980bc85ba604b7835dd",
        "env": "fe058bff95745371df5796286d33677c21137847",
        "files": [
            "astroid/brain/brain_builtin_inference.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1196",
        "problem": "getitem does not infer the actual unpacked value\nWhen trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:\r\n\r\n- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. \r\n- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.\r\n\r\n\r\nHere is a short reproducer;\r\n\r\n```py\r\nfrom astroid import parse\r\n\r\n\r\nsource = \"\"\"\r\nX = {\r\n    'A': 'B'\r\n}\r\n\r\nY = {\r\n    **X\r\n}\r\n\r\nKEY = 'A'\r\n\"\"\"\r\n\r\ntree = parse(source)\r\n\r\nfirst_dict = tree.body[0].value\r\nsecond_dict = tree.body[1].value\r\nkey = tree.body[2].value\r\n\r\nprint(f'{first_dict.getitem(key).value = }')\r\nprint(f'{second_dict.getitem(key).value = }')\r\n\r\n\r\n```\r\n\r\nThe current output;\r\n\r\n```\r\n $ python t1.py                                                                                                 3ms\r\nfirst_dict.getitem(key).value = 'B'\r\nTraceback (most recent call last):\r\n  File \"/home/isidentical/projects/astroid/t1.py\", line 23, in <module>\r\n    print(f'{second_dict.getitem(key).value = }')\r\n  File \"/home/isidentical/projects/astroid/astroid/nodes/node_classes.py\", line 2254, in getitem\r\n    return value.getitem(index, context)\r\nAttributeError: 'Name' object has no attribute 'getitem'\r\n```\r\n\r\nExpeceted output;\r\n```\r\n $ python t1.py                                                                                                 4ms\r\nfirst_dict.getitem(key).value = 'B'\r\nsecond_dict.getitem(key).value = 'B'\r\n\r\n```\r\n\n",
        "hint": "",
        "base": "39c2a9805970ca57093d32bbaf0e6a63e05041d8",
        "env": "52f6d2d7722db383af035be929f18af5e9fe8cd5",
        "files": [
            "astroid/nodes/node_classes.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/2307",
        "problem": "Regression in Astroid version 2.15.7 in handling subscriptable type parameters\nAstroid version 2.15.7 fails to correctly handle a subscriptable type parameter  most likely due to the change in this [PR](https://github.com/pylint-dev/astroid/pull/2239). \r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nfrom collections.abc import Mapping\r\nfrom typing import Generic, TypeVar, TypedDict\r\nfrom dataclasses import dataclass\r\n\r\nclass Identity(TypedDict):\r\n    \"\"\"It's the identity.\"\"\"\r\n\r\n    name: str\r\n\r\nT = TypeVar(\"T\", bound=Mapping)\r\n\r\n@dataclass\r\nclass Animal(Generic[T]):\r\n    \"\"\"It's an animal.\"\"\"\r\n\r\n    identity: T\r\n\r\nclass Dog(Animal[Identity]):\r\n    \"\"\"It's a Dog.\"\"\"\r\n\r\ndog = Dog(identity=Identity(name=\"Dog\"))\r\nprint(dog.identity[\"name\"])\r\n```\r\n\r\n### Current behavior\r\nPylint (running Astroid 2.15.7) gives the following error for the example above:\r\n```\r\nE1136: Value 'dog.identity' is unsubscriptable (unsubscriptable-object)\r\n```\r\n### Expected behavior\r\nAstroid should correctly handle a subscriptable type parameter.\r\n\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.15.7\n",
        "hint": "Thanks for the report.\n@mbyrnepr2 Would you be willing to take a look?",
        "base": "1113d490ec4a94cdc1b35f45abfdaca9f19fa31e",
        "env": "1113d490ec4a94cdc1b35f45abfdaca9f19fa31e",
        "files": [
            "astroid/brain/brain_typing.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/927",
        "problem": "infer_stmts cannot infer multiple uses of the same AssignName\nGiven multiple assignments to the same target which both reference the same AssignName, infer_stmts fails for subsequent attempts after the first.\r\n\r\n### Steps to reproduce\r\n\r\nThis appears to be a minimum working example, removing any part removes the effect:\r\n\r\n```python\r\nfails = astroid.extract_node(\"\"\"\r\n    pair = [1, 2]\r\n    ex = pair[0]\r\n    if 1 + 1 == 2:\r\n        ex = pair[1]\r\n    ex\r\n\"\"\")\r\nprint(list(fails.infer()))\r\n# [<Const.int l.2 at 0x...>, Uninferable]\r\n```\r\n\r\nFor some context, I originally saw this with attributes on an imported module, i.e.\r\n\r\n```python\r\nimport mod\r\nex = mod.One()\r\n# later ... or in some branch\r\nex = mod.Two()\r\n```\r\n\r\n### Current behavior\r\n\r\nSee above.\r\n\r\n### Expected behavior\r\n\r\nInlining the variable or switching to a different name works fine:\r\n\r\n```python\r\nworks = astroid.extract_node(\"\"\"\r\n    # pair = [1, 2]\r\n    ex = [1, 2][0]\r\n    if 1 + 1 == 2:\r\n        ex = [1, 2][1]\r\n    ex\r\n\"\"\")\r\nprint(list(works.infer()))\r\n# [<Const.int l.3 at 0x...>, <Const.int l.5 at 0x...>]\r\n\r\nworks = astroid.extract_node(\"\"\"\r\n    first = [1, 2]\r\n    second = [1, 2]\r\n    ex = first[0]\r\n    if 1 + 1 == 2:\r\n        ex = second[1]\r\n    ex\r\n\"\"\")\r\nprint(list(works.infer()))\r\n# [<Const.int l.2 at 0x...>, <Const.int l.3 at 0x...>]\r\n```\r\n\r\nI would expect that the first failing example would work similarly. This (only) worked\r\nin astroid 2.5 and appears to have been \"broken\" by the revert of cc3bfc5 in 03d15b0 (astroid 2.5.1 and above).\r\n\r\n### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\r\n\r\n```\r\n$ python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"\r\n2.5-dev\r\n$ git rev-parse HEAD\r\n03d15b0f32f7d7c9b2cb062b9321e531bd954344\r\n```\r\n\n",
        "hint": "It appears that this is caused by `InferenceContext` maintaining a strong reference to the mutable set that is shared between clones, see this simplified example:\r\n\r\n```python\r\nclass Context:\r\n    def __init__(self, path=None):\r\n        self.path = path or set()\r\n    def clone(self):\r\n        return Context(path=self.path)\r\n\r\na = Context()\r\na.path.add('hello')\r\nb = a.clone()\r\nb.path.add('world')\r\nprint(a.path, b.path)\r\n# (set(['world', 'hello']), set(['world', 'hello']))\r\nprint(a.path is b.path)\r\n# True\r\n```",
        "base": "cd8b6a43192724128af68605ef22aabf3254f495",
        "env": "d2ae3940a1a374253cc95d8fe83cb96f7a57e843",
        "files": [
            "astroid/context.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1866",
        "problem": "\"TypeError: unsupported format string passed to NoneType.__format__\" while running type inference in version 2.12.x\n### Steps to reproduce\r\n\r\nI have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is \"valid\", it runs in production here.\r\n\r\n### Current behavior\r\n\r\nWhen running pylint on some code, I get this exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 183, in visit_functiondef\r\n    inferred = _safe_infer_call_result(node, node)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 42, in _safe_infer_call_result\r\n    value = next(inferit)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1749, in infer_call_result\r\n    yield from returnnode.value.infer(context)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py\", line 159, in infer\r\n    results = list(self._explicit_inference(self, context, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/inference_tip.py\", line 45, in _inference_tip_cached\r\n    result = _cache[func, node] = list(func(*args, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py\", line 956, in _infer_str_format_call\r\n    formatted_string = format_template.format(*pos_values, **keyword_values)\r\nTypeError: unsupported format string passed to NoneType.__format__\r\n```\r\n\r\n### Expected behavior\r\n\r\nTypeError exception should not happen\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.12.10,\r\n2.12.12\n",
        "hint": "Hi @crosser, thanks for the report.\r\n\r\n> I have no concise reproducer. \r\n\r\nWe might be able to help you distill one.\r\n\r\n`pylint` produces a crash report, and shows the link in your terminal, like this:\r\n```shell\r\n************* Module a\r\na.py:1:0: F0002: a.py: Fatal error while checking 'a.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/Users/.../Library/Caches/pylint/pylint-crash-2022-10-29-08-48-25.txt'. (astroid-error)\r\n```\r\nThe offending file is at the top of the crash report. If the code is too long, or contains sensitive information, you can use the knowledge that the crash happened in `_infer_str_format_call` to look for calls to `.format()` on strings. You should be able to then just provide us those calls--and enough surrounding code to rebuild the objects you provided to `format()`. \r\n\r\nDoing this would be a tremendous help!\n> `pylint` produces a crash report, and shows the link in your terminal, like this:\r\n\r\nNo, not really, it does not. I am attaching a (censored) stderr from running the test. The line in the source code that apparently triggers the problem is pretty innocuous:\r\n\r\n```\r\n    @property\r\n    def vnet_id(self):  # <---- this is the line 266 that is mentioned in the \"Exception on node\" message\r\n        if ...:\r\n```\r\nThere is very similar property definition right before this one, that does not trigger the problem.\r\n\r\n[pyerr.txt](https://github.com/PyCQA/astroid/files/9900190/pyerr.txt)\r\n\r\nPylint command was `python3 -m pylint --jobs=0 --rcfile=test/style/pylint.conf <project-dir>`\r\n\r\n```\r\n$ pylint --version\r\npylint 2.15.5\r\nastroid 2.12.12\r\nPython 3.10.8 (main, Oct 24 2022, 10:07:16) [GCC 12.2.0]\r\n```\r\n\r\nedit:\r\n> enough surrounding code to rebuild the objects you provided to format().\r\n\r\n_I_ did not provide any objects to `format()`, astroid did...\nThanks for providing the traceback.\r\n\r\n> No, not really, it does not. I am attaching a (censored) stderr from running the test. \r\n\r\nI see now that it's because you're invoking pylint from a unittest, so your test is managing the output.\r\n\r\n> The line in the source code that apparently triggers the problem is pretty innocuous:\r\n\r\nThe deeper failure is on the call in line 268, not the function def on line 266. Is there anything you can sanitize and tell us about line 268? Thanks again for providing the help.\n> I see now that it's because you're invoking pylint from a unittest, so your test is managing the output.\r\n\r\nWhen I run pylint by hand\r\n\r\n```\r\npylint --jobs=0 --rcfile=test/style/pylint.conf <module-name> | tee /tmp/pyerr.txt\r\n```\r\nthere is still no \"Fatal error while checking ...\" message in the output\r\n\r\n> > The line in the source code that apparently triggers the problem is pretty innocuous:\r\n> \r\n> The deeper failure is on the call in line 268, not the function def on line 266. Is there anything you can sanitize and tell us about line 268? Thanks again for providing the help.\r\n\r\nOh yes, there is a `something.format()` in that line! But the \"something\" is a literal string:\r\n```\r\n    @property\r\n    def vnet_id(self):\r\n        if self.backend == \"something\":\r\n            return \"{:04x}{:04x}n{:d}\".format(  # <---- this is line 268\r\n                self.<some-attr>, self.<another-attr>, self.<third-attr>\r\n            )\r\n        if self.backend == \"somethingelse\":\r\n            return \"h{:08}n{:d}\".format(self.<more-attr>, self.<and more>)\r\n        return None\r\n```\r\n\nThanks, that was very helpful. Here is a reproducer:\r\n```python\r\nx = \"{:c}\".format(None)\r\n```",
        "base": "6cf238d089cf4b6753c94cfc089b4a47487711e5",
        "env": "fe058bff95745371df5796286d33677c21137847",
        "files": [
            "astroid/brain/brain_builtin_inference.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1092",
        "problem": "Yield self is inferred to be of a mistaken type \n### Steps to reproduce\r\n\r\n1. Run the following\r\n```\r\nimport astroid\r\n\r\n\r\nprint(list(astroid.parse('''\r\nimport contextlib\r\n\r\nclass A:\r\n    @contextlib.contextmanager\r\n    def get(self):\r\n        yield self\r\n\r\nclass B(A):\r\n    def play():\r\n        pass\r\n\r\nwith B().get() as b:\r\n    b.play()\r\n''').ilookup('b')))\r\n```\r\n\r\n### Current behavior\r\n```Prints [<Instance of .A at 0x...>]```\r\n\r\n### Expected behavior\r\n```Prints [<Instance of .B at 0x...>]```\r\n\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.6.2\n",
        "hint": "Possible duplicate of #1008 \nThis problem is due to the fact `_infer_context_manager` takes caller function, using its current context, instead of the original callers context. This may be fixed for example by adding data to `Generator` instance, by `infer_call_result` that signifies its possible value types.\r\n\r\nI'm not familiar with the codebase, it seems to me that that this is not the correct approach, but, the correct approach is to pass this data inside the `context` structure. But it's not clear to me how to do that.",
        "base": "c9c498348174b38ce35bfe001353c8ebea262802",
        "env": "c9c498348174b38ce35bfe001353c8ebea262802",
        "files": [
            "astroid/bases.py",
            "astroid/protocols.py",
            "astroid/scoped_nodes.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/2309",
        "problem": "Regression in Astroid version 2.15.7 in handling subscriptable type parameters\nAstroid version 2.15.7 fails to correctly handle a subscriptable type parameter  most likely due to the change in this [PR](https://github.com/pylint-dev/astroid/pull/2239). \r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nfrom collections.abc import Mapping\r\nfrom typing import Generic, TypeVar, TypedDict\r\nfrom dataclasses import dataclass\r\n\r\nclass Identity(TypedDict):\r\n    \"\"\"It's the identity.\"\"\"\r\n\r\n    name: str\r\n\r\nT = TypeVar(\"T\", bound=Mapping)\r\n\r\n@dataclass\r\nclass Animal(Generic[T]):\r\n    \"\"\"It's an animal.\"\"\"\r\n\r\n    identity: T\r\n\r\nclass Dog(Animal[Identity]):\r\n    \"\"\"It's a Dog.\"\"\"\r\n\r\ndog = Dog(identity=Identity(name=\"Dog\"))\r\nprint(dog.identity[\"name\"])\r\n```\r\n\r\n### Current behavior\r\nPylint (running Astroid 2.15.7) gives the following error for the example above:\r\n```\r\nE1136: Value 'dog.identity' is unsubscriptable (unsubscriptable-object)\r\n```\r\n### Expected behavior\r\nAstroid should correctly handle a subscriptable type parameter.\r\n\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.15.7\n",
        "hint": "Thanks for the report.\n@mbyrnepr2 Would you be willing to take a look?",
        "base": "29b42e5e9745b172d5980511d14efeac745a5a82",
        "env": "29b42e5e9745b172d5980511d14efeac745a5a82",
        "files": [
            "astroid/brain/brain_typing.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/2219",
        "problem": "Implement new nodes for PEP 695: Type Parameter Syntax\nThere's a new syntax in python 3.12, we need to handle it before claiming we support 3.12, see https://docs.python.org/3.12/whatsnew/3.12.html#pep-695-type-parameter-syntax\n",
        "hint": "",
        "base": "efb34f2b84c9f019ffceacef3448d8351563b6a2",
        "env": "1113d490ec4a94cdc1b35f45abfdaca9f19fa31e",
        "files": [
            "astroid/brain/brain_datetime.py",
            "astroid/brain/brain_typing.py",
            "astroid/const.py",
            "astroid/inference.py",
            "astroid/nodes/__init__.py",
            "astroid/nodes/as_string.py",
            "astroid/nodes/node_classes.py",
            "astroid/nodes/scoped_nodes/scoped_nodes.py",
            "astroid/rebuilder.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/983",
        "problem": "Cannot infer empty functions\n### Steps to reproduce\r\n```python\r\nimport astroid\r\nastroid.extract_node(\"\"\"\r\ndef f():\r\n    pass\r\nf()\r\n\"\"\").inferred()\r\n```\r\n### Current behavior\r\nraises `StopIteration`\r\n\r\n### Expected behavior\r\nReturns `[const.NoneType]`\r\n\r\n### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\r\n\r\n2.0.0\r\n\r\nThis also applies to procedural functions which don't explicitly return any values.\n",
        "hint": "Makes sense.\r\n\r\nThis happens because the inference for functions looks through each return value here (https://github.com/PyCQA/astroid/blob/629c92db2dc1b016f4bf47645c95c42e65fd3bd6/astroid/scoped_nodes.py#L1558) and tries to infer the result from there. But since functions like this don't have an explicit return value, the inference gets into `raise_if_nothing_inferred` over here (https://github.com/PyCQA/astroid/blob/ac3e82e9bd8678086325a71a927a06bbc43d415e/astroid/decorators.py#L140), resulting in the exception you see. \nWhat should we infer for a function that always raises an exception? I don't think it should be `None`. Uninferable I guess?\r\n\r\n---\r\n\r\nI tried to add this, but It caused a cascade of errors where we are looking for Uninferable instead of const.None\n@brycepg I would say it should return `Uninferable`, as raising exceptions is not necessarily returning a value from the function. Although we'd want some mechanism to get what exceptions a function could raise.\r\n\r\nRegarding your last statement, you mean you added `Uninferable` for functions that raise exceptions or for functions that return `None`? Which of these failed with the cascade of errors?\n@PCManticore I made functions that do not have any return/yield nodes infer to `None` instead of `Uninferable`, and it broke a lot of tests.",
        "base": "53a20335357bbd734d74c3bfe22e3518f74f4d11",
        "env": "d2ae3940a1a374253cc95d8fe83cb96f7a57e843",
        "files": [
            "astroid/bases.py",
            "astroid/scoped_nodes.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1268",
        "problem": "'AsStringVisitor' object has no attribute 'visit_unknown'\n```python\r\n>>> import astroid\r\n>>> astroid.nodes.Unknown().as_string()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 609, in as_string\r\n    return AsStringVisitor()(self)\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py\", line 56, in __call__\r\n    return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 220, in accept\r\n    func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\r\nAttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'\r\n>>> \r\n```\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.8.6-dev0\n",
        "hint": "Thank you for opening the issue.\nI don't believe `Unknown().as_string()` is ever called regularly. AFAIK it's only used during inference. What should the string representation of an `Unknown` node be? So not sure this needs to be addressed.\nProbably just `'Unknown'`.\nIt's mostly only a problem when we do something like this:\n\n```python\ninferred = infer(node)\nif inferred is not Uninferable:\n    if inferred.as_string().contains(some_value):\n        ...\n```\nSo for the most part, as long as it doesn't crash we're good.",
        "base": "ce5cbce5ba11cdc2f8139ade66feea1e181a7944",
        "env": "0d1211558670cfefd95b39984b8d5f7f34837f32",
        "files": [
            "astroid/nodes/as_string.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/946",
        "problem": "Delayed attribute assignment to object() may cause incorrect inference of instance attributes\n@cdce8p: `aiohttp` and `VLCTelnet` turned out to be red herrings. This case fails on current stable versions:\r\n\r\n```python\r\nclass Example:\r\n    def prev(self):\r\n        pass\r\n    def next(self):\r\n        pass\r\n    def other(self):\r\n        pass\r\n\r\n\r\nex = Example()\r\nex.other()  # no warning\r\nex.prev()   # no warning\r\nex.next()   # no warning\r\n\r\nimport typing\r\n\r\nex.other()  # no warning\r\nex.prev()   # false-positive: not-callable\r\nex.next()   # false-positive: not-callable\r\n```\r\n\r\n_Originally posted by @nelfin in https://github.com/PyCQA/astroid/issues/927#issuecomment-818626368_\r\n\r\nI've bisected this down to 78d5537. Pylint 2.3.1 passes this case with 20a7ae5 and fails with 78d5537\n",
        "hint": "Minimal case:\r\n\r\n```python\r\nclass Example:\r\n    def func(self):\r\n        pass\r\n\r\n\r\nwhatthe = object()\r\nwhatthe.func = None\r\n\r\nex = Example()\r\nex.func()   # false-positive: not-callable\r\n```\nNot caused by 78d5537, just revealed by it. `typing` imported `collections`, `collections.OrderedDict` had an ambiguously inferred case that was previously broken by failure with positional-only arguments which was fixed in 78d5537.",
        "base": "2c109eec6fe3f972c6e8c637fe956431a0d7685c",
        "env": "d2ae3940a1a374253cc95d8fe83cb96f7a57e843",
        "files": [
            "astroid/builder.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/2015",
        "problem": "Replace modutils.is_standard_module() logic with sys.stdlib_module_names\n\r\nThis extends from the conversation in https://github.com/PyCQA/pylint/pull/8190.\r\n\r\nThe logic in `modutils.is_standard_module()` should largely be able to be replaced with [sys.stdlib_module_names](https://docs.python.org/3/library/sys.html#sys.stdlib_module_names), which was introduced in 3.10. The advantages are it will be faster (no imports, no filesystem traversal), it's not dependent on the local environment,  and it's maintained upstream, generated from source. For the referenced PR, I backported the generating code in CPython to generate sets for a shim to support 3.7 - 3.9.\r\n\r\nI started working on a PR for Astroid, but it seems `modutils.is_standard_module()` actually does two different things depending on how it's called.\r\n1. If no path is specified, it tries to determine if a module is part of the standard library (or a builtin, or compiled in) by inspecting the path of module after importing it.\r\n2. If a path is specified, it does the same logic, but ultimately is determining if the module is in the path specified.\r\n\r\nFor the second case, I could only find one case in the wild, in pyreverse.\r\n\r\nhttps://github.com/PyCQA/pylint/blob/5bc4cd9a4b4c240227a41786823a6f226864dc4b/pylint/pyreverse/inspector.py#L308\r\n\r\nThese seem like different behaviors to me. I'm unsure how to proceed with PR. Here are some options I've considered.\r\n\r\n- Option 1:\r\n  - Introduce a new function, basically a wrapper for sys.stdlib_module_names and the shim\r\n  - Old behavior is preserved\r\n  - Advantage of a function, even though it's very simple, is it provides a space to add overriding logic if needed down the road\r\n   \r\n- Option 2:\r\n   - Only introduce the shim, so the code is in a common place for Astroid and Pylint\r\n   - Can be dropped with 3.9\r\n   - Old behavior is preserved\r\n\r\n- Option 3:\r\n  - Fall back to old behavior if a path is given, check sys.stdlib_module_names otherwise\r\n\r\n- Option 4:\r\n  - Deprecate `is_standard_module()`\r\n  - Introduce new functions more specific to how they are used\r\n\r\n- Option 5:\r\n  - Do Nothing\r\n\r\nI'm sure there are more options, but this is what comes to mind now. Would appreciate your thoughts and ideas.\r\n\n",
        "hint": "Option 4 seems best to me. I would even be okay with completely removing the weird behaviour described in point 2 and adding a new function that `pyreverse` can use. Imo, if we have a clear changelog entry for this it shouldn't be considered a breaking change, it is very weird that that behaviour exists in the first place.\r\n\r\nPlease go ahead with the PR! Any work on `astroid` is always highly appreciated!",
        "base": "56a65daf1ba391cc85d1a32a8802cfd0c7b7b2ab",
        "env": "29b42e5e9745b172d5980511d14efeac745a5a82",
        "files": [
            "astroid/_backport_stdlib_names.py",
            "astroid/manager.py",
            "astroid/modutils.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1262",
        "problem": "``nodes.Module`` don't have a ``end_lineno`` and ``end_col_offset``\n### Steps to reproduce\r\n\r\n```python\r\nimport astroid\r\n\r\ncode = \"\"\"\r\n    print(\"a module\")\r\n    \"\"\"\r\n\r\nmodule = astroid.parse(code)\r\nprint(module.end_lineno)\r\nprint(module.end_col_offset)\r\n```\r\n\r\n### Current behavior\r\n\r\n`AttributeError` on both of the last lines.\r\n\r\n### Expected behavior\r\n\r\n@cdce8p Let me know if I misunderstood you, but I thought we wanted these to be accessible on all nodes, just initialised as `None`.\r\nIf that was not the case, I would make the case to do so as it allows you to do `node.end_lineno` without running in to `AttributeError`'s.\r\n\r\n### Version\r\n\r\nLatest `main`.\r\n\n",
        "hint": "",
        "base": "e840a7c54d3d8b5be2db1e66f34a5368c64fc3f7",
        "env": "0d1211558670cfefd95b39984b8d5f7f34837f32",
        "files": [
            "astroid/nodes/scoped_nodes.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1351",
        "problem": "Decorator.toline is off by 1\n### Steps to reproduce\r\n\r\nI came across this inconsistency while debugging why pylint reports `missing-docstring` on the wrong line for the `g2` function in the example. As it turns out, the `toline` of the decorator seems to point to `b=3,` instead of `)`.\r\n\r\n```python\r\nimport ast\r\nimport astroid\r\n\r\nsource = \"\"\"\\\r\n@f(a=2,\r\n   b=3,\r\n)\r\ndef g2():\r\n    pass\r\n\"\"\"\r\n\r\n[f] = ast.parse(source).body\r\n[deco] = f.decorator_list\r\nprint(\"ast\", deco.lineno, deco.end_lineno)\r\n\r\n[f] = astroid.parse(source).body\r\n[deco] = f.decorators.nodes\r\nprint(\"astroid\", deco.fromlineno, deco.tolineno)\r\n```\r\n\r\n### Current behavior\r\n\r\n```\r\nast 1 3\r\nastroid 1 2\r\n```\r\n\r\n### Expected behavior\r\n\r\n```\r\nast 1 3\r\nastroid 1 3\r\n```\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.9.3\n",
        "hint": "Actually, it seems a decorator is a red herring here, because I get the same off by one issue simply parsing a call\r\n\r\n```python\r\nsource = \"\"\"\\\r\nf(a=2,\r\n   b=3,\r\n)\r\n\"\"\"\r\n\r\n[call] = ast.parse(source).body\r\nprint(\"ast\", call.lineno, call.end_lineno)\r\n\r\n[call] = astroid.parse(source).body\r\nprint(\"astroid\", call.fromlineno, call.tolineno)\r\n```\r\n\r\nwhich outputs\r\n\r\n```\r\nast 1 3\r\nastroid 1 2\r\n```\nOkay, this seems to be caused by the implementation of `NodeNG.tolineno` which uses the last line of the *child* to approximate the last line of the parent:\r\n\r\nhttps://github.com/PyCQA/astroid/blob/03efcc3f86b88bab3080fe69119ee4c69e4afd0a/astroid/nodes/node_ng.py#L437-L446\r\n\r\nOnce possible fix is to override `tolineno` in `Call`. Wdyt?\n> this seems to be caused by the implementation of NodeNG.tolineno which uses the last line of the child to approximate the last line of the parent:\r\n\r\nNaive question, would it be possible to use the last line of the node instead, directly in NodeNG ?\nYeah, I think that should work with a caveat that the `ast` module only reports end line/column since Python 3.8. I'll draft a PR.\n@superbobry I was looking at `tolineno` recently. I was wondering if it would make sense to add a check for >= 3.8 and then just use the `end_lineno` attribute that was added recently. No need to reinvent the wheel on those versions.\r\n\r\nPerhaps that's a bit out of the scope of the PR you were going to draft, but it might help!",
        "base": "cfd9e74f7b4cbac08357cadec03c736501368afa",
        "env": "da745538c7236028a22cdf0405f6829fcf6886bc",
        "files": [
            "astroid/nodes/node_ng.py",
            "astroid/rebuilder.py"
        ]
    },
    {
        "pr": "pylint-dev/astroid/1959",
        "problem": "v2.13.x regression: Crash when inspecting `PyQt5.QtWidgets` due to `RuntimeError` during `hasattr`\n### Steps to reproduce\r\n\r\nInstall PyQt5, run `pylint --extension-pkg-whitelist=PyQt5 x.py` over a file containing `from PyQt5 import QtWidgets`\r\n\r\n### Current behavior\r\n\r\nWith astroid 2.12.13 and pylint 2.15.10, this works fine. With astroid 2.13.2, this happens:\r\n\r\n```pytb\r\nException on node <ImportFrom l.1 at 0x7fc5a3c47d00> in file '/home/florian/tmp/pylintbug/x.py'\r\nTraceback (most recent call last):\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\r\n    self._check_module_attrs(node, module, name.split(\".\"))\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\r\n    module = next(module.getattr(name)[0].infer())\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\r\n    result = [self.import_module(name, relative_only=True)]\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\r\n    return self.ast_from_module(named_module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\r\n    elif hasattr(member, \"__all__\"):\r\nRuntimeError: wrapped C/C++ object of type QApplication has been deleted\r\nx.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-11-06-17.txt'. (astroid-error)\r\n```\r\n\r\nIt looks like it happens when `member` is `QtWidgets.qApp`, which is a kind of \"magic\" object referring to the QApplication singleton. Since none exists, it looks like PyQt doesn't like trying to access an attribute on that.\r\n\r\nBisected to:\r\n\r\n- #1885 \r\n\r\nIt looks like 974f26f75eb3eccb4bcd8ea143901baf60a685ff is the exact culprit.\r\n\r\ncc @nickdrozd \r\n\r\n(took the freedom to add appropriate labels already, hope that's fine)\r\n\n",
        "hint": "Thank you for the investigation @The-Compiler ! I'm going to revert the offending commit seeing it's only a cleanup. Let's do something more elegant later on if required.\nThe commit you linked is from a fork, so I tried to reproduce locally to bisect on pylint main repo but I cannot\r\n```\r\npip3 install PyQt5\r\nCollecting PyQt5\r\n Collecting PyQt5-sip<13,>=12.11\r\nCollecting PyQt5-Qt5>=5.15.0\r\nInstalling collected packages: PyQt5-Qt5, PyQt5-sip, PyQt5\r\nSuccessfully installed PyQt5-5.15.7 PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.0\r\n(venv) fix-crash-regression-2.13.2: pylint --extension-pkg-whitelist=PyQt5 x.py \r\n************* Module x\r\nx.py:1:0: W0611: Unused QtWidgets imported from PyQt5 (unused-import)\r\n---------------------------------------------------------------------\r\n\r\nYour code has been rated at 0.00/10 (previous run: -50.00/10, +50.00)\r\n```\r\nCould you provide your other dependencies maybe ?\r\n\n> The commit you linked is from a fork, so I tried to reproduce locally to bisect on pylint main repo but I cannot\r\n\r\nThe main astroid repo bisects to the merge of #1885 (f26dbe419ac15a87ed65e9b55ed15d3d8100b608) - that was a squash merge, 974f26f75eb3eccb4bcd8ea143901baf60a685ff is my guess at what in the PR is the problem.\r\n\r\n> Could you provide your other dependencies maybe ?\r\n\r\nNothing else really:\r\n\r\n```\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ python3 --version\r\nPython 3.10.8\r\n\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ python3 -m venv .venv\r\npython3 -m venv .venv  2.87s user 0.26s system 98% cpu 3.170 total\r\n\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ .venv/bin/pip install pylint PyQt5\r\nCollecting pylint\r\n  Using cached pylint-2.15.10-py3-none-any.whl (509 kB)\r\nCollecting PyQt5\r\n  Using cached PyQt5-5.15.7-cp37-abi3-manylinux1_x86_64.whl (8.4 MB)\r\nCollecting tomli>=1.1.0\r\n  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\r\nCollecting tomlkit>=0.10.1\r\n  Using cached tomlkit-0.11.6-py3-none-any.whl (35 kB)\r\nCollecting platformdirs>=2.2.0\r\n  Using cached platformdirs-2.6.2-py3-none-any.whl (14 kB)\r\nCollecting mccabe<0.8,>=0.6\r\n  Using cached mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\r\nCollecting dill>=0.2\r\n  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\r\nCollecting astroid<=2.14.0-dev0,>=2.12.13\r\n  Using cached astroid-2.13.2-py3-none-any.whl (272 kB)\r\nCollecting isort<6,>=4.2.5\r\n  Using cached isort-5.11.4-py3-none-any.whl (104 kB)\r\nCollecting PyQt5-Qt5>=5.15.0\r\n  Using cached PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\r\nCollecting PyQt5-sip<13,>=12.11\r\n  Using cached PyQt5_sip-12.11.0-cp310-cp310-manylinux1_x86_64.whl (359 kB)\r\nCollecting lazy-object-proxy>=1.4.0\r\n  Using cached lazy_object_proxy-1.9.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63 kB)\r\nCollecting wrapt<2,>=1.11\r\n  Using cached wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\r\nCollecting typing-extensions>=4.0.0\r\n  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\r\nInstalling collected packages: PyQt5-Qt5, wrapt, typing-extensions, tomlkit, tomli, PyQt5-sip, platformdirs, mccabe, lazy-object-proxy, isort, dill, PyQt5, astroid, pylint\r\nSuccessfully installed PyQt5-5.15.7 PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.0 astroid-2.13.2 dill-0.3.6 isort-5.11.4 lazy-object-proxy-1.9.0 mccabe-0.7.0 platformdirs-2.6.2 pylint-2.15.10 tomli-2.0.1 tomlkit-0.11.6 typing-extensions-4.4.0 wrapt-1.14.1\r\n\r\n[notice] A new release of pip available: 22.2.2 -> 22.3.1\r\n[notice] To update, run: python3 -m pip install --upgrade pip\r\n.venv/bin/pip install pylint PyQt5  3.18s user 0.57s system 74% cpu 5.004 total\r\n\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ cat x.py\r\nfrom PyQt5 import QtWidgets\r\n\r\n\u2500[florian@aragog]\u2500\u2500[~/tmp/pylintbug]\u2500\u2500[23-01-10]\u2500\u2500[12:35]\u2500\u2500\u2500\u2500\u2504\r\n$ .venv/bin/pylint --extension-pkg-whitelist=PyQt5 x.py \r\n************* Module x\r\nx.py:1:0: C0114: Missing module docstring (missing-module-docstring)\r\nException on node <ImportFrom l.1 at 0x7fa1f6e37d90> in file '/home/florian/tmp/pylintbug/x.py'\r\nTraceback (most recent call last):\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\r\n    self._check_module_attrs(node, module, name.split(\".\"))\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\r\n    module = next(module.getattr(name)[0].infer())\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\r\n    result = [self.import_module(name, relative_only=True)]\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\r\n    return self.ast_from_module(named_module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\r\n    elif hasattr(member, \"__all__\"):\r\nRuntimeError: wrapped C/C++ object of type QApplication has been deleted\r\nx.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-12-36-04.txt'. (astroid-error)\r\n\r\n------------------------------------------------------------------\r\nYour code has been rated at 0.00/10 (previous run: 0.00/10, +0.00)\r\n```\r\n",
        "base": "49691cc04f2d38b174787280f7ed38f818c828bd",
        "env": "0c9ab0fe56703fa83c73e514a1020d398d23fa7f",
        "files": [
            "astroid/raw_building.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4853",
        "problem": "Confusing behaviour of ParametricEllipsoid\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nWhen creating a ParametricEllispoid using a direction of [0, 1, 0], the ellipsoid is rotated along the y axis.  \r\nFor example if setting the direction to [1e-5, 1, 0], which corresponds to approximately similar direction, the ellipsoid displays then the correct behaviour.\r\n\r\n### Steps to reproduce the bug.\r\n\r\n```python\r\nimport pyvista as pv\r\n\r\nellipsoid = pv.ParametricEllipsoid(300, 100, 10, direction=[0, 1, 0])\r\n```\r\n\r\n### System Information\r\n\r\n```shell\r\nDate: Wed Sep 06 14:07:38 2023 CEST\r\n\r\n                OS : Linux\r\n            CPU(s) : 8\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n               RAM : 31.2 GiB\r\n       Environment : Jupyter\r\n       File system : ext4\r\n        GPU Vendor : Intel\r\n      GPU Renderer : Mesa Intel(R) UHD Graphics (CML GT2)\r\n       GPU Version : 4.6 (Core Profile) Mesa 22.0.1\r\n  MathText Support : False\r\n\r\n  Python 3.8.13 (default, Apr 19 2022, 02:32:06)  [GCC 11.2.0]\r\n\r\n           pyvista : 0.41.1\r\n               vtk : 9.2.6\r\n             numpy : 1.24.4\r\n        matplotlib : 3.3.4\r\n            scooby : 0.7.2\r\n             pooch : v1.7.0\r\n           imageio : 2.31.1\r\n           IPython : 8.12.2\r\n        ipywidgets : 8.0.7\r\n             scipy : 1.10.1\r\n              tqdm : 4.65.0\r\n        jupyterlab : 3.6.5\r\n             trame : 2.5.2\r\n      trame_client : 2.10.0\r\n      trame_server : 2.11.7\r\n         trame_vtk : 2.5.8\r\n      nest_asyncio : 1.5.6\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\nHere is the given ellipsoid\r\n![confusing_ellipsoid](https://github.com/pyvista/pyvista/assets/57682091/f0e1b5f7-eca1-4224-a020-df44385ed68b)\r\nHere is what is expected\r\n![expected_ellipsoid](https://github.com/pyvista/pyvista/assets/57682091/d4f67ead-9928-4af3-9c3a-b6121180b780)\r\n\n",
        "hint": "",
        "base": "4a44e4c63c6b8d6a3f1db0aa193f4ccb631ed698",
        "env": "17ed0eb49a942b297e61a83a1c8ba828c5922b99",
        "files": [
            "pyvista/core/utilities/geometric_objects.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4648",
        "problem": "Clean up and clarify sampling-like filters\n### Describe what maintenance you would like added.\n\nThere was a discussion on slack on the use of sampling-like filters, i.e. `sample`, `probe`, and `interpolate`.  One issue is that it is hard to figure out when to use which filter.  The other issue is that `probe` has the opposite behavior of `sample` and `interpolate` in regards to order of operation (see below).\n\n### Links to source code.\n\n_No response_\n\n### Pseudocode or Screenshots\n\n```python\r\nimport pyvista as pv\r\n\r\nsmall = pv.ImageData(dimensions=(5, 5, 5))\r\nlarge = pv.ImageData(dimensions=(10, 10, 10))\r\nprint(small.n_points)\r\nprint(large.n_points)\r\nprint(small.probe(large).n_points)  # gives different result\r\nprint(small.sample(large).n_points)\r\nprint(small.interpolate(large).n_points)\r\n```\r\n\r\n\r\nThis  gives\r\n\r\n```txt\r\n125\r\n1000\r\n1000\r\n125\r\n125\r\n```\n",
        "hint": "`probe` uses [vtkProbeFilter](https://vtk.org/doc/nightly/html/classvtkProbeFilter.html).  Note that `vtkCompositeDataProbeFilter` is a subclass, and adds the ability to use Composite Data as in input (only one of the two slots in the algorithm).\r\n\r\n`sample` uses [vtkResampleWithDataSet](https://vtk.org/doc/nightly/html/classvtkResampleWithDataSet.html#details).  This uses `vtkCompositeDataProbeFilter` under the hood, but also allows composite data to be used in both the source and the input.\r\n\r\nSo I propose that we deprecate `probe` and only keep `sample` and `interpolate`.\r\n\r\n`imterpolate` is separate from the other two since it uses a different sampling/interpolation method. \nThe pyvista standard, at least in my experience, is that we should generally expect the shape of the mesh to be equal to the mesh on which the filter attribute is called.  That is, `mesh1.filter(mesh2)` should return a mesh closer to `mesh1`.  This also enables `inplace=True` usage when possible.\r\n\r\nSo, if we were to keep `probe` we should switch the order of operation, and the deprecation/breaking change would have to be done carefully.  I still think it should be removed entirely instead as above, but wanted to lay out other options.\n> So, if we were to keep `probe` we should switch the order of operation, and the deprecation/breaking change would have to be done carefully. I still think it should be removed entirely instead as above, but wanted to lay out other options.\r\n\r\nFrom a design standpoint, we'd probably have to do the deprecation the same way: deprecating the old method and introducing a new one that has the right semantics. Switching the input and output is not the kind of change we should subject downstream to.\n+1 to this, I've definitely been confused by it before. \r\n\r\n> The pyvista standard, at least in my experience, is that we should generally expect the shape of the mesh to be equal to the mesh on which the filter attribute is called. \r\n\r\n+1 again\r\n\r\n> From a design standpoint, we'd probably have to do the deprecation the same way: deprecating the old method and introducing a new one that has the right semantics. Switching the input and output is not the kind of change we should subject downstream to.\r\n\r\ndeprecate `probe` and introduce `eborp` \ud83d\ude09 \r\n\n+1 for `eborp`",
        "base": "d804a93a3bcae250c74a9f0f7c37fbc8bf002011",
        "env": "d804a93a3bcae250c74a9f0f7c37fbc8bf002011",
        "files": [
            "examples/01-filter/interpolate.py",
            "examples/01-filter/resample.py",
            "pyvista/core/filters/data_set.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4808",
        "problem": "Boolean Operation freezes/crashes \n### Describe the bug, what's wrong, and what you expected.\n\nApparently, if two polyData have the exact same shape, their boolean operation freezes/crashes the application!\r\n\n\n### Steps to reproduce the bug.\n\n```python\r\np1 = pv.Sphere().triangulate()\r\np2 = pv.Sphere().triangulate()\r\n\r\np1.boolean_intersection(p2)\r\n``````\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Tue Aug 22 12:17:01 2023 EEST\r\n\r\n                OS : Darwin\r\n            CPU(s) : 12\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n               RAM : 16.0 GiB\r\n       Environment : Jupyter\r\n       File system : apfs\r\n        GPU Vendor : ATI Technologies Inc.\r\n      GPU Renderer : AMD Radeon Pro 5300M OpenGL Engine\r\n       GPU Version : 4.1 ATI-4.14.1\r\n  MathText Support : False\r\n\r\n  Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0\r\n  (clang-1300.0.29.30)]\r\n\r\n           pyvista : 0.41.1\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n        matplotlib : 3.7.1\r\n            scooby : 0.7.2\r\n             pooch : v1.7.0\r\n           IPython : 8.14.0\r\n             scipy : 1.10.1\r\n        jupyterlab : 4.0.5\r\n      nest_asyncio : 1.5.7\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n_No response_\n",
        "hint": "",
        "base": "d0c2e12cac39af1872b00907f9526dbfb59ec69e",
        "env": "e58d53d0ec16a16121c84912867dae6199b034a9",
        "files": [
            "pyvista/core/filters/poly_data.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4311",
        "problem": "Allow passing through cell data in `to_tetrahedra` method in RectilinearGrid\n### Describe the feature you would like to be added.\n\nNo cell data is passed through when converting to a tetrahedra.  The user can currently request to pass through the original cell id, but it requires one more step to regenerate the cell data on the tetrahedralized mesh.\n\n### Links to VTK Documentation, Examples, or Class Definitions.\n\n_No response_\n\n### Pseudocode or Screenshots\n\nCurrently we have to do\r\n\r\n```python\r\nmesh # Rectilinear or UniformGrid, which has cell data \"cell_data\"\r\ntetra_mesh = mesh.to_tetrahedra(pass_cell_ids=True)\r\ntetra_mesh[\"cell_data\"] = mesh[\"cell_data\"][tetra_mesh.cell_data.active_scalars]\r\n```\r\n\r\nIt would be better to do something like\r\n\r\n```python\r\nmesh # Rectilinear or UniformGrid, which has cell data \"cell_data\"\r\ntetra_mesh = mesh.to_tetrahedra(pass_cell_data=True)  # the prior code would occur inside the method\r\n```\n",
        "hint": "",
        "base": "db6ee8dd4a747b8864caae36c5d05883976a3ae5",
        "env": "4c2d1aed10b1600d520271beba8579c71433e808",
        "files": [
            "pyvista/core/datasetattributes.py",
            "pyvista/core/filters/rectilinear_grid.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4414",
        "problem": "Adding ``CircularArc``s together does not provide a line\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nDon't know if it can be considered a bug or not but...\r\n\r\nIf you define two consecutive ``pv.CircularArc`` and you plot them, weird things start to appear with the new PyVista 0.39 version. Run the following code snippet using ``pyvista==0.38.6`` and ``pyvista==0.39.0``\r\n\r\n### Steps to reproduce the bug.\r\n\r\n```python\r\nimport pyvista as pv\r\n\r\n\r\n# Define your arcs\r\n#\r\n#              Y       (s2)\r\n#              ^   ____(e1)____\r\n#              |  /             \\\r\n#              | /               \\\r\n#              |/                 \\\r\n#          (s1)O ------(c)-------(e2)----> X\r\n#\r\n#   Let's imagine the above is an arc from (0,0) to (10,10) and origin\r\n#   at (10,0); and another consecutive arc from (10,10) to (20,0) and\r\n#   origin at (10,0)\r\n#\r\narc_1 = pv.CircularArc([0, 0, 0], [10, 10, 0], [10, 0, 0], negative=False)\r\narc_2 = pv.CircularArc([10, 10, 0], [20, 0, 0], [10, 0, 0], negative=False)\r\n\r\n# ========== CRITICAL BEHAVIOR ==========\r\n# I add them together\r\narc = arc_1 + arc_2\r\n# ========== CRITICAL BEHAVIOR ==========\r\n\r\n# Instantiate plotter\r\npl = pv.Plotter()\r\n\r\n# Add the polydata\r\npl.add_mesh(arc)\r\n\r\n# Plotter config: view from the top\r\npl.view_vector(vector=[0, 0, 1], viewup=[0, 1, 0])\r\n\r\n# Plot\r\npl.show()\r\n\r\n```\r\n\r\n### System Information\r\n\r\n```shell\r\nFor PyVista 0.38.6\r\n\r\n--------------------------------------------------------------------------------\r\n  Date: Thu May 11 13:49:09 2023 Romance Daylight Time\r\n\r\n                OS : Windows\r\n            CPU(s) : 16\r\n           Machine : AMD64\r\n      Architecture : 64bit\r\n       Environment : Python\r\n        GPU Vendor : Intel\r\n      GPU Renderer : Intel(R) UHD Graphics\r\n       GPU Version : 4.5.0 - Build 30.0.100.9955\r\n\r\n  Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64\r\n  bit (AMD64)]\r\n\r\n           pyvista : 0.38.6\r\n               vtk : 9.2.6\r\n             numpy : 1.24.3\r\n           imageio : 2.28.1\r\n            scooby : 0.7.2\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n--------------------------------------------------------------------------------\r\n\r\nFor PyVista 0.39.0\r\n\r\n--------------------------------------------------------------------------------\r\n  Date: Thu May 11 13:50:00 2023 Romance Daylight Time\r\n\r\n                OS : Windows\r\n            CPU(s) : 16\r\n           Machine : AMD64\r\n      Architecture : 64bit\r\n       Environment : Python\r\n        GPU Vendor : Intel\r\n      GPU Renderer : Intel(R) UHD Graphics\r\n       GPU Version : 4.5.0 - Build 30.0.100.9955\r\n  MathText Support : False\r\n\r\n  Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64\r\n  bit (AMD64)]\r\n\r\n           pyvista : 0.39.0\r\n               vtk : 9.2.6\r\n             numpy : 1.24.3\r\n        matplotlib : 3.7.1\r\n            scooby : 0.7.2\r\n             pooch : v1.7.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\nPyVista 0.39\r\n\r\n![PyVista 0.39](https://github.com/pyvista/pyvista/assets/37798125/87bda0a2-2eb7-4171-8005-239f5a4f27c2)\r\n\r\nPyVista 0.38.6\r\n\r\n![PyVista 0.38.6](https://github.com/pyvista/pyvista/assets/37798125/b6159e12-97bc-4768-9691-91005fadfb26)\r\n\n",
        "hint": "IMO, we shouldn't be seeing a surface, but two consecutive lines as it is shown in PyVista 0.38.6.",
        "base": "438dd0d6ea2ebee73ecccdba878837af8fd83d7d",
        "env": "19938bf14e7705ed1e7d4c9aa6558d90df6de9d1",
        "files": [
            "pyvista/core/filters/poly_data.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4417",
        "problem": "``Multiblock``.plot does not work when using ``PointSet``\n### Describe the bug, what's wrong, and what you expected.\n\nIt seems ``MultiBlock`` entities made of ``PointSet`` plot nothing when using ``plot`` method.\n\n### Steps to reproduce the bug.\n\n```python\r\nimport pyvista as pv\r\nimport numpy as np\r\n\r\npoints_arr = np.array(\r\n    [\r\n        [0.0, 1.0, 0.0],\r\n        [0.0, 0.0, 0.0],\r\n        [1.0, 1.0, 0.0],\r\n        [1.0, 0.0, 0.0],\r\n        [0.0, 0.0, 1.0],\r\n        [1.0, 0.0, 1.0],\r\n        [1.0, 1.0, 1.0],\r\n        [0.0, 1.0, 1.0],\r\n    ]\r\n)\r\n\r\npoints = pv.MultiBlock()\r\nfor each_kp in points_arr:\r\n    points.append(pv.PointSet(each_kp))\r\n\r\npoints.plot()\r\n```\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Wed May 10 18:07:18 2023 CEST\r\n\r\n                OS : Darwin\r\n            CPU(s) : 8\r\n           Machine : arm64\r\n      Architecture : 64bit\r\n               RAM : 16.0 GiB\r\n       Environment : IPython\r\n       File system : apfs\r\n        GPU Vendor : Apple\r\n      GPU Renderer : Apple M2\r\n       GPU Version : 4.1 Metal - 83.1\r\n  MathText Support : False\r\n\r\n  Python 3.11.1 (main, Dec 23 2022, 09:28:24) [Clang 14.0.0\r\n  (clang-1400.0.29.202)]\r\n\r\n           pyvista : 0.39.0\r\n               vtk : 9.2.6\r\n             numpy : 1.24.3\r\n        matplotlib : 3.7.1\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n           imageio : 2.28.0\r\n           IPython : 8.12.1\r\n        ipywidgets : 8.0.6\r\n             scipy : 1.10.1\r\n              tqdm : 4.65.0\r\n        jupyterlab : 3.6.3\r\n         pythreejs : 2.4.2\r\n      nest_asyncio : 1.5.6\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n<img width=\"624\" alt=\"image\" src=\"https://github.com/pyvista/pyvista/assets/28149841/a1b0999f-2d35-4911-a216-eb6503955860\">\r\n\n",
        "hint": "I can reproduce this problem. We'll add it for the v0.39.1 milestone.\nThe trick is this bit of code:\r\n\r\nhttps://github.com/pyvista/pyvista/blob/a8921b94b91a7d9809c9b5ac2ef9c981b5f71ea1/pyvista/plotting/plotting.py#L3218-L3224\r\n\r\nwhich isn't used for `MultiBlock` plotting because `add_mesh` forwards to `add_composite()`\r\n\r\nhttps://github.com/pyvista/pyvista/blob/a8921b94b91a7d9809c9b5ac2ef9c981b5f71ea1/pyvista/plotting/plotting.py#L3230\r\n\r\nBut then I realized this block should handle it\r\n\r\nhttps://github.com/pyvista/pyvista/blob/a8921b94b91a7d9809c9b5ac2ef9c981b5f71ea1/pyvista/plotting/plotting.py#L2544\r\n\r\n\r\nso maybe there's a bug in that method or the copy isn't propagating? Not sure...\r\n\r\n",
        "base": "a8921b94b91a7d9809c9b5ac2ef9c981b5f71ea1",
        "env": "19938bf14e7705ed1e7d4c9aa6558d90df6de9d1",
        "files": [
            "pyvista/core/composite.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4406",
        "problem": "to_tetrahedra active scalars\n### Describe the bug, what's wrong, and what you expected.\n\n#4311 passes cell data through the `to_tetrahedra` call. However, after these changes.  The active scalars information is lost.\r\n\r\ncc @akaszynski who implemented these changes in that PR.\n\n### Steps to reproduce the bug.\n\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\nmesh = pv.UniformGrid(dimensions=(10, 10, 10))\r\nmesh[\"a\"] = np.zeros(mesh.n_cells)\r\nmesh[\"b\"] = np.ones(mesh.n_cells)\r\nprint(mesh.cell_data)\r\ntet = mesh.to_tetrahedra()\r\nprint(tet.cell_data)\r\n```\r\n\r\n```txt\r\npyvista DataSetAttributes\r\nAssociation     : CELL\r\nActive Scalars  : a\r\nActive Vectors  : None\r\nActive Texture  : None\r\nActive Normals  : None\r\nContains arrays :\r\n    a                       float64    (729,)               SCALARS\r\n    b                       float64    (729,)\r\npyvista DataSetAttributes\r\nAssociation     : CELL\r\nActive Scalars  : None\r\nActive Vectors  : None\r\nActive Texture  : None\r\nActive Normals  : None\r\nContains arrays :\r\n    a                       float64    (3645,)\r\n    b                       float64    (3645,)\r\n    vtkOriginalCellIds      int32      (3645,)\r\n```\n\n### System Information\n\n```shell\nPython 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.39.0\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n        matplotlib : 3.7.1\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n           imageio : 2.27.0\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n_No response_\n",
        "hint": "First, these lines should reset the active_scalars to the last scalars added to the tetrahedral mesh:\r\n\r\nhttps://github.com/pyvista/pyvista/blob/a87cf37d9cea6fc68d8099a56d86ded8f6e78734/pyvista/core/filters/rectilinear_grid.py#L119-L123\r\n\r\nThese subsequent lines pop out the active scalars, which is somehow the blank scalars, and set it as \"vtkOriginalCellIds\".\r\n\r\nhttps://github.com/pyvista/pyvista/blob/a87cf37d9cea6fc68d8099a56d86ded8f6e78734/pyvista/core/filters/rectilinear_grid.py#L125-L129\r\n\r\nEdit: I am wrong the first set of lines, would only set active scalars if there are none present, but the unnamed scalars are active.  We just need to reset the active scalars according to the original mesh I think.\nGood point. Thanks @MatthewFlamm for pointing this out. We'll have this added in the v0.39.1 patch.",
        "base": "461bb5ed5be8c9dda6e9ed765fab3224149d9c62",
        "env": "19938bf14e7705ed1e7d4c9aa6558d90df6de9d1",
        "files": [
            "pyvista/core/filters/rectilinear_grid.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4226",
        "problem": "Diffuse and Specular setters silently ignore invalid values\n### Describe the bug, what's wrong, and what you expected.\n\nWhile working on #3870, I noticed that `diffuse` and `specular` do not always get set on `pyvista.Property`. This happens if an invalid value is used. For example, diffuse should be between 0-1, but if you pass a value of 2.0, `vtkProperty` corrects it to 1.0:\r\n\r\n```py\r\n>>> import vtk\r\n>>> prop = vtk.vtkProperty()\r\n>>> prop.SetDiffuse(2.0)\r\n>>> prop.GetDiffuse()\r\n1.0\r\n```\r\n\r\nThis similarly happens for specular, which should also have a valid range of 0-1.\r\n\r\nShould we have `pyvista.Property`'s setters for these methods error out when an invalid value is passed? I ask because I definitely wasted time trying to figure out why a diffuse value of 1.0 looks the same as 2.0 before thinking it should be between 0 and 1.\r\n\r\nPerhaps this at a minimum should be documented in the setters and docstring for `add_mesh()`?\n\n### Steps to reproduce the bug.\n\n```python\r\nimport pyvista as pv\r\n\r\npl = pv.Plotter()\r\na = pl.add_mesh(pv.Sphere(), diffuse=3.0, specular=10)\r\n# Expected to error for invalid values\r\n```\n\n### System Information\n\n```shell\nmain branch\n```\n\n\n### Screenshots\n\n_No response_\n",
        "hint": "Something similar came up [on this PR](https://github.com/pyvista/pyvista/pull/1040#issuecomment-739850576). To quote that comment of mine:\r\n>  If the user sets nonsense data we should either raise, or pass it on to vtk which probably clamps internally (I haven't tested yet).\r\n\r\n(later it was discussed that yes, VTK clamps values internally). I'm not sure we came back to this point later during the PR, but anyway we ended up letting VTK do its thing, whatever that was.\r\n\r\nI'm still not sure that checking and raising makes most sense, but I agree that at least documenting these values would be nice. We apparently have a handful of these documented, but not the others. Then again if we find it to be worth specifying in the docstring, we might as well make it a hard check on our side.",
        "base": "1f28aadcb39b5be2f955feae9d462eadb0dfc5c1",
        "env": "4c2d1aed10b1600d520271beba8579c71433e808",
        "files": [
            "pyvista/plotting/_property.py",
            "pyvista/plotting/composite_mapper.py",
            "pyvista/themes.py",
            "pyvista/utilities/misc.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/3750",
        "problem": "Unexpected threshold behavior\n### Describe the bug, what's wrong, and what you expected.\n\nI'm using simple structed grids of cells, and need to filter-out some \"nodata\" cells. To do this, I'm setting scalar values to the cell data, then using [threshold](https://docs.pyvista.org/api/core/_autosummary/pyvista.DataSetFilters.threshold.html) with the nodata value with `invert=True`. However, I'm getting confusing and inconsistent results compared to ParaView.\n\n### Steps to reproduce the bug.\n\n```python\r\nimport numpy as np\r\nimport pyvista\r\n\r\nx = np.arange(5, dtype=float)\r\ny = np.arange(6, dtype=float)\r\nz = np.arange(2, dtype=float)\r\nxx, yy, zz = np.meshgrid(x, y, z)\r\nmesh = pyvista.StructuredGrid(xx, yy, zz)\r\nmesh.cell_data.set_scalars(np.repeat(range(5), 4))\r\n\r\n# All data\r\nmesh.plot(show_edges=True)\r\n# output is normal\r\n\r\n# Filtering out nodata (zero) values\r\nmesh.threshold(0, invert=True).plot(show_edges=True)\r\n# output does not look normal, only 0-value cells are shown\r\n```\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Thu Nov 17 15:23:57 2022 New Zealand Daylight Time\r\n\r\n                OS : Windows\r\n            CPU(s) : 12\r\n           Machine : AMD64\r\n      Architecture : 64bit\r\n               RAM : 31.7 GiB\r\n       Environment : IPython\r\n        GPU Vendor : NVIDIA Corporation\r\n      GPU Renderer : NVIDIA RTX A4000/PCIe/SSE2\r\n       GPU Version : 4.5.0 NVIDIA 472.39\r\n\r\n  Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC\r\n  v.1929 64 bit (AMD64)]\r\n\r\n           pyvista : 0.37.0\r\n               vtk : 9.1.0\r\n             numpy : 1.22.3\r\n           imageio : 2.22.0\r\n            scooby : 0.7.0\r\n             pooch : v1.6.0\r\n        matplotlib : 3.6.2\r\n             PyQt5 : 5.12.3\r\n           IPython : 8.6.0\r\n          colorcet : 3.0.1\r\n             scipy : 1.8.0\r\n              tqdm : 4.63.0\r\n            meshio : 5.3.4\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\nNormal looking whole grid:\r\n![image](https://user-images.githubusercontent.com/895458/202339692-5046b23f-c3c8-4b2c-aaa7-4aa06afbae9f.png)\r\n\r\nOdd-looking threshold attempt with pyvista, showing only 0-values:\r\n![image](https://user-images.githubusercontent.com/895458/202339879-b2270e4c-a71b-4d43-86f8-4f67445b7b69.png)\r\n\r\nExpected result with ParaView theshold filter with upper/lower set to 0 and invert selected:\r\n![image](https://user-images.githubusercontent.com/895458/202340379-fea26838-b0f4-4828-b510-825f53522e87.png)\r\n\r\nApologies for any \"user error\", as I'm new to this package.\n",
        "hint": "Can confirm, and I am seeing quite a few inconsistent results with the threshold filter. So this is not a user error!\r\n\r\n For example `inverter=True/False` should produce two logical inverses for this mesh, but it doesn't:\r\n<img width=\"624\" alt=\"Screen Shot 2022-11-16 at 11 18 48 PM\" src=\"https://user-images.githubusercontent.com/22067021/202371190-7dcd64df-1882-4876-b4c3-43fe614913a6.png\">\r\n\r\n\r\nSecond, depending on if `value` is a single value `0` or a range `[0, 0]` yields completely different results from the PyVista filter (but not in ParaView):\r\n\r\n\r\n```py\r\np = pv.Plotter(notebook=0, shape=(1,2))\r\np.add_mesh(mesh.threshold(0, invert=False))\r\np.subplot(0,1)\r\np.add_mesh(mesh.threshold(0, invert=True))\r\np.link_views()\r\np.view_isometric()\r\np.show()\r\n```\r\n\r\n<img width=\"624\" alt=\"Screen Shot 2022-11-16 at 11 20 01 PM\" src=\"https://user-images.githubusercontent.com/22067021/202371391-393280c1-1091-4c61-82b2-e95e76b49327.png\">\r\n\r\n\r\nvs.\r\n\r\n```py\r\np = pv.Plotter(notebook=0, shape=(1,2))\r\np.add_mesh(mesh.threshold([0, 0], invert=False))\r\np.subplot(0,1)\r\np.add_mesh(mesh.threshold([0, 0], invert=True))\r\np.link_views()\r\np.view_isometric()\r\np.show()\r\n```\r\n\r\n<img width=\"624\" alt=\"Screen Shot 2022-11-16 at 11 20 34 PM\" src=\"https://user-images.githubusercontent.com/22067021/202371476-cfd0fabb-daad-47db-acc5-855b43504f21.png\">\r\n\r\n\r\nThis is not good... I'll start digging into this and see if I can fix the `threshold` filter such that I has consistency with itself and with ParaView\r\n\nThanks for taking a closer look. I should have mentioned that I get the same behavior on linux using a similar conda-forge setup.\r\n\r\nI've found that ranges like `[1, 1]` work as expected to filter on values == 1, but to filter on zero, it needs to span a very small range:\r\n```python\r\np = pv.Plotter(notebook=0, shape=(1,2))\r\np.add_mesh(mesh.threshold([-1e-30, 1e-30], invert=False))\r\np.subplot(0,1)\r\np.add_mesh(mesh.threshold([-1e-30, 1e-30], invert=True))\r\np.link_views()\r\np.view_isometric()\r\np.show()\r\n```\r\nwhich also logs this message three times:\r\n> 2022-11-17 22:30:30.021 ( 559.089s) [        20F0C740]       vtkThreshold.cxx:96    WARN| vtkThreshold::ThresholdBetween was deprecated for VTK 9.1 and will be removed in a future version.\r\n\nAnother solution is to use [`extract_cells`](https://docs.pyvista.org/api/core/_autosummary/pyvista.StructuredGrid.extract_cells.html):\r\n```python\r\nmesh.extract_cells(mesh.cell_data.active_scalars != 0).plot()\r\nmesh.extract_cells(mesh.cell_data.active_scalars != 1).plot()\r\n```",
        "base": "3db44d72070ccd6e8fc5c7f708500424011bcc47",
        "env": "8dd8eeb80248cea4360c753847bd622e8652e059",
        "files": [
            "pyvista/core/filters/data_set.py",
            "pyvista/plotting/widgets.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/3747",
        "problem": "`bounds` property return type is mutable from `MultiBlock`\nThe `bounds` property has a different return type for meshes and `MultiBlock` objects:\r\n\r\n```\r\n>>> import pyvista as pv\r\n>>> slices = pv.Sphere().slice_orthogonal()\r\n# MultiBlock returns list (mutable)\r\n>>> slices.bounds\r\n[-0.49926671385765076, 0.49926671385765076, -0.4965316653251648, 0.4965316653251648, -0.5, 0.5]\r\n# Mesh returns tuple (immutable)\r\n>>> slices[0].bounds\r\n(-6.162975822039155e-33, 0.0, -0.4965316653251648, 0.4965316653251648, -0.5, 0.5)\r\n```\r\n\r\nIMO, the return value should be immutable and the `bounds` property should  be cast to a tuple before returning.\n",
        "hint": "@banesullivan Possibly related to #3180 .",
        "base": "0f800862380f60efb1841e5e0b116e945a3079a9",
        "env": "8dd8eeb80248cea4360c753847bd622e8652e059",
        "files": [
            "pyvista/core/composite.py",
            "pyvista/plotting/actor.py",
            "pyvista/plotting/plotting.py",
            "pyvista/plotting/renderer.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4329",
        "problem": "PolyData faces array is not updatable in-place and has unexpected behavior\n### Describe the bug, what's wrong, and what you expected.\n\nWhen accessing `PolyData.faces` (and likely other cell data), we cannot update the array in place. Further, there is some unexpected behavior where accessing `PolyData.faces` will override existing, modified views of the array.\n\n### Steps to reproduce the bug.\n\n```python \r\n>>> import pyvista as pv\r\n>>> mesh = pv.Sphere()\r\n>>> f = mesh.faces\r\n>>> f\r\narray([  3,   2,  30, ..., 840,  29,  28])\r\n>>> a = f[1:4]\r\n>>> a\r\narray([ 2, 30,  0])\r\n>>> b = f[5:8]\r\n>>> b\r\narray([30, 58,  0])\r\n>>> f[1:4] = b\r\n>>> f[5:8] = a\r\n>>> f\r\narray([  3,  30,  58, ..., 840,  29,  28])\r\n>>> assert all(f[1:4] == b) and all(f[5:8] == a)\r\n>>> mesh.faces  # access overwrites `f` in place which is unexpected and causes the check above to now fail\r\n>>> assert all(f[1:4] == b) and all(f[5:8] == a)\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-82-08205e08097f> in <cell line: 13>()\r\n     11 assert all(f[1:4] == b) and all(f[5:8] == a)\r\n     12 mesh.faces  # access overwrites `f` in place\r\n---> 13 assert all(f[1:4] == b) and all(f[5:8] == a)\r\n\r\nAssertionError: \r\n ```\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Thu May 26 11:45:54 2022 MDT\r\n\r\n                OS : Darwin\r\n            CPU(s) : 16\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n               RAM : 64.0 GiB\r\n       Environment : Jupyter\r\n       File system : apfs\r\n        GPU Vendor : ATI Technologies Inc.\r\n      GPU Renderer : AMD Radeon Pro 5500M OpenGL Engine\r\n       GPU Version : 4.1 ATI-4.8.13\r\n\r\n  Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:12:38)\r\n  [Clang 11.0.1 ]\r\n\r\n           pyvista : 0.35.dev0\r\n               vtk : 9.1.0\r\n             numpy : 1.22.1\r\n           imageio : 2.9.0\r\n           appdirs : 1.4.4\r\n            scooby : 0.5.12\r\n        matplotlib : 3.5.2\r\n           IPython : 7.32.0\r\n          colorcet : 3.0.0\r\n           cmocean : 2.0\r\n        ipyvtklink : 0.2.2\r\n             scipy : 1.8.0\r\n        itkwidgets : 0.32.1\r\n              tqdm : 4.60.0\r\n            meshio : 5.3.4\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct\n",
        "hint": "Considering we are purely using VTK for this interface through the `vtk_to_numpy` function, I'm inclined to think this may be an upstream bug where the array doesn't interface completely\r\n\r\nhttps://github.com/pyvista/pyvista/blob/eaa71a640264765c8a9b40d485b6ccdb970c87b0/pyvista/core/pointset.py#L762 \r\n\r\nor perhaps there is a better way to access the Polys from the VTK object?\nA pure VTK demonstration of the issue:\r\n\r\n```py\r\nimport vtk\r\nfrom vtkmodules.util.numpy_support import vtk_to_numpy\r\n\r\n# https://github.com/pyvista/pyvista/issues/2705\r\nsphere = vtk.vtkSphereSource()\r\nsphere.Update()\r\nmesh = sphere.GetOutput()\r\n\r\n\r\nf = vtk_to_numpy(mesh.GetPolys().GetData())\r\n\r\n>>> f\r\narray([  3,   2,  30, ..., 840,  29,  28])\r\n>>> a = f[1:4]\r\n>>> a\r\narray([ 2, 30,  0])\r\n>>> b = f[5:8]\r\n>>> b\r\narray([30, 58,  0])\r\n>>> f[1:4] = b\r\n>>> f[5:8] = a\r\n>>> f\r\narray([  3,  30,  58, ..., 840,  29,  28])\r\n\r\nassert all(f[1:4] == b) and all(f[5:8] == a), 'if this fails, we have more serious problems'\r\n\r\n# access overwrites `f` in place which is unexpected and causes the check above to now fail\r\nvtk_to_numpy(mesh.GetPolys().GetData())\r\nassert all(f[1:4] == b) and all(f[5:8] == a), 'well, this is not expected'\r\n```\nWell, this was a joy to track down. The short answer: Use `vtkCellArray.GetConnectivityArray()` instead of `vtkCellArray.GetData()`.\r\n\r\nThe long explanation: A while ago, `vtkCellArray` was changed so that it can store either 32 bit or 64 bit connectivity data. This enabled saving memory when VTK is compiled with 64 bit ids (`vtkIdType`) AND if the mesh has less than 2 billion connectivity entries. This meant that `vtkCellArray` can store either a 32 bit or 64 bit array for connectivity. However, `GetData()` returns a `vtkIdTypeArray` which is usually compiled to be 64 bit so `vtkCellArray` could no longer return its internal array in this method. So, the placeholder implementation now makes a copy from its internal array to return. So changing this array does not actually affect the connectivity. The issue shown here is a side effect of this implementation. `vtkCellArray.GetConnectivityArray()` actually returns the internal array and can be safely changed to affect the connectivity. pyvista and VTK Python classes have to be changed to use `vtkCellArray.GetConnectivityArray()` and `vtkCellArray.GetOffsetsArray()`. Also don't use `vtkCellArray.SetCells()`. Use `vtkCellArray.SetData()` instead.\nUnfortunately for PyVista, this is not an easy fix as `GetConnectivityArray()` is not a direct stand-in for `GetData()`. \r\n\r\n`GetData()` array has connectivity offsets (`GetOffsetsArray()`) embedded in the array, whereas `GetConnectivityArray()` does not. This is quite nice for `vtkPolyData.GetPolys()` if and only if all of the polys are the same size (e.g., 3 for triangulated mesh), but if there are mixed polys, this is pretty tough to work with when trying to update the array in-place:\r\n\r\n```py\r\nimport numpy as np\r\nimport pyvista as pv\r\nfrom pyvista import _vtk\r\n\r\nvertices = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0], [0.5, 0.5, -1]])\r\n\r\n# mesh faces\r\nfaces = np.hstack(\r\n    [\r\n        [4, 0, 1, 2, 3],  # square\r\n        [3, 0, 1, 4],  # triangle\r\n        [3, 1, 2, 4],  # triangle\r\n    ]\r\n)\r\n\r\nsurf = pv.PolyData(vertices, faces)\r\n```\r\n\r\n```py\r\n>>> _vtk.vtk_to_numpy(surf.GetPolys().GetData())\r\narray([4, 0, 1, 2, 3, 3, 0, 1, 4, 3, 1, 2, 4])\r\n```\r\n\r\n```py\r\n>>> _vtk.vtk_to_numpy(surf.GetPolys().GetConnectivityArray())\r\narray([0, 1, 2, 3, 0, 1, 4, 1, 2, 4])\r\n```\r\n\r\n```py\r\n>>> _vtk.vtk_to_numpy(surf.GetPolys().GetOffsetsArray())\r\narray([ 0,  4,  7, 10])\r\n```\r\n\r\nIn PyVista, we designed the `.cells`, `.faces`, `.strips`, `.lines`, etc. APIs around the assumption of having the connectivity offsets embedded in the array. This means we'd have to redesign these accessors entirely (breaking a lot of downstream code) just to enable this in-place operation. Further, changing the API to require users to deal with both connectivities and offsets as separate arrays will likely be too much of a burden.\r\n\r\n------\r\n\r\n> the placeholder implementation now makes a copy from its internal array to return. So changing this array does not actually affect the connectivity.\r\n\r\n@berkgeveci, I'm curious if you think this copy could/will be fixed upstream in VTK so that `GetData()` could work with in-place operations?\nAh I see. Unfortunately, what you want is not possible. The underlying structure for `vtkCellArray` changed and it does not store the number of vertices per cell anymore. This is the main reason why `GetData()` creates a new array (something that I forgot). For backwards compatibility. But it has the side effect of this array being \"detached\" from the actual topology of the mesh. There are good reasons for this change:\r\n\r\n- Most other codes and OpenGL prefer the new structure so there is more likelihood that VTK will be binary compatible with them.\r\n- When we get rid of the vertex count and always have the offsets, we can have random access at no additional memory cost. In the past, for polydata, the offsets were built when needed to save memory.\r\n\r\nNote that there is no loss of information here. You can simply recover the number of vertices of a cell by doing `offset[cellId+1]-offset[cellId]`. To make this work, the offsets array actually is of size `ncells+1` so that you can get the number of vertices for the last cell.\r\n\r\nI understand that this will be a backwards incompatible change for PyVista but there is no other way. You can still support\r\n\r\n```\r\n# mesh faces\r\nfaces = np.hstack(\r\n    [\r\n        [4, 0, 1, 2, 3],  # square\r\n        [3, 0, 1, 4],  # triangle\r\n        [3, 1, 2, 4],  # triangle\r\n    ]\r\n)\r\n\r\nsurf = pv.PolyData(vertices, faces)\r\n```\r\n\r\nbut it will have to create the connectivity and offsets array under the covers (or let VTK do it). But if you want to avoid the deep copy, you have to support `pv.PolyData(vertices, offsets, connectivity)`, which has to create a `vtkCellArray` and call `SetData()` on it.\r\nI suggest avoiding the use of `GetData()` all together.\r\n\r\nAlso note that there is a bug in how `SetData()` works currently and when called from Python, it does not cause `vtkCellArray()` to hold a reference to the underlying numpy buffer. Unless you are keeping a reference to that object from the Python side, the VTK object will hold a reference to a junk pointer once the numpy array goes out of scope. I am working on a fix for this issue.\r\n",
        "base": "b9ee9b9c0625d737dfd4a60e9ed10609e7e0b7fd",
        "env": "4c2d1aed10b1600d520271beba8579c71433e808",
        "files": [
            "pyvista/core/pointset.py",
            "pyvista/demos/logo.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4225",
        "problem": "Cell wise and dimension reducing filters do not work for PointSet\n### Describe the bug, what's wrong, and what you expected.\n\n`PointSet` is an odd data type as it has no cells and simply represents a 0-dimensional geometry: point clouds.\r\n\r\nThis means that two common types of operations are not possible on this data type:\r\n\r\n1. Cell-wise operations like thresholding\r\n2. Dimension-reducing operations like contouring\r\n\r\nCell wise operations can easily be fixed by adding cells. This can be done with `cast_to_polydata()`\r\n\r\nDimension-reducing operations, on the other hand, have no solution and should not be allowed on `PointSet`.\r\n\r\nHow can we properly error out or convert to `PolyData` when calling dataset filters like `threshold()` and `contour()`? Should these types of filters be overridden on the `PointSet` class?\n\n### Steps to reproduce the bug.\n\n### Cell wise operation produces *invalid* output\r\n\r\n```python\r\nimport pyvista as pv\r\nimport numpy as np\r\npc = pv.PointSet(np.random.random((100, 3)))\r\npc['foo'] = np.arange(pc.n_points)\r\npc.threshold()\r\n```\r\n\r\n<img width=\"575\" alt=\"Screen Shot 2023-01-11 at 5 47 02 PM 1\" src=\"https://user-images.githubusercontent.com/22067021/211949301-8d10e9ac-172e-4f27-ad81-c3ec2d335263.png\">\r\n\r\n\r\n### Dimension reducing operation produces *no* output\r\n\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npc = pv.PointSet(np.random.random((100, 3)))\r\npc['foo'] = np.arange(pc.n_points)\r\npc.contour()\r\n```\r\n<img width=\"417\" alt=\"Screen Shot 2023-01-11 at 5 47 57 PM\" src=\"https://user-images.githubusercontent.com/22067021/211949430-a3e77292-6b1e-4d2d-b2e3-b2a640ed65fc.png\">\r\n\r\n\n\n### System Information\n\n```shell\nn/a\n```\n\n\n### Screenshots\n\nn/a\n",
        "hint": "",
        "base": "18694fa1a4e95027a055a958fef61ffdc5dba4d7",
        "env": "4c2d1aed10b1600d520271beba8579c71433e808",
        "files": [
            "doc/source/conf.py",
            "pyvista/core/errors.py",
            "pyvista/core/pointset.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/432",
        "problem": "Strictly enforce keyword arguments\nI see folks quite often forget the s in the `scalars` argument for the `BasePlotter.add_mesh()` method. We should allow `scalar` as an alias much like how we allow `rng` and `clim` for the colorbar range/limits\n",
        "hint": "I would disagree. 2 arguments:\r\n\r\n`There should be one-- and preferably only one --obvious way to do it.` as says https://www.python.org/dev/peps/pep-0020/\r\n\r\nit makes documentation harder.\nRaise an exception to tell the user to use `scalars` when `scalar` is used as a keyword arg. \nOkay, I'm convinced. \r\n\r\nWe may want to do some keyword refactoring and not just allow an endless dict of `**kwargs` to be passed to `add_mesh`. For example, you can pass all types of typos and not have an error thrown:\r\n\r\n```py\r\nimport pyvista as pv\r\np = pv.Plotter()\r\np.add_mesh(pv.Cube(), my_crazy_argument=\"foooooooo\")\r\np.show()\r\n```\r\n\r\nThere are several places in the API where this can happen that will need to be refactored \nWhat we could do is pop out the allowed aliases like `rng` and then raise an exception if the `kwargs` dict is not empty that might say something like:\r\n\r\n> \"[list of arguments still in the kwargs] are not valid keyword arguments.\"",
        "base": "e1bac62d2f3df7bcba25d58ed5e83d89f9a6be98",
        "env": "1affc211d8b93024fa3e163584f235975d3536ed",
        "files": [
            "examples/01-filter/decimate.py",
            "examples/01-filter/streamlines.py",
            "pyvista/core/filters.py",
            "pyvista/core/objects.py",
            "pyvista/plotting/helpers.py",
            "pyvista/plotting/plotting.py",
            "pyvista/plotting/qt_plotting.py",
            "pyvista/plotting/widgets.py",
            "pyvista/utilities/__init__.py",
            "pyvista/utilities/errors.py",
            "pyvista/utilities/geometric_objects.py",
            "pyvista/utilities/helpers.py",
            "pyvista/utilities/parametric_objects.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/3710",
        "problem": "circle creates creates one zero length edge\n### Describe the bug, what's wrong, and what you expected.\n\nI expected that:\r\n> circle = pv.Circle(radius, resolution=n)\r\n\r\n1. would create a circle with n points and edge. \r\nIt does yay :-) \r\n\r\n3. That each edge of the would have similar length.\r\nIt does _not_ :-(\r\n\r\nThe problems seems circle  closed is doubly:\r\n- once by the coordinates (circle.points[:, 0] == approx(circle.points[:, -1])\r\n- and a second time by the face (circle.faces[0] == [n, 0, 1, 2, ... n-1])\r\n\r\n\r\n\r\n\n\n### Steps to reproduce the bug.\n\n```python\r\nimport pyvista as pv\r\n\r\ncircle = pv.Circle(radius=1, resolution=4) # lets make a low res circle\r\nprint(circle.faces)  # out: array([4, 0, 1, 2, 3])\r\nprint(circle.n_points)  # out: 4\r\n\r\n# the print outputs gives the expectation that circle.plot() will look like a square \r\ncircle.plot()\r\n```\r\n![pv.Circle(radius=1, resolution=4).plot()](https://user-images.githubusercontent.com/107837123/207049939-9b24ac31-a8e8-4ca7-97d3-3f4105a524dc.png)\r\n\r\n\r\n\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Mon Dec 12 13:55:16 2022 CET\r\n\r\n                OS : Linux\r\n            CPU(s) : 8\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n       GPU Details : error\r\n\r\n  Python 3.10.4 (main, Mar 23 2022, 20:25:24) [GCC 11.3.0]\r\n\r\n           pyvista : 0.36.1\r\n               vtk : 9.1.0\r\n             numpy : 1.23.5\r\n           imageio : 2.22.4\r\n           appdirs : 1.4.4\r\n            scooby : 0.7.0\r\n        matplotlib : 3.6.2\r\n         pyvistaqt : 0.9.0\r\n             PyQt5 : Version unknown\r\n           IPython : 8.2.0\r\n             scipy : 1.9.3\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n_No response_\n",
        "hint": "",
        "base": "846f1834f7428acff9395ef4b8a3bd39b42c99e6",
        "env": "8dd8eeb80248cea4360c753847bd622e8652e059",
        "files": [
            "pyvista/utilities/geometric_objects.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/3675",
        "problem": "vtkVolume needs wrapping like vtkActor\nWe wrap vtkActor nicely and should do the same for vtkVolume to make lookup table modification during volume rendering nicer.\r\n\r\n```py\r\nimport pyvista as pv\r\nfrom pyvista import examples\r\n\r\nvol = examples.download_knee_full()\r\n\r\np = pv.Plotter(notebook=0)\r\nactor = p.add_volume(vol, cmap=\"bone\", opacity=\"sigmoid\")\r\nactor.mapper.lookup_table.cmap = 'viridis'\r\np.show()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nInput In [3], in <cell line: 8>()\r\n      6 p = pv.Plotter(notebook=0)\r\n      7 actor = p.add_volume(vol, cmap=\"bone\", opacity=\"sigmoid\")\r\n----> 8 actor.mapper.lookup_table.cmap = 'viridis'\r\n      9 p.show()\r\n\r\nAttributeError: 'vtkmodules.vtkRenderingCore.vtkVolume' object has no attribute 'mapper'\r\n```\n",
        "hint": "",
        "base": "905044f26df65188c6c612695c3370fcc2d007f0",
        "env": "8dd8eeb80248cea4360c753847bd622e8652e059",
        "files": [
            "pyvista/_vtk.py",
            "pyvista/plotting/actor.py",
            "pyvista/plotting/mapper.py",
            "pyvista/plotting/plotting.py",
            "pyvista/plotting/prop3d.py",
            "pyvista/plotting/volume.py",
            "pyvista/plotting/volume_property.py"
        ]
    },
    {
        "pr": "pyvista/pyvista/4315",
        "problem": "Rectilinear grid does not allow Sequences as inputs\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nRectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.\r\n\r\n### Steps to reproduce the bug.\r\n\r\nThis doesn't work\r\n```python\r\nimport pyvista as pv\r\npv.RectilinearGrid([0, 1], [0, 1], [0, 1])\r\n```\r\n\r\nThis works\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))\r\n```\r\n### System Information\r\n\r\n```shell\r\n--------------------------------------------------------------------------------\r\n  Date: Wed Apr 19 20:15:10 2023 UTC\r\n\r\n                OS : Linux\r\n            CPU(s) : 2\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n        GPU Vendor : Mesa/X.org\r\n      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)\r\n       GPU Version : 4.5 (Core Profile) Mesa 20.3.5\r\n\r\n  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.38.5\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n           imageio : 2.27.0\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\n_No response_\n",
        "hint": "",
        "base": "db6ee8dd4a747b8864caae36c5d05883976a3ae5",
        "env": "4c2d1aed10b1600d520271beba8579c71433e808",
        "files": [
            "pyvista/core/grid.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/996",
        "problem": "Memory leaks when accessing sequence tags with Dataset.__getattr__.\n**Describe the bug**\r\nAccessing sequences via `Dataset.__getattr__` seems to leak memory. The bug occurred for me when I was processing many DICOMs and manipulating some tags contained in sequences and each leaked a bit of memory, ultimately crashing the process.\r\n\r\n**Expected behavior**\r\nMemory should not leak. It works correctly when you replace the `__getattr__` call with `__getitem__` (by manually constructing the necessary tag beforehand).\r\n\r\nWithout being an expert in the codebase, one difference I think that could explain it is that `__getattr__` sets `value.parent = self` for sequences while `__getitem__` doesn't seem to do that. Maybe this loop of references somehow confuses Python's garbage collection?\r\n\r\n**Steps To Reproduce**\r\nThis increases the memory consumption of the Python process by about 700 MB on my machine. The DICOM file I've tested it with is 27MB and has one item in `SourceImageSequence`. Note that the memory leak plateaus after a while in this example, maybe because it's the same file. In my actual workflow when iterating over many different files, the process filled all memory and crashed.\r\n\r\n```python\r\nimport pydicom\r\nfor i in range(100):\r\n  dcm = pydicom.dcmread(\"my_dicom.dcm\")\r\n  test = dcm.SourceImageSequence\r\n```\r\n\r\nFor comparison, this keeps the memory constant. `(0x0008, 0x2112)` is `SourceImageSequence`: \r\n\r\n```python\r\nimport pydicom\r\nimport pydicom.tag\r\nfor i in range(100):\r\n  dcm = pydicom.dcmread(\"my_dicom.dcm\")\r\n  test = dcm[pydicom.tag.TupleTag((0x0008, 0x2112))]\r\n```\r\n\r\n**Your environment**\r\n\r\n```bash\r\nLinux-4.15.0-72-generic-x86_64-with-Ubuntu-18.04-bionic\r\nPython  3.6.8 (default, Jan 14 2019, 11:02:34)\r\npydicom  1.3.0\r\n```\r\n\n",
        "hint": "I could reproduce this with a smaller sequence (2MB) under Windows. The memory usage got up until it was about twice the initial value (after a few hundred iterations), after that got back down to the initial value and started to rise again.\r\nIf removing the code that sets the parent, this does not happen - the memory remains at the initial value.\r\nSo, your assessment about the garbage collection delayed due to cyclic references seems to be correct.\r\nI will have a look how to resolve this - probably later this week.\nI'm not seeing any memory leakage using [memory-profiler](https://pypi.org/project/memory-profiler/). What are you using to profile?\r\n\r\nI get similar results (~38 MB, no sign of gradual increase) for each of the following branches and Python 3.6.5, current `master`:\r\n```python\r\nimport time\r\nfrom pydicom import dcmread\r\nfrom pydicom.data import get_testdata_file\r\n\r\nfname = get_testdata_file(\"rtplan.dcm\")\r\nbranch = 'A'\r\n\r\nif branch == 'A':\r\n    for ii in range(1):\r\n        ds = dcmread(fname)\r\nelif branch == 'B':\r\n    for ii in range(1):\r\n        ds = dcmread(fname)\r\n        seq = ds.BeamSequence\r\nelif branch == 'C':\r\n    for ii in range(10000):\r\n        ds = dcmread(fname)\r\nelif branch == 'D':\r\n    for ii in range(10000):\r\n        ds = dcmread(fname)\r\n        seq = ds.BeamSequence\r\nelif branch == 'E':\r\n    for ii in range(10000):\r\n        ds = dcmread(fname)\r\n        seq = ds[0x300a, 0x00b0].value\r\n\r\ntime.sleep(1)\r\n```\r\nIt looks like there's a bit of overhead for ds.BeamSequence but not a lot?\n@scaramallion - I didn't use a profiler, just watched the private memory consumption. I can add a screenshot if I get to it tonight. A couple of remarks:\r\n- there is no real leak here, as the memory is freed eventually, just not immediately as with the scenario without setting a parent\r\n- the test data you used is quite small - I used a real rtstruct file for the test with a sequence of about 2MB size, so that is more visible in this case\r\n- not setting the parent didn't fail any test, so while I'm sure there was a scenario where this was not needed, there is no tests for it; I have to check if and when this is really needed\r\n- the solution is not complete anyway, as with the access via `__getitem__` the parent is not set",
        "base": "d21e97c9a35b5c225dc3340faaa7c293e7c8ee9b",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/dataset.py",
            "pydicom/filewriter.py",
            "pydicom/sequence.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1241",
        "problem": "Add support for Extended Offset Table to encaps module\n[CP1818](http://webcache.googleusercontent.com/search?q=cache:xeWXtrAs9G4J:ftp://medical.nema.org/medical/dicom/final/cp1818_ft_whenoffsettabletoosmall.pdf) added the use of an Extended Offset Table for encapsulated pixel data when the Basic Offset Table isn't suitable.\n",
        "hint": "Some notes for myself:\r\n\r\n* The EOT is optional and intended to support users where there are multiple compressed frames and the total length of all the frames exceeds the `2**32 - 1` limit available in the BOT item lengths. \r\n* Only 1 fragment per frame is allowed with an EOT\r\n* The *Extended Table Offset Lengths* is the length (in bytes) of each compressed frame\r\n* It's not usable where each compressed frame is larger than `2**32 - 1` due to the limit in the 4-byte (FFFE,E000) Item Tag length (who has 4 GB compressed images anyway?)\r\n\r\nRelevant links: [google groups issue](https://groups.google.com/forum/?nomobile=true#!searchin/comp.protocols.dicom/extended$20offset|sort:date/comp.protocols.dicom/piMk2TmcyEg/daCk33zDBwAJ), [Image Pixel module](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.3.html), [BOT](http://dicom.nema.org/medical/dicom/current/output/chtml/part05/sect_A.4.html)\r\n\r\nImplementation shouldn't be difficult, something along the lines of:\r\n```python\r\nfrom struct import pack\r\nfrom pydicom.encaps import encapsulate\r\n\r\nframe_data: List[bytes] = [...]\r\nds.PixelData = encapsulate(frame_data, has_bot=False, fragments_per_frame=1)\r\n\r\nframe_lengths = [len(f) for f in frame_data]\r\nframe_offsets = [0]\r\nfor ii, length in enumerate(frame_lengths[:-1]):\r\n    frame_offsets .append(frame_offsets [ii] + length + 8)\r\nds.ExtendedOffsetTable =  pack(\"<{len(frame_offsets )}Q\", *frame_offsets )\r\nds.ExtendedOffsetTableLengths = pack(\"<{len(frame_lengths)}Q\", *frame_lengths)\r\n```\r\n\r\nMaybe just add a helper function to ensure conformance? And add a check to `encapsulate()` for data that's too long.\r\n```python\r\nfrom pydicom.encaps import encapsulate_extended\r\n\r\nout: Tuple[bytes, bytes, bytes] = encapsulate_extended(frame_data)\r\nds.PixelData = out[0]\r\nds.ExtendedOffsetTable = out[1]\r\nds.ExtendedOffsetTableLengths = out[2]\r\n```\r\n\r\nParsing should already be handled correctly by `generate_pixel_data()` since it's 1 frame per fragment.",
        "base": "9d69811e539774f296c2f289839147e741251716",
        "env": "9d69811e539774f296c2f289839147e741251716",
        "files": [
            "pydicom/encaps.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/866",
        "problem": "Handle odd-sized dicoms with warning\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n\r\nWe have some uncompressed dicoms with an odd number of pixel bytes (saved by older versions of pydicom actually). \r\n\r\nWhen we re-open with pydicom 1.2.2, we're now unable to extract the image, due to the change made by https://github.com/pydicom/pydicom/pull/601\r\n\r\nWould it be possible to emit a warning instead of rejecting the dicom for such cases?\r\n\r\n#### Version\r\n1.2.2\n",
        "hint": "I agree that this would be sensible. Checking the mentioned PR, I also found that we had [discussed this briefly](https://github.com/pydicom/pydicom/pull/601#issuecomment-374710953) (with no result, obviously).\r\n@darcymason - I think this is another case of \"try to handle broken DICOM, only raise in 'strict' mode\", especially as these images have been written by pydicom itself...\nunsponsored plug: I like reviewable.io to keep track of what comments are resolved/unresolved/just-a-nit before merging. Looks like its free for open source repos.\n> @darcymason - I think this is another case of \"try to handle broken DICOM, only raise in 'strict' mode\", especially as these images have been written by pydicom itself...\r\n\r\nYes, I agree, for sure.  Even without it being written by pydicom, it is better to be tolerant on reading.",
        "base": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "env": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "files": [
            "pydicom/pixel_data_handlers/numpy_handler.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1050",
        "problem": "LUT Descriptor tag with no value yields TypeError\n**Describe the bug**\r\nI have a DICOM image with the following tag (copied from ImageJ)\r\n\r\n```\r\n0028,1101  Red Palette Color Lookup Table Descriptor: \r\n```\r\n\r\nwhich corresponds to the raw data element, produced by [`DataElement_from_raw`](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L699):\r\n```\r\nRawDataElement(tag=(0028, 1101), VR='US', length=0, value=None, value_tell=1850, is_implicit_VR=False, is_little_endian=True)\r\n```\r\n\r\nBecause this tag is matched by the [LUT Descriptor tags](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L696) and the value is empty (`None`), the [following line](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L761):\r\n```\r\nif raw.tag in _LUT_DESCRIPTOR_TAGS and value[0] < 0:\r\n```\r\nresults in \r\n```\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\n**Expected behavior**\r\n\r\nGiven that I discovered this by parsing what seems to be a set of faulty DICOMs (mangled pixel data), I'm not sure if an error should be raised if the colour attribute value is not provided.\r\n\r\nHowever, given that `value` can be `None` for other tags, the simple fix is\r\n\r\n```python\r\ntry:\r\n    if raw.tag in _LUT_DESCRIPTOR_TAGS and value[0] < 0:\r\n        # We only fix the first value as the third value is 8 or 16\r\n        value[0] += 65536\r\nexcept TypeError:\r\n    pass\r\n```\r\n\r\n(or test if `value` is iterable).\r\n\r\n**Your environment**\r\n```\r\nDarwin-19.3.0-x86_64-i386-64bit\r\nPython  3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:05:27)\r\n[Clang 9.0.1 ]\r\npydicom  1.4.1\r\n```\r\n\r\nMany thanks!\n",
        "hint": "",
        "base": "00c248441ffb8b7d46c6d855b723e696a8f5aada",
        "env": "5098c9147fadcb3e5918487036867931435adeb8",
        "files": [
            "pydicom/dataelem.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/944",
        "problem": "Embedded Null character\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in convert_encodings(encodings)\r\n    624         try:\r\n--> 625             py_encodings.append(python_encoding[encoding])\r\n    626         except KeyError:\r\n\r\nKeyError: 'ISO_IR 100\\x00'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-605c3c3edcf4> in <module>\r\n      4 print(filename)\r\n      5 dcm = pydicom.dcmread(filename,force=True)\r\n----> 6 dcm = pydicom.dcmread('/home/zhuzhemin/XrayKeyPoints/data/10-31-13_11H18M20_3674972_FACE_0_SC.dcm',force=True)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in dcmread(fp, defer_size, stop_before_pixels, force, specific_tags)\r\n    848     try:\r\n    849         dataset = read_partial(fp, stop_when, defer_size=defer_size,\r\n--> 850                                force=force, specific_tags=specific_tags)\r\n    851     finally:\r\n    852         if not caller_owns_file:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in read_partial(fileobj, stop_when, defer_size, force, specific_tags)\r\n    726         dataset = read_dataset(fileobj, is_implicit_VR, is_little_endian,\r\n    727                                stop_when=stop_when, defer_size=defer_size,\r\n--> 728                                specific_tags=specific_tags)\r\n    729     except EOFError:\r\n    730         pass  # error already logged in read_dataset\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in read_dataset(fp, is_implicit_VR, is_little_endian, bytelength, stop_when, defer_size, parent_encoding, specific_tags)\r\n    361     try:\r\n    362         while (bytelength is None) or (fp.tell() - fp_start < bytelength):\r\n--> 363             raw_data_element = next(de_gen)\r\n    364             # Read data elements. Stop on some errors, but return what was read\r\n    365             tag = raw_data_element.tag\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in data_element_generator(fp, is_implicit_VR, is_little_endian, stop_when, defer_size, encoding, specific_tags)\r\n    203                 # Store the encoding value in the generator\r\n    204                 # for use with future elements (SQs)\r\n--> 205                 encoding = convert_encodings(encoding)\r\n    206 \r\n    207             yield RawDataElement(tag, VR, length, value, value_tell,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in convert_encodings(encodings)\r\n    626         except KeyError:\r\n    627             py_encodings.append(\r\n--> 628                 _python_encoding_for_corrected_encoding(encoding))\r\n    629 \r\n    630     if len(encodings) > 1:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in _python_encoding_for_corrected_encoding(encoding)\r\n    664     # fallback: assume that it is already a python encoding\r\n    665     try:\r\n--> 666         codecs.lookup(encoding)\r\n    667         return encoding\r\n    668     except LookupError:\r\n\r\nValueError: embedded null character\r\n#### Steps/Code to Reproduce\r\n<!--\r\nExample:\r\n```py\r\nfrom io import BytesIO\r\nfrom pydicom import dcmread\r\n\r\nbytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\r\n             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\r\n             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\r\n             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\r\n             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\r\n\r\nfp = BytesIO(bytestream)\r\nds = dcmread(fp, force=True)\r\n\r\nprint(ds.PatientID)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n\r\nWhen possible use pydicom testing examples to reproduce the errors. Otherwise, provide\r\nan anonymous version of the data in order to replicate the errors.\r\n-->\r\nimport pydicom\r\ndcm = pydicom.dcmread('/home/zhuzhemin/XrayKeyPoints/data/10-31-13_11H18M20_3674972_FACE_0_SC.dcm')\r\n\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\nNo error\r\nI used dcmread function in matlab to read the same file and it was ok. So it should not be the problem of the file.\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\nError: Embedded Null character\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n1.3.0\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
        "hint": "Well, this is invalid DICOM, so strictly speaking this is not a bug, but we can probably just ignore the null character and change the error into a warning (if `config.enforce_valid_values` is not set).\n> Well, this is invalid DICOM, so strictly speaking this is not a bug, but we can probably just ignore the null character and change the error into a warning (if `config.enforce_valid_values` is not set).\r\n\r\nWe could do that, but given that this is the first time seeing an error like this, I don't think it is worth the effort.  Instead, I think using [data_element_callback](https://pydicom.github.io/pydicom/stable/api_ref.html#pydicom.config.data_element_callback) is better suited - example of how to use in my comment [here](https://github.com/pydicom/pydicom/issues/820#issuecomment-473500989).  Not quite the same situation, but could be adapted to replace the bad value.\nI'm in favour of ignoring it with a warning/exception, its not really any different from all the other non-conformant fixes we have.\nI guess a terminating null is probably common enough carry-over from C that we could be tolerant to reading that.  I'm worried about chasing invalid DICOM endlessly, though, when there is an existing facility for people to filter any kind of invalid values out, and every extra check hits performance, if only a small amount.  Perhaps we just need to make that `util.fixer` code easier to use, maybe a `pydicom.config` setting to set characters to strip as a one-liner before reading a file.\nYeah, I see your point... the code gets messier each time we add a workaround for another incarnation of invalid DICOM. \r\nThis concrete exception happens during Python encoding lookup (as a fallback to check if the encoding is already a Python encoding), where we only expect a `LookupError`. The actual fix, if we would add one, would have to happen earlier (like stripping any trailing zero byte from string values), but I'm not sure if that's worth it.\r\nIt may be interesting to understand where this comes from, as I doubt any major DICOM library or modality would have written such a value, and if this may happen elsewhere. I would also check if dcmtk handles this - if they do, I would be more inclined to add a fix. \nOk, dcmdump just ignores it (I replaced the last '0' by a a zero):\r\n```(0008,0005) CS [ISO_IR 10 ]                             #  10, 1 SpecificCharacterSet```\r\n\n> I'm worried about chasing invalid DICOM endlessly, though, when there is an existing facility for people to filter any kind of invalid values out, and every extra check hits performance, if only a small amount.\r\n\r\nFair enough. Perhaps we could update fixer (if needed) and have a 'library' of available fixes instead and make sure its all documented. That way in the future we can just add to the library instead of adding a workaround to the codebase.\nI read your answers but cannot adapt the callback to my own situation. I really do not understand how dicom is organized and parsed.  All I want is to grab the pixel array and do something. Would Appreciate if you could update the version and fix it.\nAs @mrbean-bremen said, this can't currently be fixed using `config.data_element_callback` because that gets called after reading while the exception is raised during reading (because character set is special). We'd need an earlier hook if we want to go the fixer route.\nLet's try this again... quick workaround.\r\n```python\r\nimport codecs\r\nimport re\r\n\r\nfrom pydicom import charset\r\nfrom pydicom import dcmread\r\n\r\ndef _python_encoding_for_corrected_encoding(encoding):\r\n    encoding = encoding.strip(' \\r\\t\\n\\0')\r\n\r\n    # standard encodings\r\n    patched = None\r\n    if re.match('^ISO[^_]IR', encoding) is not None:\r\n        patched = 'ISO_IR' + encoding[6:]\r\n    # encodings with code extensions\r\n    elif re.match('^(?=ISO.2022.IR.)(?!ISO 2022 IR )',\r\n                  encoding) is not None:\r\n        patched = 'ISO 2022 IR ' + encoding[12:]\r\n\r\n    if patched:\r\n        # handle encoding patched for common spelling errors\r\n        try:\r\n            py_encoding = python_encoding[patched]\r\n            charset._warn_about_invalid_encoding(encoding, patched)\r\n            return py_encoding\r\n        except KeyError:\r\n            charset._warn_about_invalid_encoding(encoding)\r\n            return default_encoding\r\n\r\n    # fallback: assume that it is already a python encoding\r\n    try:\r\n        codecs.lookup(encoding)\r\n        return encoding\r\n    except LookupError:\r\n        charset._warn_about_invalid_encoding(encoding)\r\n        return default_encoding\r\n\r\ncharset._python_encoding_for_corrected_encoding = _python_encoding_for_corrected_encoding\r\n\r\nds = dcmread(...)\r\n```\nInterestingly we actually do handle charset values that have trailing padding `\\x00`, through `valuerep.MultiString`, what we don't do is handle values that end in more than one null (i.e. `\\x00\\x00`) which is the case here.\r\n\r\nWe could change [this line](https://github.com/pydicom/pydicom/blob/a0300a69a1da1626caef0d9738cff29b17ce79cc/pydicom/valuerep.py#L548)\r\n```python\r\nwhile val.endswith(' ') or val.endswith('\\x00'):\r\n    val = val[:-1]\r\n```\nHm, that looks like an easy fix without impact - I wasn't aware of this. The only question is - should we warn in this case?\r\nThat being said, I still think that your proposal to add a repository of available fixes is a good one, even if not applicable to this issue.",
        "base": "a0300a69a1da1626caef0d9738cff29b17ce79cc",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/valuerep.py",
            "pydicom/values.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1255",
        "problem": "Mypy errors\n**Describe the bug**\r\nSeveral of the type hints are problematic and result in mypy errors.\r\n\r\nOne example:\r\n\r\n```none\r\ncat << EOF > /tmp/test.py\r\nfrom pydicom import Dataset, dcmread\r\n\r\ndataset = Dataset()\r\ndataset.Rows = 10\r\ndataset.Columns = 20\r\ndataset.NumberOfFrames = \"5\"\r\n\r\nassert int(dataset.NumberOfFrames) == 5\r\n\r\nfilename = '/tmp/test.dcm'\r\ndataset.save_as(str(filename))\r\n\r\ndataset = dcmread(filename)\r\n\r\nassert int(dataset.NumberOfFrames) == 5\r\nEOF\r\n```\r\n\r\n```none\r\nmypy /tmp/test.py\r\n/tmp/test.py:15: error: No overload variant of \"int\" matches argument type \"object\"\r\n/tmp/test.py:15: note: Possible overload variant:\r\n/tmp/test.py:15: note:     def int(self, x: Union[str, bytes, SupportsInt, _SupportsIndex] = ...) -> int\r\n/tmp/test.py:15: note:     <1 more non-matching overload not shown>\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\n**Expected behavior**\r\nMypy should not report any errors.\r\n\r\n**Steps To Reproduce**\r\nSee above\r\n\r\n**Your environment**\r\n```none\r\npython -m pydicom.env_info\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.6-x86_64-i386-64bit\r\nPython       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\r\npydicom      | 2.1.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.3\r\nPIL          | 8.0.1\r\n```\nImportError: cannot import name 'NoReturn'\n**Describe the bug**\r\nthrow following excetion when import pydicom package:\r\n```\r\nxxx/python3.6/site-packages/pydicom/filebase.py in <module>\r\n5 from struct import unpack, pack\r\n      6 from types import TracebackType\r\n----> 7 from typing import (\r\n      8     Tuple, Optional, NoReturn, BinaryIO, Callable, Type, Union, cast, TextIO,\r\n      9     TYPE_CHECKING, Any\r\n\r\nImportError: cannot import name 'NoReturn'\r\n```\r\n\r\n**Expected behavior**\r\nimort pydicom sucessfully\r\n\r\n**Steps To Reproduce**\r\nHow to reproduce the issue. Please include a minimum working code sample, the\r\ntraceback (if any) and the anonymized DICOM dataset (if relevant).\r\n\r\n**Your environment**\r\npython:3.6.0\r\npydicom:2.1\r\n\n",
        "hint": "I ran into multiple other errors and would suggest removing `py.typed` until the type annotations have been properly tested. \n@hackermd, just for my education, since I have only dabbled in 'typing'  so far - does this break your workflow somehow?  Is it not possible to exclude pydicom from forcing errors? (other than removing `py.typed`)\r\n\nHmm, maybe we should just use `Any` for the element values rather than `object`. The problem is the element value type will be/is so broad users are pretty much going to have to `cast` everything no matter what we do.\n@darcymason \r\n\r\n> does this break your workflow somehow\r\n\r\nWe are running mypy on all our Pyhon code and our unit test pipelines are failing. I have fixed the pydicom version to `2.0.0`, but I would like to avoid doing that moving forward.\r\n\r\n>  Is it not possible to exclude pydicom from forcing errors? (other than removing py.typed)\r\n\r\nOne can add `#type: ingore` to exclude individual lines from type checks, but at the moment we are getting hundreds of errors.\r\n\r\nInstead of removing `py.typed`, one could also remove the type hints from problematic functions. `object` or `Any` are not that useful and seem to cause more trouble than benefit.\n@scaramallion \r\n\r\n> The problem is the element value type will be/is so broad users are pretty much going to have to cast everything no matter what we do\r\n\r\nAgreed. The return value of these \"magic\" methods will be difficult to type. It's basically a Union of all Python types corresponding to any of the DICOM Value Representations.\n@hackermd, thanks for the explanations.\r\n\r\nI suggest doing a patch release for this if @scaramallion is in agreement.  We could just remove the `py.typed`, as suggested, and do it quickly (and push 'typing' to v2.2),  or perhaps within a few days/couple of weeks if removal of `object` can be a better solution. @scaramallion, your thoughts?  \r\n\r\n@hackermd, would you be able to test on master if we try the correction route?\r\n\n> would you be able to test on master if we try the correction route\r\n\r\nYes, happy to help with that. \nYeah, lets do it. Fix #1253 and #1254 while we're at it. Pushing typing back to at least v2.2 is probably the way to go, too. \nNew in 3.6.2, well that's annoying",
        "base": "59be4993000cc8af2cd9a4176f15398b6a7875d7",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/_version.py",
            "pydicom/filebase.py",
            "pydicom/jsonrep.py",
            "setup.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1017",
        "problem": "Add support for missing VRs\nMissing: OV, SV, UV\r\n\r\n\n",
        "hint": "",
        "base": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/filewriter.py",
            "pydicom/values.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1048",
        "problem": "dcmread cannot handle pathlib.Path objects\n**Describe the bug**\r\nThe `dcmread()` currently fails when passed an instance of `pathlib.Path`. The problem is the following line:\r\nhttps://github.com/pydicom/pydicom/blob/8b0bbaf92d7a8218ceb94dedbee3a0463c5123e3/pydicom/filereader.py#L832\r\n\r\n**Expected behavior**\r\n`dcmread()` should open and read the file to which the `pathlib.Path` object points.\r\n\r\nThe line above should probably be:\r\n```python\r\nif isinstance(fp, (str, Path)):\r\n````\r\n\r\n**Steps To Reproduce**\r\n```python\r\nfrom pathlib import Path\r\nfrom pydicom.filereader import dcmread\r\n\r\ndcm_filepath = Path('path/to/file')\r\ndcmread(dcm_filepath)\r\n```\n",
        "hint": "Good point. We could use `os.fspath`, but that is only available from Python 3.6 onwards, so we probably have to add a respective check...\nThe `save_as()` method of `Dataset` has the same issue.\nYes, I noticed. I'm on it.\nYep.  I actually started writing code for it at one point, but got side-tracked with other issues.  Basically just wrapping the argument in str() fixes it.\r\n",
        "base": "00c248441ffb8b7d46c6d855b723e696a8f5aada",
        "env": "5098c9147fadcb3e5918487036867931435adeb8",
        "files": [
            "pydicom/data/data_manager.py",
            "pydicom/dataset.py",
            "pydicom/dicomdir.py",
            "pydicom/filereader.py",
            "pydicom/fileutil.py",
            "pydicom/filewriter.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1608",
        "problem": "Unable to assign single element list to PN field\nI am getting `AttributeError` while trying to assign a list of single element to a `PN` field.\r\nIt's converting `val` to a 2D array [here](https://github.com/pydicom/pydicom/blob/master/pydicom/filewriter.py#L328) when `VM` is 1. \r\n\r\n**Code**\r\n```\r\n>>> from pydicom import dcmread, dcmwrite\r\n>>> ds = dcmread(\"SOP1.dcm\")\r\n>>> a = [\"name1\"]\r\n>>> b = [\"name1\", \"name2\"]\r\n>>> ds.PatientName = a\r\n>>> dcmwrite(\"out.dcm\", ds)     # throws the error below\r\n>>> ds.PatientName = b\r\n>>> dcmwrite(\"out.dcm\", ds)     # works fine\r\n```\r\n\r\n**Error**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 28, in tag_in_exception\r\n    yield\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 562, in write_data_element\r\n    fn(buffer, elem, encodings=encodings)  # type: ignore[operator]\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in write_PN\r\n    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in <listcomp>\r\n    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\r\nAttributeError: 'MultiValue' object has no attribute 'encode'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 1153, in dcmwrite\r\n    _write_dataset(fp, dataset, write_like_original)\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 889, in _write_dataset\r\n    write_dataset(fp, get_item(dataset, slice(0x00010000, None)))\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/contextlib.py\", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 32, in tag_in_exception\r\n    raise type(exc)(msg) from exc\r\nAttributeError: With tag (0010, 0010) got exception: 'MultiValue' object has no attribute 'encode'\r\nTraceback (most recent call last):\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 28, in tag_in_exception\r\n    yield\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 562, in write_data_element\r\n    fn(buffer, elem, encodings=encodings)  # type: ignore[operator]\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in write_PN\r\n    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in <listcomp>\r\n    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\r\nAttributeError: 'MultiValue' object has no attribute 'encode'\r\n```\n",
        "hint": "Not sure if this is a bug, but at least it is unexpected behavior. Shall be easy to fix.",
        "base": "37dd49e2754a10db22e7cde2ac18aa8afc1f3af6",
        "env": "0fa18d2a2179c92efc22200ed6b3689e66cecf92",
        "files": [
            "pydicom/dataelem.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/839",
        "problem": "Ambiguous VR element could be read in <=1.1.0 but is broken in >=1.2.0\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nAttribute Error thrown when printing (0x0028, 0x0120) PixelPaddingValue\r\n\r\n#### Steps/Code to Reproduce\r\nUsing pydicom 1.2.2 and above (including master branch as of issue creation date):\r\n```\r\nfrom pydicom import dcmread\r\n\r\nds = dcmread('rtss.dcm')\r\nds\r\n\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 157, in correct_ambiguous_vr_element\r\n    _correct_ambiguous_vr_element(elem, ds, is_little_endian)\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 75, in _correct_ambiguous_vr_element\r\n    if ds.PixelRepresentation == 0:\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 711, in __getattr__\r\n    return super(Dataset, self).__getattribute__(name)\r\nAttributeError: 'FileDataset' object has no attribute 'PixelRepresentation'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 865, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/Users/apanchal/Projects/test.py\", line 107, in processing_thread\r\n    dp.ds, name, patientid, patientdob)\r\n  File \"/Users/apanchal/Projects/test.py\", line 144, in UpdateElements\r\n    for item in data:\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 1045, in __iter__\r\n    yield self[tag]\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 805, in __getitem__\r\n    self[tag], self, data_elem[6])\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 161, in correct_ambiguous_vr_element\r\n    raise AttributeError(reason)\r\nAttributeError: Failed to resolve ambiguous VR for tag (0028, 0120): 'FileDataset' object has no attribute 'PixelRepresentation'\r\n```\r\n\r\nAnonymized RTSTRUCT file is attached: [RTSTRUCT.zip](https://github.com/pydicom/pydicom/files/3124625/RTSTRUCT.zip)\r\n\r\n#### Expected Results\r\nThe dataset is printed. This worked in pydicom 1.1.0 and below.\r\n\r\nSince `PixelRepresentation` is not defined in the dataset, this attribute cannot be printed anymore.\r\n\r\nWhat's strange is that according to the standard PixelPaddingValue (0028, 0120) is 1C for RTSTRUCT, but in this file it has no other tags referencing PixelData. So it probably should not have been included by the vendor.\r\n\r\nI am wondering if there should be another path like in #809 that can handle the missing PixelRepresentation attribute.\r\n\r\n#### Actual Results\r\n```AttributeError: Failed to resolve ambiguous VR for tag (0028, 0120): 'FileDataset' object has no attribute 'PixelRepresentation'```\r\n\r\n#### Versions\r\n```\r\nDarwin-17.7.0-x86_64-i386-64bit\r\nPython 3.7.0 (default, Jul 23 2018, 20:22:55) \r\n[Clang 9.1.0 (clang-902.0.39.2)]\r\npydicom 1.2.2\r\n```\n",
        "hint": "Hm, in this case `PixelPadding` doesn't make sense, as there is no `PixelData` present, so we could just ignore it (e.g. use some default VR). With `PixelData` present, a missing `PixelRepresentation` would be a real DICOM violation, that would justify an exception as with the current behavior in my opinion. \r\nThe same is true for all tags with ambiguous VR dependent on `PixelRepresentation`.\r\nIf this is sufficient, I can put together a respective PR.  ",
        "base": "da9e0df8da28a35596f830379341a379de8ac439",
        "env": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "files": [
            "pydicom/filewriter.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/995",
        "problem": "Dataset.pixel_array doesn't change unless PixelData does\n#### Description\r\nCurrently `ds.pixel_array` produces a numpy array that depends on element values for Rows, Columns, Samples Per Pixel, etc, however the code for `ds.pixel_array` only changes the returned array if the value for `ds.PixelData` changes. This may lead to confusion/undesirable behaviour if the values for related elements are changed after `ds.pixel_array` is called but not the underlying pixel data.\r\n\r\nI can't think of any real use cases except maybe in an interactive session when debugging a non-conformant dataset, but I suggest we change the way `Dataset._pixel_id` is calculated so that it takes into account changes in related elements as well.\r\n\n",
        "hint": "",
        "base": "29be72498a4f4131808a45843b15692234ae7652",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/dataset.py",
            "pydicom/pixel_data_handlers/util.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1416",
        "problem": "Segmented LUTs are incorrectly expanded\n**Describe the bug**\r\n`pydicom.pixel_data_handlers.util._expand_segmented_lut()` expands segmented LUTs to an incorrect length.\r\n\r\n**Expected behavior**\r\nA correct length LUT to be produced.\r\n\r\n**Steps To Reproduce**\r\nInitialize the following variables.\r\n```\r\nimport numpy as np\r\nlength = 48\r\ny0 = 163\r\ny1 = 255\r\n```\r\n\r\nRun the following two lines from [`pydicom.pixel_data_handlers.util._expand_segmented_lut()`](https://github.com/pydicom/pydicom/blob/699c9f0a8e190d463dd828822106250523d38154/pydicom/pixel_data_handlers/util.py#L875\r\n)\r\n```\r\nstep = (y1 - y0) / length\r\nvals = np.around(np.arange(y0 + step, y1 + step, step))\r\n```\r\n\r\nConfirm that variable `vals` if of incorrect length\r\n```\r\nprint(len(vals) == length)\r\n> False\r\n```\r\n\r\nAlternatively, the code below produces similarly false results\r\n\r\n```\r\nfrom pydicom.pixel_data_handlers.util import _expand_segmented_lut \r\nlut = _expand_segmented_lut(([0, 1, 163, 1, 48, 255]), \"B\")\r\nprint(len(lut) == (1+48))\r\n> False\r\n```\r\n\r\n`np.arange` [explicitly states](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) that it's \"results will often not be consistent\" when using \"non-integer step\", which is a very possible scenario in this function. The following alternative code does function correctly:\r\n\r\n```\r\nvals = np.around(np.linspace(y0 + step, y1, length))\r\n```\r\n\r\n**Your environment**\r\n```bash\r\n$ python -m pydicom.env_info\r\nmodule       | version\r\n------       | -------\r\nplatform     | Darwin-20.5.0-x86_64-i386-64bit\r\nPython       | 3.7.10 (default, Feb 26 2021, 10:16:00)  [Clang 10.0.0 ]\r\npydicom      | 2.1.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.20.3\r\nPIL          | 8.2.0\r\n```\n",
        "hint": "PRs welcome!\r\n\r\nMore test data, too",
        "base": "699c9f0a8e190d463dd828822106250523d38154",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/pixel_data_handlers/util.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1000",
        "problem": "Heuristic for Explicit VR acting in sequence datasets\n**Describe the bug**\r\nThere is a check to confirm implicit VR by looking for two ascii characters and switching to explicit with a warning (#823).  It was thought this was safe because length in first data elements would not be that large.  However, in sequence item datasets this may not be true.\r\n\r\nNoted in google group conversation at https://groups.google.com/forum/#!topic/pydicom/VUmvUYmQxc0 (note that the title of that thread is not correct. that was not the problem).\r\n\r\nTest demonstrating it and fix already done - PR to follow shortly.\r\n\n",
        "hint": "",
        "base": "14acdd4d767ee55e8e84daeb6f80f81ef4748fee",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/filereader.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1069",
        "problem": "apply_color_lut() incorrect exception when missing RedPaletteColorLUTDescriptor\n**Describe the bug**\r\n`AttributeError` when used on a dataset without `RedPaletteColorLookupTableDescriptor `\r\n\r\n**Expected behavior**\r\nShould raise `ValueError` for consistency with later exceptions\r\n\r\n**Steps To Reproduce**\r\n```python\r\nfrom pydicom.pixel_data_handlers.util import apply_color_lut\r\nds = dcmread(\"CT_small.dcm\")\r\narr = ds.apply_color_lut(arr, ds)\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \".../pydicom/pixel_data_handlers/util.py\", line 116, in apply_color_lut\r\n    lut_desc = ds.RedPaletteColorLookupTableDescriptor\r\n  File \".../pydicom/dataset.py\", line 768, in __getattr__\r\n    return object.__getattribute__(self, name)\r\nAttributeError: 'FileDataset' object has no attribute 'RedPaletteColorLookupTableDescriptor'\r\n```\n",
        "hint": "",
        "base": "30ac743bcaedbc06f0e0d5cef590cb173756eb2d",
        "env": "5098c9147fadcb3e5918487036867931435adeb8",
        "files": [
            "pydicom/pixel_data_handlers/util.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1439",
        "problem": "Exception decompressing RLE encoded data with non-conformant padding\nGetting Following error \r\n\"Could not convert:  The amount of decoded RLE segment data doesn't match the expected amount (786433 vs. 786432 bytes)\"\r\nFor following code\r\nplt.imsave(os.path.join(output_folder,file)+'.png', convert_color_space(ds.pixel_array, ds[0x28,0x04].value, 'RGB'))\r\nAlso attaching DICOM file \r\n[US.1.2.156.112536.1.2127.130145051254127131.13912524190.144.txt](https://github.com/pydicom/pydicom/files/6799721/US.1.2.156.112536.1.2127.130145051254127131.13912524190.144.txt)\r\n\r\nPlease remove .txt extension to use DICOM file\r\n\r\n\r\n\n",
        "hint": "It looks like the RLE segment has been padded out to 786433 bytes even though:\r\n\r\n> Each RLE segment must be an even number of bytes or padded at its end with zero to make it even.\r\n\r\nThe solution would be to only raise an exception if the segment is shorter than expected and trim excess padding.\r\n\r\nThere's no simple workaround, but you should be able to do:\r\n```python\r\nfrom pydicom import dcmread\r\nfrom pydicom.pixel_data_handlers import rle_handler\r\nfrom pydicom.pixel_data_handlers.rle_handler import _parse_rle_header, _rle_decode_segment\r\n\r\ndef frame_decoder(data, rows, columns, nr_samples, nr_bits, segment_order = '>'):\r\n    if nr_bits % 8:\r\n        raise NotImplementedError(\r\n            \"Unable to decode RLE encoded pixel data with a (0028,0100) \"\r\n            f\"'Bits Allocated' value of {nr_bits}\"\r\n        )\r\n\r\n    offsets = _parse_rle_header(data[:64])\r\n    nr_segments = len(offsets)\r\n\r\n    bytes_per_sample = nr_bits // 8\r\n    if nr_segments != nr_samples * bytes_per_sample:\r\n        raise ValueError(\r\n            \"The number of RLE segments in the pixel data doesn't match the \"\r\n            f\"expected amount ({nr_segments} vs. \"\r\n            f\"{nr_samples * bytes_per_sample} segments)\"\r\n        )\r\n\r\n    offsets.append(len(data))\r\n\r\n    decoded = bytearray(rows * columns * nr_samples * bytes_per_sample)\r\n\r\n    stride = bytes_per_sample * rows * columns\r\n    for sample_number in range(nr_samples):\r\n        le_gen = range(bytes_per_sample)\r\n        byte_offsets = le_gen if segment_order == '<' else reversed(le_gen)\r\n        for byte_offset in byte_offsets:\r\n            ii = sample_number * bytes_per_sample + byte_offset\r\n            segment = _rle_decode_segment(data[offsets[ii]:offsets[ii + 1]])\r\n            if len(segment) < rows * columns:\r\n                raise ValueError(\r\n                    \"The amount of decoded RLE segment data doesn't match the \"\r\n                    f\"expected amount ({len(segment)} vs. \"\r\n                    f\"{rows * columns} bytes)\"\r\n                )\r\n\r\n            if segment_order == '>':\r\n                byte_offset = bytes_per_sample - byte_offset - 1\r\n\r\n            start = byte_offset + (sample_number * stride)\r\n            decoded[start:start + stride:bytes_per_sample] = segment[:rows * columns]\r\n\r\n    return decoded\r\n\r\nrle_handler._rle_decode_frame = frame_decoder\r\n\r\nds = dcmread('path/to/file.dcm')\r\nds.decompress('rle')\r\narr = ds.pixel_array\r\n```\r\n",
        "base": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/pixel_data_handlers/rle_handler.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/900",
        "problem": "can open my dicom, error in re.match('^ISO[^_]IR', encoding)\n```\r\n(test) root@DESKTOP-COPUCVT:/mnt/e/test# python3 mydicom.py\r\nTraceback (most recent call last):\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 625, in convert_encodings\r\n    py_encodings.append(python_encoding[encoding])\r\nKeyError: 73\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"mydicom.py\", line 12, in <module>\r\n    pydicom.dcmread(\"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\")\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 850, in dcmread\r\n    force=force, specific_tags=specific_tags)\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 728, in read_partial\r\n    specific_tags=specific_tags)\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 382, in read_dataset\r\n    encoding = convert_encodings(char_set)\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 628, in convert_encodings\r\n    _python_encoding_for_corrected_encoding(encoding))\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 647, in _python_encoding_for_corrected_encoding\r\n    if re.match('^ISO[^_]IR', encoding) is not None:\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/re.py\", line 172, in match\r\n    return _compile(pattern, flags).match(string)\r\nTypeError: expected string or bytes-like object\r\n```\r\n\r\n#### Description\r\n I dont know why pydicom cant open my pictures, but other python library can read the picture and read some meta data correctly. I suspect \" if re.match('^ISO[^_]IR', encoding) is not None:\"  the encoding here is not string for my dicom picture.   I am new to pydicom, \r\nHas anyone encountered a similar problem? how to solve it?  need help,thanks!\r\n\r\nhere is some dicom tags:\r\n![image](https://user-images.githubusercontent.com/32253100/61868213-8016f500-af0b-11e9-8736-8703230229cf.png)\r\n![image](https://user-images.githubusercontent.com/32253100/61868247-91600180-af0b-11e9-8767-a4045e901b8f.png)\r\n![image](https://user-images.githubusercontent.com/32253100/61868284-a50b6800-af0b-11e9-88fd-10180e0acf56.png)\r\n\r\n\r\n\r\n#### Steps/Code to Reproduce\r\n```py\r\n\r\nimport pydicom\r\nimport os\r\nimport numpy\r\nchild_path = \"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\"\r\npydicom.dcmread(\"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\"\uff09\r\n\r\n```\r\n\r\n#### Expected Results\r\nExample: read the file without error\r\n\r\n#### Actual Results\r\ncant read the file\r\n\r\n#### Versions\r\nv1.3.0\r\n\r\npython v3.6\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
        "hint": "The traceback is not really conclusive - apparently there is a syntax error somewhere in `mydicom.py`.\r\nCan you show the contents that code please? The traceback seems not to match the lines you gave below. \r\n\r\nIn the DICOM file I cannot see anything out of the order, apart from ProtocolName being in the wrong encoding (the encoding is latin1, the ProtocolName in some other, possibly Chinese encoding).\nThe most recent traceback is the one that's important I think. Can you compress the anonymised DICOM file as a zip and attach it as well?",
        "base": "3746878d8edf1cbda6fbcf35eec69f9ba79301ca",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/dataelem.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/897",
        "problem": "Inconsistencies in value testing for PersonName3\n```python\r\nfrom pydicom.dataset import Dataset\r\n\r\nds = Dataset()\r\nds.PatientName = None  # or ''\r\nif ds.PatientName:\r\n    print('Has a value')\r\nelse:\r\n    print('Has no value')\r\n\r\nif None:  # or ''\r\n    print('Evaluates as True')\r\nelse:\r\n    print('Evaluates as False')\r\n```\r\nPrints `Has a value` then `Evaluates as False`. Should print `Has no value` instead (encoded dataset will have a zero-length element).\r\n\r\nCurrent master, python 3.6.\n",
        "hint": "Should that `if None` read `if ds.PatientName is None`?\r\n\r\nI can see the inconsistency if assigning `None` - which results in an empty value instead of `None`. Assigning an empty string works as expected (has an empty string afterwards).\r\nFor other string tags `None` or `''` can be assigned, though I now wonder if this is the correct behavior, as there is no difference between an empty value and no value in DICOM for string VRs. IMHO, the correct behavior should be to assign no value, e.g. the following should be equivalent:\r\n```python\r\nds.PatientName = ''\r\nds.PatientName = None\r\nds.PatientName = []\r\n```\r\nCurrently, the first 2 result in VM=1, the last in VM=0 (which is correct).\r\n@darcymason, @scaramallion - what do you think?\nThe `if None` was just to demonstrate how python behaves and hence how I naively expect an empty pydicom element value to behave.\r\n\r\nI just want a simple consistent \"does this element have a value\" test, which means an element with `None` and `''` should both fail the test. Essentially if the element will end up encoded with zero-length it should fail. It works as I expect for simple text element values, but `PersonName3` and maybe some of the other VR classes might need `__bool__()` or `__len__()`?\r\n\r\n```python\r\n# I'd like this to work cleanly\r\nfor elem in ds:\r\n   if elem.value:\r\n       print(elem.value)\r\n   else:\r\n       print('(no value available)')\r\n```\r\n\r\nI've been playing around with printing out pretty datasets for pynetdicom and have run into a some problems with consistency in how element values are evaluated and printed.\r\n\r\nI think you're right, they should be VM = 0. I think the last case gives you a `MultiValue` rather than a `PersonName3` which is probably why.\n```python\r\n#!/usr/bin/env python\r\n\r\nfrom pydicom.dataset import Dataset\r\nfrom pydicom.datadict import tag_for_keyword\r\n\r\nVRs = {\r\n    'AE' : 'Receiver',\r\n    'AS' : 'PatientAge',\r\n    'AT' : 'OffendingElement',\r\n    'CS' : 'QualityControlSubject',\r\n    'DA' : 'PatientBirthDate',\r\n    'DS' : 'PatientWeight',\r\n    'DT' : 'AcquisitionDateTime',\r\n    'IS' : 'BeamNumber',\r\n    'LO' : 'DataSetSubtype',\r\n    'LT' : 'ExtendedCodeMeaning',\r\n    'PN' : 'PatientName',\r\n    'SH' : 'CodeValue',\r\n    'ST' : 'InstitutionAddress',\r\n    'TM' : 'StudyTime',\r\n    'UC' : 'LongCodeValue',\r\n    'UI' : 'SOPClassUID',\r\n    'UR' : 'CodingSchemeURL',\r\n    'UT' : 'StrainAdditionalInformation',\r\n    'SL' : 'RationalNumeratorValue',\r\n    'SS' : 'SelectorSSValue',\r\n    'UL' : 'SimpleFrameList',\r\n    'US' : 'SourceAcquisitionBeamNumber',\r\n    'FD' : 'RealWorldValueLUTData',\r\n    'FL' : 'VectorAccuracy',\r\n    'OB' : 'FillPattern',\r\n    'OD' : 'DoubleFloatPixelData',\r\n    'OF' : 'UValueData',\r\n    'OL' : 'TrackPointIndexList',\r\n    'OW' : 'TrianglePointIndexList',\r\n    #'OV' : '',\r\n    'UN' : 'SelectorUNValue',\r\n}\r\n\r\nds = Dataset()\r\nfor vr, kw in VRs.items():\r\n    try:\r\n        setattr(ds, kw, None)\r\n    except:\r\n        print('Failed assigning None', vr)\r\n        continue\r\n\r\n    elem = ds[tag_for_keyword(kw)]\r\n\r\n    try:\r\n        assert bool(elem.value) is False\r\n    except:\r\n        print('Failed empty value test', vr)\r\n\r\n    try:\r\n        assert elem.VM == 0\r\n    except Exception as exc:\r\n        print('Failed VM == 0', vr, elem.VM)\r\n```\r\nUI fails assignment using `None`, they all fail the VM == 0 test, PN fails the value test in Python 3.6, passes in 2.7.\nPN in Python 2 is derived from `unicode`, so this is to be expected. So you agree that VM shall be 0 after assigning `None` or `''`?\n> So you agree that VM shall be 0 after assigning None or ''?\r\n\r\nAs far as I can see the standard doesn't explicitly say so, but I think that's reasonable. DCMTK's `dcmdump` uses that convention.\r\n\r\nWhat do you think @darcymason?\n> I think you're right, they should be VM = 0. I think the last case gives you a MultiValue rather than a PersonName3 which is probably why.\r\n\r\nSorry, I overlooked that post. Yes, and that may be the easiest way to change this - assign an empty `MultiValue` (aka an empty list) in these cases instead of `None` or `''`.\nThere is also the question of how to represent empty values. Currently we get:\r\n```\r\n>>> ds.CodeValue = None\r\n>>> ds.CodeValue\r\n>>> print(ds.CodeValue)\r\nNone\r\n>>> ds['CodeValue'].VM\r\n1\r\n>>> ds.CodeValue = []\r\n>>> ds.CodeValue\r\n['']\r\n>>> ds['CodeValue'].VM\r\n0\r\n```\r\nwhich is quite inconsistent. `None` for an empty value may make sense, or an empty list (other than a list with an empty string), not sure about this. Any ideas?\nI think as long as `bool(value)` would normally evaluate as `False` then it should be considered empty.\r\n```python\r\nvalue = '' # or None or [] or b'' or whatever\r\nif not value:\r\n    print('Empty')\r\n```\r\nFor `MultiValue` it should probably be an OR on `[bool(val) for val in MultiValue]`\n> > So you agree that VM shall be 0 after assigning None or ''?\r\n> \r\n> As far as I can see the standard doesn't explicitly say so, but I think that's reasonable. DCMTK's `dcmdump` uses that convention.\r\n> \r\n> What do you think @darcymason?\r\n\r\nHmmm... just read quickly through, so apologies if I've repeated (or missed) something already said.  Haven't looked at any code yet.\r\n\r\nOne thought is that DICOM type 2 data elements allow blank values, which to me is a real valid value, different than a None.\r\n\r\nSecond thought is we've normally handled a non-existent value by just not having the attribute set, i.e. just don't set the attribute, or if it already exists, delete it.  You check for existence of values by `if keyword in ds`.\r\n\r\nThird thought (the last in this comment I think) is that when we drop support for earlier python versions and can start typing pydicom, do we cause trouble by encouraging use of None?  We have not forbidden setting None, or even incorrect types in many cases, but this is in line with python's duck typing philosophy.  It may be that at some point more control over this could be automatically introduced via typing. Yes, we could Union everything with None, but I would foresee a whole lot of `if` statements to handle those cases.\r\n\r\n\nI frequently use `None` for type 2 elements that are required to be present but may or may not have a value. It just seems to me that this is the 'obvious' thing to do when I want an element with no value. Does it make sense that you need to set an integer VR to `''` if you want an empty value?\r\n\r\n`if keyword in ds` tests for the presence of the element in the dataset, not that the element has a value. \nWell, thinking this through some more, I may be coming around to your way of thinking...I suppose blank type 2 can be considered a zero-length (no value) data element.  Using a null (python None) value for those does have a certain logic to it, similar to Null in databases and so on.  And in that case, VM should be zero, although VM is a weak concept to me anyway, since it exists in dictionaries but not in actual DICOM files, so I'm not too invested in that one way or the other.\nThe other thing is that some elements don't encode properly with a value set to `None`, while others work as I expect.\r\n\r\n```python\r\n>>> from pydicom.dataset import Dataset\r\n>>> from pynetdicom.dsutils import encode  # wrapper for filewriter.write_dataset\r\n>>> ds = Dataset()\r\n>>> ds.Receiver = None  # VR of AE\r\n>>> type(ds.Receiver)\r\nNoneType\r\n>>> encode(ds, True, True)  # Works OK\r\nb'\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00'\r\n>>> ds = Dataset()\r\n>>> ds.BitsStored = None  # VR of US\r\n>>> type(ds.BitsStored)\r\nNoneType\r\n>>> encode(ds, True, True)  # Fails to encode\r\n>>>\r\n```\nSo I think this has elevated to a larger conversation on \"`None` support\" in pydicom, make it consistent across all types, or raise exceptions if there are any types or situations where it should not be allowed.\r\n\r\nI'll sleep on it for now, pick it up again tomorrow...\r\n\nMy current instinct is to:\r\n- handle assigning `None`, `''` or `[]` to an element with a text VR as assigning an empty string (as they are all equivalent, and an empty string is most convienient for the user, and probably expected), return `False` as truth value, and have a VM of 0 (this is as implemented in the PR) \r\n- handle assigning `None` or `[]` to an element with a binary VR as assigning `None` (I think this better meets the expections for an element without a value), return `False` as truth value, and have a VM of 0 (in the PR I use a `MultiValue` object, which I would change)\r\n- in the case of IS and DS (these are kind of in-between), we have to make sure they handle the empty string case by returning `None` instead of 0 if getting the numerical value (haven't checked the current behavior) \r\n\r\nUsing `None` for text values, apart from not being upwards compatible, is a thing I have been burned with before. I used to write DICOM elements into a database and had used database `null` to represent elements without values. This turned out to be a big mistake, as there is actually no use case where you have to differentiate no value from an empty value for text values (and indeed there is no such distinction in DICOM itself for text VRs), and led to a lot of unnecessary checking for null values in the user code (and exceptions if they have been missing).\nI've started a separate issue for the empty values discussion #896",
        "base": "b682e1dc71eda183786ad724da25f2fb30b5a621",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/dataelem.py",
            "pydicom/multival.py",
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/903",
        "problem": "Handling of DS too long to be encoded in explicit encoding\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nThis is probably not a bug, but I'm not sure about the wanted behavior.\r\nAn RTPlan dataset encoded as Little Endian Implicit contains multiple values in the DS tag DHV Data (3004,0058) with an overall length not fitting into 2 bytes. Trying to write this as explicit Little Endian fails with an exception (`\"ushort format requires 0 &lt;= number &lt;= (0x7fff * 2 + 1)\"`) which is raised by the `pack` call in `write_leUS` while trying to write the length.\r\n\r\nThe standard says for this case in PS3.5, Table 6.2-1 (for VR DS):\r\n```\r\nNote\r\nData Elements with multiple values using this VR may not be properly encoded if Explicit-VR transfer syntax is used and the VL of this attribute exceeds 65534 bytes.\r\n```\r\nSo, as I understand it, this is valid DICOM, that cannot be converted to explicit encoding without data loss.\r\nThe question is how to handle this. What comes to mind:\r\n- truncate the value and log a warning\r\n- raise a meaningful exception\r\n- adapt the behavior depending on some config setting\r\n\r\nAny thoughts?\r\n\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n\r\n#### Steps/Code to Reproduce\r\n<!--\r\nExample:\r\n```py\r\nfrom io import BytesIO\r\nfrom pydicom import dcmread\r\n\r\nbytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\r\n             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\r\n             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\r\n             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\r\n             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\r\n\r\nfp = BytesIO(bytestream)\r\nds = dcmread(fp, force=True)\r\n\r\nprint(ds.PatientID)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n\r\nWhen possible use pydicom testing examples to reproduce the errors. Otherwise, provide\r\nan anonymous version of the data in order to replicate the errors.\r\n-->\r\n\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
        "hint": "Interesting situation.  To summarize:  with implicit VR and the four-byte length it uses, we can read in something longer than **Ex**plicit VR can write, because it uses only two bytes for encoding the length (for VRs like DS).\r\n\r\nI say we catch the exception and re-raise with a meaningful error message.  If the user wants to truncate the data to avoid this, they can choose how to do so, i.e. what to leave out.  Or they can save the dataset using Implicit VR.  The error message could explain those two options.\r\n\r\n\n> I say we catch the exception and re-raise with a meaningful error message\r\n\r\nAgreed - I will have a go at this some time later. \r\nI have to admit that I was quite surprised to see this behavior - while I did know that implicit transfer syntax has other length fields, I never thought that this would ever matter.\nReopen as the fix is incorrect - the VR shall be changed to UN instead (see #900).",
        "base": "88ca0bff105c75a632d7351acb6895b70230fa12",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/filewriter.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1033",
        "problem": "to_json does not work with binary data in pixel_array\n**Describe the issue**\r\nLoading a dicom file and then performing a to_json() on it does not work with binary data in pixel_array.\r\n\r\n\r\n\r\n**Expected behavior**\r\nI would have expected that a base64 conversion is first performed on the binary data and then encoded to json. \r\n\r\n**Steps To Reproduce**\r\nHow to reproduce the issue. Please include:\r\n1. A minimum working code sample\r\n\r\nimport pydicom\r\nds = pydicom.dcmread('path_to_file')\r\noutput = ds.to_json()\r\n\r\n\r\n2. The traceback (if one occurred)\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.virtualenvs/my_env/lib/python3.7/site-packages/pydicom/dataset.py\", line 2003, in to_json\r\n    dump_handler=dump_handler\r\n  File \"/.virtualenvs/my_env/lib/python3.7/site-packages/pydicom/dataset.py\", line 1889, in _data_element_to_json\r\n    binary_value = data_element.value.encode('utf-8')\r\nAttributeError: 'bytes' object has no attribute 'encode'\r\n\r\n\r\n3. Which of the following packages are available and their versions:\r\n  * Numpy\r\nnumpy==1.17.2\r\n  * Pillow\r\nPillow==6.1.0\r\n  * JPEG-LS\r\n  * GDCM\r\n4. The anonymized DICOM dataset (if possible).\r\n\r\n**Your environment**\r\nPlease run the following and paste the output.\r\n```bash\r\n$ python -c \"import platform; print(platform.platform())\"\r\nDarwin-19.2.0-x86_64-i386-64bit\r\n$ python -c \"import sys; print('Python ', sys.version)\"\r\nPython  3.7.6 (default, Dec 30 2019, 19:38:26) \r\n[Clang 11.0.0 (clang-1100.0.33.16)]\r\n$ python -c \"import pydicom; print('pydicom ', pydicom.__version__)\"\r\npydicom  1.3.0\r\n```\r\n\n",
        "hint": "Can you please check with pydicom 1.4? Binary data handling should have been fixed there. \nok works now once I set the bulk_data_threshold value to a higher value. \r\n\r\nThank you!\nOk, that may be an issue with the data size. Currently, the default for `bulk_data_threshold` is 1, if I remember correctly, which may not be the best value - meaning that all binary data larger than that expect a bulk data element handler. Setting the threshold to a large value shall fix this, if the data is encoded directly.\nAh, you already found that, ok!\r\n\nyep. thanks, works now. \n@darcymason - json support is still flagged as alpha - I think we can consider it at least beta now, and add a small section in the documentation.\r\nWe may also rethink the `bulk_data_threshold` parameter - maybe just ignore it if no bulk data handler is set, and set the default value to some sensible value (1kB or something).\n> @darcymason - json support is still flagged as alpha - I think we can consider it at least beta now, and add a small section in the documentation.\r\n> We may also rethink the `bulk_data_threshold` parameter - maybe just ignore it if no bulk data handler is set, and set the default value to some sensible value (1kB or something).\r\n\r\nI agree that updating the documentation is a good idea for this. As its pretty common to have binary image data in dicom files, and its guaranteed to fail with the default value for bulk_data_threshold\r\n\r\nThanks once again though!\nPing @pieper, @hackermd for comment about `bulk_data_threshold`.\n> We may also rethink the bulk_data_threshold parameter - maybe just ignore it if no bulk data handler is set, and set the default value to some sensible value (1kB or something).\r\n\r\n1k threshold makes sense to me.  \ud83d\udc4d ",
        "base": "701c1062bf66de2b29df68fa5540be6009943885",
        "env": "5098c9147fadcb3e5918487036867931435adeb8",
        "files": [
            "pydicom/dataelem.py",
            "pydicom/dataset.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1076",
        "problem": "Error writing values with VR OF\n[Related to this comment](https://github.com/pydicom/pydicom/issues/452#issuecomment-614038937) (I think)\r\n\r\n```python\r\nfrom pydicom.dataset import Dataset\r\nds = Dataset()\r\nds.is_little_endian = True\r\nds.is_implicit_VR = True\r\nds.FloatPixelData = b'\\x00\\x00\\x00\\x00'\r\nds.save_as('out.dcm')\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \".../pydicom/filewriter.py\", line 228, in write_numbers\r\n    value.append  # works only if list, not if string or number\r\nAttributeError: 'bytes' object has no attribute 'append'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \".../pydicom/filewriter.py\", line 230, in write_numbers\r\n    fp.write(pack(format_string, value))\r\nstruct.error: required argument is not a float\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \".../pydicom/tag.py\", line 27, in tag_in_exception\r\n    yield\r\n  File \".../pydicom/filewriter.py\", line 543, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \".../pydicom/filewriter.py\", line 472, in write_data_element\r\n    writer_function(buffer, data_element, writer_param)\r\n  File \".../pydicom/filewriter.py\", line 236, in write_numbers\r\n    \"{0}\\nfor data_element:\\n{1}\".format(str(e), str(data_element)))\r\nOSError: required argument is not a float\r\nfor data_element:\r\n(7fe0, 0008) Float Pixel Data                    OF: b'\\x00\\x00\\x00\\x00'\r\n\r\n[skip]\r\n```\r\n[Error in filewriter](https://github.com/pydicom/pydicom/blob/master/pydicom/filewriter.py#L1007) using `write_numbers` instead of `write_OBvalue`/`write_OWvalue`. Looks like it's been wrong [since 2008](https://github.com/pydicom/pydicom/commit/5d3ea61ffe6877ae79267bf233f258c07c726998). I'm a bit surprised it hasn't come up before.\n",
        "hint": "",
        "base": "caf0db105ddf389ff5025937fd5f3aa1e61e85e7",
        "env": "5098c9147fadcb3e5918487036867931435adeb8",
        "files": [
            "pydicom/filewriter.py",
            "pydicom/values.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1458",
        "problem": "Pixel Representation attribute should be optional for pixel data handler\n**Describe the bug**\r\nThe NumPy pixel data handler currently [requires the Pixel Representation attribute](https://github.com/pydicom/pydicom/blob/8da0b9b215ebfad5756051c891def88e426787e7/pydicom/pixel_data_handlers/numpy_handler.py#L46). This is problematic, because in case of Float Pixel Data or Double Float Pixel Data the attribute shall be absent. Compare [Floating Point Image Pixel Module Attributes](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.24.html) versus [Image Pixel Description Macro Attributes](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.3.html#table_C.7-11c)\r\n\r\n**Expected behavior**\r\nI would expect the `Dataset.pixel_array` property to be able to decode a Float Pixel Data or Double Float Pixel Data element without presence of the Pixel Representation element in the metadata.\r\n\r\n**Steps To Reproduce**\r\n```python\r\nimport numpy as np\r\nfrom pydicom.dataset import Dataset, FileMetaDataset\r\n\r\n\r\nds = Dataset()\r\nds.file_meta = FileMetaDataset()\r\nds.file_meta.TransferSyntaxUID = '1.2.840.10008.1.2.1'\r\n\r\nds.BitsAllocated = 32\r\nds.SamplesPerPixel = 1\r\nds.Rows = 5\r\nds.Columns = 5\r\nds.PhotometricInterpretation = 'MONOCHROME2'\r\n\r\npixel_array = np.zeros((ds.Rows, ds.Columns), dtype=np.float32)\r\nds.FloatPixelData = pixel_array.flatten().tobytes()\r\n\r\nnp.array_equal(ds.pixel_array, pixel_array)\r\n```\n",
        "hint": "",
        "base": "8da0b9b215ebfad5756051c891def88e426787e7",
        "env": "0fa18d2a2179c92efc22200ed6b3689e66cecf92",
        "files": [
            "pydicom/pixel_data_handlers/numpy_handler.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1720",
        "problem": "Strict adherence to VR during parsing is detrimental due to commonplace vendor interpretations\n**Describe the bug**\r\nDICOM Files from GE modalities, which when parsed, raise a TypeError caused by \"violating\" the VR imposed by the DICOM standard; however, real world modalities have and continue to generate such files for good cause.\r\n\r\nFor example the following is raised\r\n\r\n`TypeError('Could not convert value to integer without loss')`\r\n\r\nby a real world DICOM file which has a value\r\n\r\n`(0018,1152) IS [14.5]                                   #   4, 1 Exposure`\r\n\r\nwhere IS is a Value Representation defined as\r\n\r\n> IS - Integer String\r\n\r\n> A string of characters representing an Integer in base-10 (decimal), shall contain only the characters 0 - 9, with an optional leading \"+\" or \"-\". It may be padded with leading and/or trailing spaces. Embedded spaces are not allowed.\r\n\r\n> The integer, n, represented shall be in the range: -231<= n <= (231-1).\r\n\r\n[See DICOM Part 5 Section 6.2](https://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html)\r\n\r\nwhich means `14.5` is an invalid value due to the fractional portion .5 which definitely would lead to a loss in precision if converted to a pure integer value (of 14). \r\n\r\nAfter discussion with a senior engineer for the vendor, the following dialogue was obtained which quotes an article by David Clune, a well-respected, long-time member of the DICOM committee and community:\r\n\r\n> The tag pair in question is meant to contain the mAs value used for the exposure, which is not constrained to integer values, but for some reason the DICOM standard defines it as such.\r\n\r\n> An interesting article from someone responsible for maintaining the DICOM documentation explains the conundrum quite well:  \r\n\r\nhttp://dclunie.blogspot.com/2008/11/dicom-exposure-attribute-fiasco.html\r\n\r\n> Of note are two excerpts from that article:\r\n\r\n> \"The original ACR-NEMA standard specified ASCII numeric data elements for Exposure, Exposure Time and X-Ray Tube Current that could be decimal values; for no apparent reason DICOM 3.0 in 1993 constrained these to be integers, which for some modalities and subjects are too small to be sufficiently precise\"\r\n\r\n> and\r\n\r\n> \"The authors of DICOM, in attempting to maintain some semblance of backward compatibility with ACR-NEMA and at the same time apply more precise constraints, re-defined all ACR-NEMA data elements of VR AN as either IS or DS, the former being the AN integer numbers (with new size constraints), and the latter being the AN fixed point and floating point numbers. In the process of categorizing the old data elements into either IS or DS, not only were the obvious integers (like counts of images and other things) made into integers, but it appears that also any \"real world\" attribute that in somebody's expert opinion did not need greater precision than a whole integer, was so constrained as well.\"\r\n\r\n> I have inspected a few random DICOM files generated by various modalities and the value is stored accurately, even though it is a violation of the explicit value representation. Additionally, I have worked with (and support) various PACS platforms, and this is the first time this has been raised as an issue. So technically, you are correct that encoding that value as decimal violates the explicit VR, but it appears to be common practice to do so. \r\n\r\n**Expected behavior**\r\nTo deal with the reality of history with respect to the current standard, my opinion, as a long-standing DICOM PACS implementer at Medstrat, is that there is nothing to gain and everything to lose by raising a `TypeError` here. For cases where an integer VR, such as `IS`, could be read as a floating point number instead, then it should be allowed to be so, for at least a limited whitelist of tags.\r\n\r\nArguments against which come to mind are of the ilk that do not heed \"Although practicality beats purity\" as can be read if you \r\n\r\n[`>>> import this`](https://peps.python.org/pep-0020/)\r\n\r\n> Special cases aren't special enough to break the rules.\r\n> Although practicality beats purity.\r\n\r\n**Steps To Reproduce**\r\n\r\n`(0018,1152) IS [14.5]                                   #   4, 1 Exposure`\r\n\r\nSet any DICOM file to have the above for `Exposure` and then do this:\r\n\r\n```\r\n>>> from pydicom import config\r\n>>> pydicom.__version__\r\n'2.3.0'\r\n>>> config.settings.reading_validation_mode = config.IGNORE\r\n>>> ds = pydicom.dcmread('1.2.840.113619.2.107.20220429121335.1.1.dcm')\r\n>>> ds\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 2306, in __str__\r\n    return self._pretty_str()\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 2020, in _pretty_str\r\n    for elem in self:\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 1240, in __iter__\r\n    yield self[tag]\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 939, in __getitem__\r\n    self[tag] = DataElement_from_raw(elem, character_set, self)\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataelem.py\", line 859, in DataElement_from_raw\r\n    value = convert_value(vr, raw, encoding)\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/values.py\", line 771, in convert_value\r\n    return converter(byte_string, is_little_endian, num_format)\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/values.py\", line 348, in convert_IS_string\r\n    return MultiString(num_string, valtype=pydicom.valuerep.IS)\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/valuerep.py\", line 1213, in MultiString\r\n    return valtype(splitup[0])\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/valuerep.py\", line 1131, in __new__\r\n    raise TypeError(\"Could not convert value to integer without loss\")\r\nTypeError: Could not convert value to integer without loss\r\n```\r\n\r\n**Your environment**\r\n\r\n```bash\r\nmodule       | version\r\n------       | -------\r\nplatform     | Darwin-21.5.0-x86_64-i386-64bit\r\nPython       | 3.7.5 (v3.7.5:5c02a39a0b, Oct 14 2019, 18:49:57)  [Clang 6.0 (clang-600.0.57)]\r\npydicom      | 2.2.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | _module not found_\r\nPIL          | _module not found_\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n```\r\n\n",
        "hint": "Thank you for that thorough explanation (and of course we all know and respect David Clunie \ud83d\ude04 )!\r\nI understand the specific problem with `Exposure Time` (for a similar reason, there exists the tag `Exposure in \u03bcAs` additionally to `Exposure`, but no such thing exists for `Exposure Time`), and I am aware that this is not the only case where float values are written into `IS`. \r\n\r\nI think a similar issue came up before, and my preference would be to make this behavior dependent on the validation mode. The default validation mode for reading is to issue a warning, while for writing new data it is to raise an exception. This would not resolve the problem, though. The real problem is in the implementation of the tag class: the class representing `IS` values is derived from `int`. In my opinion, it would make sense to change that, though I'm not sure if we can do this consistently in a backwards-compatible way.\r\n\r\n@darcymason - I'm quite sure that we have discussed this before, I will try to find the respective issue. Anyway, what are your current thoughts here? \r\n\r\nCC @dclunie\r\n\nAh ok, @gsmethells has already [commented](https://github.com/pydicom/pydicom/issues/1643#issuecomment-1180789608) in the respective issue #1643.\n> I'm quite sure that we have discussed this before\r\n\r\nYes, it has come up several times.\r\n\r\nA new thought has come to me (maybe not thought through properly, or maybe someone has mentioned this before and I'm just thinking it is new): maybe we could create an ISFloat class, operating similarly to the DSFloat idea.  Then, just have an allow_IS_float config flag, perhaps even true by default (return an ISFloat only if it is not an exact int).  Python allows mixed math anyway, so I don't see any real problem in returning a non-int if further math is done.   Only problem might be code that did an `isinstance` check against `IS` but that should be very rare.\n> maybe we could create an ISFloat class, operating similarly to the DSFloat idea\r\n\r\nMy preference would be to change the `IS` class to support both `int` and `float`. We cannot always use float, as that could decrease the precision of integers, but in the case that float numbers are written in the tag, I would prefer to return a float. Not sure what problems this would bring, though... \r\nAdditionally, I think we can still couple the behavior to the validation mode, but I'm not completely sure here.\n> My preference would be to change the `IS` class to support both `int` and `float`.\r\n\r\nIs that actually possible to do, without recreating all the class methods for `int` (or `float`) for math operations?\n> Is that actually possible to do, without recreating all the class methods for int (or float) for math operations?\r\n\r\nI guess not - that would be the downside of that approach. Also an `isinstance(int)` would fail, of course. It is probably better to use `ISFloat` as you proposed and dynamically decide which class to use.\n> It is probably better to use ISFloat as you proposed and dynamically decide which class to use.\r\n\r\nActually that is what you have proposed - sorry, I misread that, I understood that you wanted to configure which class to use. Yes, I like your proposal!\n> Is that actually possible to do, without recreating all the class methods for int (or float) for math operations?\r\n\r\nActually, I feel like we did return a different class from `__new__` somewhere, or at least talked about it.  It turns out that it is possible to [return a different class from `__new__`](https://stackoverflow.com/questions/20221858/python-new-method-returning-something-other-than-class-instance).  I'm not sure it is advisable, though, that is really not being explicit to the user.\n> Actually, I feel like we did return a different class from `__new__` somewhere\r\n\r\nYes, we actually use this to return either a string or an int/float from `IS`/`DS`. This is also a common pattern in Python (they use it for example in `pathlib`), so I think it would not be unexpected.\nThank you for taking the time to discuss ideas and consider this. I understand wanting to adhere tightly to the standard (we do so for edge cases the majority of the time ourselves for our ortho PACS). I also understand the desire to listen to the \"import this\" zen of \"Simple is better than complex\" and, yes, I understand all too well how a large system can grow complicated.\r\n\r\nMy usual pause comes when my design desires run up against DICOM files in the field where there is a common violation of the standard. We maintain petabytes of DICOM images and this issue is common. I would be happy if reading/writing `dataset`s from/to files via pydicom continued to support maintaining existing values, regardless of whether those values violate the standard, (it does now, but is that an intentional design decision? A `dataset` only raises a `TypeError` when the value is directly attempted to be read out, whether via `__iter__` or otherwise) and also provided a (new/existing?) preference for reading `int` VR tag values as `float`.\r\n\r\nMy two cents.\n> reading/writing `dataset`s from/to files via pydicom continued to support maintaining existing values, regardless of whether those values violate the standard\r\n\r\nAny other position would become a deal breaker for our use of the library if it were ever otherwise, simply due to the realities of supporting customers. Especially when it comes to the need to update tags (e.g., a misspelled PatientName) while keeping other unrelated tags the same (i.e., no side-effects).\nSetting 2.4 milestone to at least consider this for the release.",
        "base": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680",
        "env": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680",
        "files": [
            "pydicom/config.py",
            "pydicom/dataset.py",
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/816",
        "problem": "LookupError: unknown encoding: Not Supplied\n#### Description\r\nOutput from `ds = pydicom.read_file(dcmFile)` (an RTSTRUCT dicom file, SOP UID 1.2.840.10008.5.1.4.1.1.481.3) results in some tags throwing a LookupError: \"LookupError: unknown encoding: Not Supplied\"\r\nSpecific tags which cannot be decoded are as follows:\r\n['DeviceSerialNumber',\r\n 'Manufacturer',\r\n 'ManufacturerModelName',\r\n 'PatientID',\r\n 'PatientName',\r\n 'RTROIObservationsSequence',\r\n 'ReferringPhysicianName',\r\n 'SeriesDescription',\r\n 'SoftwareVersions',\r\n 'StructureSetLabel',\r\n 'StructureSetName',\r\n 'StructureSetROISequence',\r\n 'StudyDescription',\r\n 'StudyID']\r\n\r\nI suspect that it's due to the fact that `ds.SpecificCharacterSet = 'Not Supplied'`, but when I try to set `ds.SpecificCharacterSet` to something reasonable (ie ISO_IR_100 or 'iso8859'), it doesn't seem to make any difference.\r\n\r\nReading the same file, with NO modifications, in gdcm does not result in any errors and all fields are readable.\r\n\r\n#### Steps/Code to Reproduce\r\n```py\r\nimport pydicom \r\nds = pydicom.read_file(dcmFile)\r\nprint(ds.PatientName)\r\n```\r\n\r\n#### Expected Results\r\nNo error is thrown and the name of the patient is printed.\r\n\r\n#### Actual Results\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 706, in __str__\r\n    return '='.join(self.components).__str__()\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 641, in components\r\n    self._components = _decode_personname(groups, self.encodings)\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 564, in _decode_personname\r\n    for comp in components]\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 564, in <listcomp>\r\n    for comp in components]\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\charset.py\", line 129, in decode_string\r\n    return value.decode(encodings[0])\r\nLookupError: unknown encoding: Not Supplied\r\n\r\n#### Versions\r\nPlatform: Windows-10-10.0.17763-SP0\r\nPython Version: Python 3.6.4 |Anaconda, Inc.| (default, Mar 12 2018, 20:20:50) [MSC v.1900 64 bit (AMD64)]\r\npydicom Version: pydicom 1.2.2\r\n\n",
        "hint": "You said on the pynetdicom issue you can't upload an anonymised file, but can you open the file in a hex editor and post the raw byte output from the first few (non-identifying) elements? From the start of the file to the end of say (0008,0070) should be enough.\r\n\r\nAlternatively you could truncate the file at the end of the (0008,0070) element and upload that.\r\n\r\nIf you need to know how to interpret the encoded data check out [Part 5, Chapter 7](http://dicom.nema.org/medical/dicom/current/output/chtml/part05/chapter_7.html) of the DICOM Standard. And if the file's been saved in the [DICOM File Format](http://dicom.nema.org/medical/dicom/current/output/chtml/part10/chapter_7.html) there may also be a 128 byte header  following by 'DICM' before the start of the dataset (which we don't need).\n> open the file in a hex editor and post the raw byte output from the first few (non-identifying) elements\r\n\r\nAlternatively:\r\n```\r\nimport pydicom.config\r\npydicom.config.debug(True)\r\nds = dcmread(youfilename)\r\n```\r\nAnd as suggested copy the first non-identifying part of the debug output for posting.\nPlease find the truncated dataset attached as requested. I wasn't allowed to upload a *.dcm file so just wrote it as a *.txt file. The file is readable by pydicom but exhibits the same aforementioned problems, where the \"LookupError: unknown encoding: Not Supplied\" happens only for the Manufacturer tag.\r\n\r\n[truncated.txt](https://github.com/pydicom/pydicom/files/2947883/truncated.txt)\r\n\nOkay, so I've tried deleting SpecificCharacterSet and the error still occurs.  I think pydicom is still holding on to the original values, and needs some code to handle the case when SpecificCharacterSet is deleted or set again after reading the file. We can dig into it a little further.\r\n\r\n\nWorkaround:\r\n```python\r\nfrom pydicom import dcmread\r\n\r\nds = dcmread('path/to/file')\r\ndel ds.SpecificCharacterSet\r\nds.read_encoding = []\r\n```\r\n\nBrilliant!  That worked! \r\n\r\nThank you for the quick fix! I've found pydicom is a lot more user-friendly than gdcm so I'm really glad I don't have to resort to that. \ud83d\ude00 \nNo problem, @mrbean-bremen do you want to handle the underlying issue?\nCertainly - as I may have introduced it... Not sure if I'll find the time today though. ",
        "base": "3551f5b5a5f8d4de3ed92e5e479ac8c74a8c893a",
        "env": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "files": [
            "pydicom/charset.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/916",
        "problem": "To_Json 'str' object has no attribute 'components'\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n\r\nWhen converting a dataset to json the following error occurs.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/anaconda3/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"~/pacs-proxy/pacs/service.py\", line 172, in saveFunction\r\n    jsonObj = ds.to_json()\r\n  File \"~/lib/python3.6/site-packages/pydicom/dataset.py\", line 2046, in to_json\r\n    dump_handler=dump_handler\r\n  File \"~/lib/python3.6/site-packages/pydicom/dataelem.py\", line 447, in to_json\r\n    if len(elem_value.components) > 2:\r\nAttributeError: 'str' object has no attribute 'components'\r\n```\r\n#### Steps/Code to Reproduce\r\n\r\nds = pydicom.dcmread(\"testImg\")\r\njsonObj = ds.to_json()\r\n\r\nI'm working on getting an anonymous version of the image, will update. But any advice, suggestions would be appreciated.\r\n\r\n#### \n",
        "hint": "Thanks for the report! \r\nFor some reason, the value is not a `PersonName3` instance, as expected, but a string - this is obviously a bug. No need to provide a test dataset, as this is reproducible with existing test data. I will have a look tonight.\nAwesome. Thank you very much. This is the last hurdle for our project. :D",
        "base": "a94cc9996d2c716298c846f2dbba84f9f901e0a7",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/dataelem.py",
            "pydicom/jsonrep.py",
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1674",
        "problem": "Codify not generating content sequences correctly\n**Describe the bug**\r\nI am trying to generate a radiation dose structure report. I ran Codify on an existing RDSR to generate a template. The sequence content is reproduced but does not seem to be attached  to the base dataset. When I run the generated python file the dicom file it saves has no sequence content information.\r\n\r\n**Expected behavior**\r\nI expect the dicom file generated by the python code from Codify to be similar to the original file.\r\n\r\n**Steps To Reproduce**\r\n$ python codify X-RayRadiationDoseReport001_ESR.dcm rdsr.py\r\n$ python rsdr.py\r\n\r\nI am not able to attached the above files but can supply them.\r\n\r\n**Your environment**\r\nmodule       | version\r\n------       | -------\r\nplatform     | Linux-5.18.7-200.fc36.x86_64-x86_64-with-glibc2.35\r\nPython       | 3.10.5 (main, Jun  9 2022, 00:00:00) [GCC 12.1.1 20220507 (Red Hat 12.1.1-1)]\r\npydicom      | 2.3.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.22.4\r\nPIL          | 9.2.0\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n\r\nRegards\r\nAlan\n",
        "hint": "Hi Alan,\r\n\r\nI'm happy to look into this, but may need a file to work with.  But perhaps we can first have a look at the generated code, which normally immediately attaches any sequence it creates.  For example:\r\n\r\n```\r\n$ pydicom codify pydicom::rtplan.dcm\r\n```\r\nGives lines like\r\n\r\n```python\r\n# Beam Sequence\r\nbeam_sequence = Sequence()\r\nds.BeamSequence = beam_sequence\r\n```\r\n\r\nand then proceeds to create a new Dataset, set its data elements, and eventually `append` it to the sequence:\r\n\r\n```python\r\n# Beam Sequence: Beam 1\r\nbeam1 = Dataset()\r\nbeam1.Manufacturer = 'Linac co.'\r\nbeam1.InstitutionName = 'Here'\r\n.\r\n.\r\n.\r\nbeam_sequence.append(beam1)\r\n```\r\nCan you check your generated code for those kinds of patterns and let us know what you see?\r\n\r\n\nHi Darcy\r\n\r\nThanks for the guidance. Stepping through the code I've picked up two problems. The first has to do with nested content. Eg. if the DICOM file has:\r\n![NestedContent](https://user-images.githubusercontent.com/34305581/185054806-ed2559c6-2e0f-46e5-a9de-085ada6e7753.png)\r\n\r\nThe second Content1 overwrites the first. See lines 121-191 in the attached file.\r\n\r\n[rdsr.txt](https://github.com/pydicom/pydicom/files/9356766/rdsr.txt) Rename this file to rsdr.py.\r\n\r\nThe second problem is more puzzling. In lines 121-191 we have:\r\n\r\n```\r\ncontent_sequence = Sequence()\r\nds.ContentSequence = content_sequence\r\n\r\n# Content Sequence: Content 1\r\ncontent1 = Dataset()\r\ncontent1.add_new((0x0040, 0x0000), 'UL', 512)\r\ncontent1.RelationshipType = 'HAS CONCEPT MOD'\r\ncontent1.ValueType = 'CODE'\r\n.\r\n.\r\n.\r\ncontent_sequence.append(content1)\r\n```\r\nAfter stepping though this ds.ContentSequence is still empty even though there are definitely items in content1 and the items were added to content_sequence. It is not related to the nested content as content2 (lines 193 - 223 in rdsr.py) which has no nested content shows the same behaviour.\r\n\r\nIf I shift the line \"ds.ContentSequence = content_sequence\" to just before the save statement at the end of the file the script crashes while saving with:\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\nThis suggests two things:\r\n1) The previous code is generating a nested loop somewhere.\r\n2) The line \"ds.ContentSequence = content_sequence\" appears to be creating a copy of content_sequence. My understanding of python is that an assigning objects should merely update pointers, but this does not appear to be the case.\r\n\r\nI would suggest generating content names that reflect the content level if possible, eg. content1-1.\r\n\r\nRegards\r\nAlan\nAlan, if you are unable to share your RDSR for testing, do you want to check the behaviour is the same with one of the OpenREM test RDSR files and use that for Darcy or anyone else to work on? One of the more straight forward ones, maybe [this Siemens Flash one](https://bitbucket.org/openrem/openrem/src/develop/openrem/remapp/tests/test_files/CT-RDSR-Siemens_Flash-TAP-SS.dcm)?\nHi Ed\r\nI ran Codify on  CT-RDSR-Siemens_Flash-TAP-SS.dcm as you suggested. This gave the attached file:\r\n[CT-RDSR-Siemens_Flash-TAP-SS.py.txt](https://github.com/pydicom/pydicom/files/9359044/CT-RDSR-Siemens_Flash-TAP-SS.py.txt) rename to .py\r\nRunning this file produces a dicom file 10 times smaller than the original file (attached)\r\n[CT-RDSR-Siemens_Flash-TAP-SS_from_codify.dcm.txt](https://github.com/pydicom/pydicom/files/9359049/CT-RDSR-Siemens_Flash-TAP-SS_from_codify.dcm.txt) Rename to .dcm\r\nLooking at this in a dicom viewer the sequence content is gone.\r\nHope this helps\r\nRegards\r\nAlan\r\n\r\n\r\n\n> The second Content1 overwrites the first.\r\n> ...\r\n> I would suggest generating content names that reflect the content level if possible, eg. content1-1.\r\n\r\nRight, it makes sense - the code didn't consider that a nested item could have the same name.  I can update that.\r\n\r\n\r\n> RecursionError: maximum recursion depth exceeded while calling a Python object\r\n\r\nNot too surprising - I think I hit recursion problems when I first wrote codify.\r\n\r\nThanks for the detailed investigation and example files (thanks also @edmcdonagh  for the suggestion).  I can set up some failing tests with those and then see about fixing them.  I've got some time in the next couple of days, hopefully it isn't too difficult and I can get a PR soon.",
        "base": "e77e6586fa38e6f7e98efca80d560d0fea8a9669",
        "env": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680",
        "files": [
            "pydicom/cli/main.py",
            "pydicom/util/codify.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1228",
        "problem": "Add Tag and VR to the bulk data handling in `from_json`\nCurrently, if you convert back to a Dataset format from a JSON format, you MUST re-hydrate all of the bulk data URI's or you will loose the information.\r\n\r\nThis causes a problem if you just wish to use the Dataset's header (maybe to extract some data, or rearrange some data), because now you have to pay the cost of getting all the pixel data and then handling the pixel data again upon conversion back to JSON\r\n\r\n**Describe the solution you'd like**\r\nAdd the tag and the vr to the bulk data handler in `from_json` (this can be done in a backwards compatible way). This will allow the user to store the BulkDataURI's by tag in a map, return dummy data large enough to trigger the bulk handling when to_json is called next, and to use the map to convert back to the original URI's when bulk handling is triggered from to_json.\r\n\r\nI'm going to drop a PR tomorrow that does this in a fully backward compatible, non-breaking fashion.\r\n\n",
        "hint": "",
        "base": "8112bb69bfc0423c3a08cb89e7960defbe7237bf",
        "env": "9d69811e539774f296c2f289839147e741251716",
        "files": [
            "pydicom/dataelem.py",
            "pydicom/dataset.py",
            "pydicom/jsonrep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1365",
        "problem": "DA class is inconsistent\n**Describe the bug**\r\npydicom.valuerep.DA accepts strings or datetime.date objects - but DA objects created with datetime.date inputs are invalid. \r\n\r\n**Expected behavior**\r\nI would expect both of these expressions to generate the same output:\r\n```\r\nprint(f'DA(\"20201117\") => {DA(\"20201117\")}')\r\nprint(f'DA(date(2020, 11, 17)) => {DA(date(2020, 11, 17))}')\r\n```\r\nbut instead I get\r\n```\r\nDA(\"20201117\") => 20201117\r\nDA(date(2020, 11, 17)) => 2020-11-17\r\n```\r\nThe hyphens inserted into the output are not valid DICOM - see the DA description in [Table 6.2-1](http://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html)\r\n\r\n**Steps To Reproduce**\r\nRun the following commands:\r\n```\r\nfrom pydicom.valuerep import DA\r\nfrom pydicom.dataset import Dataset\r\nfrom datetime import date, datetime\r\n\r\nprint(f'DA(\"20201117\") => {DA(\"20201117\")}')\r\nprint(f'DA(date(2020, 11, 17)) => {DA(date(2020, 11, 17))}')\r\n\r\n# 1. JSON serialization with formatted string works\r\nds = Dataset()\r\nds.ContentDate = '20201117'\r\njson_output = ds.to_json()\r\nprint(f'json_output works = {json_output}')\r\n\r\n# 2. JSON serialization with date object input is invalid.\r\nds = Dataset()\r\nds.ContentDate = str(DA(date(2020, 11, 17)))\r\njson_output = ds.to_json()\r\nprint(f'json_output with str(DA..) - invalid DICOM {json_output}')\r\n\r\n# 3. JSON serialization with date object fails\r\nds = Dataset()\r\nds.ContentDate = DA(date(2020, 11, 17))\r\n\r\n# Exception on this line: TypeError: Object of type DA is not JSON serializable\r\njson_output = ds.to_json()\r\n\r\n```\r\n\r\nI believe that all three approaches should work - but only the first is valid. The method signature on DA's `__new__` method accepts datetime.date objects. \r\n\r\n**Your environment**\r\n```\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.7-x86_64-i386-64bit\r\nPython       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\r\npydicom      | 2.1.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.4\r\nPIL          | 8.0.1\r\n```\r\n\r\n\n",
        "hint": "Thanks for this - I agree that the print output should be consistent, and that the class should work correctly for json.\r\n\nI think that the problem is in the _DateTimeBase method __str__ - there is no 'original_string' so it's calling the __str__ method if the object's superclass and the default for datetime.date puts in the hyphens.  The 'DT' class has a similar problem - it's not formatting the time the DICOM way.\nThe easiest way to handle this is probably to add an `original_string` attribute to `DA`, `TM` and `DT`, and set this attribute to the canonical DICOM representation if initialized with a date or time. This would only change the behavior of `__str__` and `__repr__` for these types, and I think that this is indeed the expected behavior.\r\n@darcymason , what do you think? This would actually be a small change, that could make it into the next release.",
        "base": "bd82f01faf4212f6e43f55a1cc6da17956122d8f",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/965",
        "problem": "Empty data elements with value representation SQ are set to None\n**Describe the bug**\r\nIn the current `master`, empty data elements are not read correctly from files. The attribute value is set to `None` instead of `[]`.\r\n\r\n**Expected behavior**\r\nCreate empty list `[]` for empty sequence, i.e., a sequence with zero items.\r\n\r\n**Steps To Reproduce**\r\n```python\r\nimport pydicom\r\nds = pydicom.Dataset()\r\nds.AcquisitionContextSequence = []\r\nprint(ds)\r\nds.is_little_endian = True\r\nds.is_implicit_VR = True\r\nds.save_as('/tmp/test.dcm')\r\n\r\nreloaded_ds = pydicom.dcmread('/tmp/test.dcm', force=True)\r\nprint(reloaded_ds)\r\n```\r\nThis prints:\r\n```\r\n(0040, 0555)  Acquisition Context Sequence   0 item(s) ----\r\n...\r\nTypeError: With tag (0040, 0555) got exception: object of type 'NoneType' has no len()\r\nTraceback (most recent call last):\r\n  File \"/private/tmp/pydicom/pydicom/tag.py\", line 30, in tag_in_exception\r\n    yield\r\n  File \"/private/tmp/pydicom/pydicom/dataset.py\", line 1599, in _pretty_str\r\n    len(data_element.value)))\r\nTypeError: object of type 'NoneType' has no len()\r\n```\r\n\r\n**Your environment**\r\n```\r\nDarwin-18.6.0-x86_64-i386-64bit\r\nPython  3.7.3 (default, Mar 27 2019, 09:23:15)\r\n[Clang 10.0.1 (clang-1001.0.46.3)]\r\npydicom  1.4.0.dev0\r\n```\n",
        "hint": "",
        "base": "ee775c8a137cd8e0b69b46dc24c23648c31fe34c",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/config.py",
            "pydicom/dataelem.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1375",
        "problem": "Pickling/unpickling timezone in DT does not work\n**Describe the bug**\r\n\r\nThe following tests fail because the timezone is not set in the unpickled `DT`:\r\n```py\r\n    def test_pickling_with_timezone():\r\n        dt = pydicom.valuerep.DT(\"19111213212123-0630\")\r\n        loaded_dt = pickle.loads(pickle.dumps(dt))\r\n        assert dt == loaded_dt\r\n\r\n    def test_pickling_dt_from_datetime_with_timezone():\r\n        tz_info = timezone(timedelta(seconds=-23400), '-0630')\r\n        dt_object = datetime(2022, 12, 31, 23, 59, 59, 42, tzinfo=tz_info)\r\n        dt = pydicom.valuerep.DT(dt_object)\r\n        loaded_dt = pickle.loads(pickle.dumps(dt))\r\n        assert dt == loaded_dt\r\n```\r\n\r\nThis is a spin-off of PR #1365, see [this comment](https://github.com/pydicom/pydicom/pull/1365#issuecomment-829544827).\n",
        "hint": "",
        "base": "c14bf96040e4cc930db890638b9f190d4ede0a21",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1334",
        "problem": "Strings with Value Representation DS are too long\n**Describe the bug**\r\nStrings of Value Representation DS are restricted to a maximum length of 16 bytes according to [Part 5 Section 6.2](http://dicom.nema.org/medical/dicom/current/output/chtml/part05/sect_6.2.html#para_15754884-9ca2-4b12-9368-d66f32bc8ce1), but `pydicom.valuerep.DS` may represent numbers with more than 16 bytes.\r\n\r\n**Expected behavior**\r\n`pydicom.valuerep.DS` should create a string of maximum length 16, when passed a fixed point number with many decimals.\r\n\r\n**Steps To Reproduce**\r\n```python\r\nlen(str(pydicom.valuerep.DS(3.14159265358979323846264338327950288419716939937510582097)).encode('utf-8'))\r\nlen(str(pydicom.valuerep.DS(\"3.14159265358979323846264338327950288419716939937510582097\")).encode('utf-8'))\r\n```\r\nreturns `17` and `58`, respectively, instead of `16`.\r\n\r\n**Your environment**\r\n```\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.6-x86_64-i386-64bit\r\nPython       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\r\npydicom      | 2.0.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.4\r\nPIL          | 8.0.1\r\n```\n",
        "hint": "Thanks for this, @hackermd.  Pydicom has traditionally been permissive about values as they are set, because sometimes people want to replicate existing invalid DICOM.  But for sure this should be an error when `config.enforce_valid_values` is `True` (perhaps a warning otherwise), and pydicom should leave it to the calling code to figure out how to truncate the value.\nI've just checked, because I thought I remembered this coming up before.  There is a check - but only for `DS` derived from `Decimal` (and when `enforce_valid_values is True`).  Wouldn't be hard to replicate the checks for the `DSfloat` class.",
        "base": "24a86b316441ac3a46e569779627e24482786a8a",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/958",
        "problem": "Encoding to ISO 2022 IR 159 doesn't work\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nEncoding to ISO 2022 IR 159 doesn't work even if 'ISO 2022 IR 159' is passed to pydicom.charset.convert_encodings.\r\n\r\n#### Steps/Code to Reproduce\r\nISO 2022 IR 159 is designed as supplement characters to ISO 2022 IR 87. So these characters are not frequent use. But person name sometimes contains them. In the following example, the letter of \"\u9dd7\" is only in ISO 2022 IR 159. But we cannot encode them correctly. \r\n\r\n```\r\nimport pydicom\r\n\r\njapanese_pn = u\"Mori^Ogai=\u68ee^\u9dd7\u5916=\u3082\u308a^\u304a\u3046\u304c\u3044\"\r\nspecific_character_sets = [\"ISO 2022 IR 6\", \"ISO 2022 IR 87\", \"ISO 2022 IR 159\"]\r\nexpect_encoded = (\r\n    b\"\\x4d\\x6f\\x72\\x69\\x5e\\x4f\\x67\\x61\\x69\\x3d\\x1b\\x24\\x42\\x3f\"\r\n    b\"\\x39\\x1b\\x28\\x42\\x5e\\x1b\\x24\\x28\\x44\\x6c\\x3f\\x1b\\x24\\x42\"\r\n    b\"\\x33\\x30\\x1b\\x28\\x42\\x3d\\x1b\\x24\\x42\\x24\\x62\\x24\\x6a\\x1b\"\r\n    b\"\\x28\\x42\\x5e\\x1b\\x24\\x42\\x24\\x2a\\x24\\x26\\x24\\x2c\\x24\\x24\"\r\n    b\"\\x1b\\x28\\x42\"\r\n)\r\n\r\npython_encodings = pydicom.charset.convert_encodings(specific_character_sets)\r\nactual_encoded = pydicom.charset.encode_string(japanese_pn, python_encodings)\r\n\r\nprint(\"actual:{}\".format(actual_encoded))\r\nprint(\"expect:{}\".format(expect_encoded))\r\n```\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\n```\r\nb'Mori^Ogai=\\x1b$B?9\\x1b(B^\\x1b$(Dl?\\x1b$B30\\x1b(B=\\x1b$B$b$j\\x1b(B^\\x1b$B$*$&$,$$\\x1b(B'\r\n```\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\n```\r\nb'Mori^Ogai=?^??=??^????'\r\n```\r\n\r\nAnd the followin exception occurs.\r\n\r\n```\r\n/PATH/TO/MY/PYTHON/PACKAGES/pydicom/charset.py:488: UserWarning: Failed to encode value with encodings: iso8859, iso2022_jp, iso-2022-jp - using replacement characters in encoded string\r\n  .format(', '.join(encodings)))\r\n```\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n```\r\nLinux-4.15.0-55-generic-x86_64-with-debian-buster-sid\r\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)\r\n[GCC 7.3.0]\r\npydicom 1.3.0\r\n```\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
        "hint": "In my opinion, this is caused by selecting the wrong python codec corresponding to 'ISO 2022 IR 159'. In the current implementation,  'iso-2022-jp' is used if 'ISO 2022 IR 159' is passed. But 'iso-2022-jp' is alias to 'iso200_jp'. I think we have to use 'iso-2022-jp-2'. It contains all 'iso-2022-jp' characters and 'ISO 2022 IR 159' characters.\r\n\r\nIf you don't mind, please assign this issue to me. I will make a PR for this issue.\nSure, go ahead! And thanks for the support!",
        "base": "40652fc0a18fd9f1204cb3d4b9829e3a8be5cbe0",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/charset.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/863",
        "problem": "Wrong encoding occurs if the value 1 of SpecificCharacterSets is ISO 2022 IR 13.\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nAll Japanese characters are encoded into shift_jis if the value 1 of SpecificCharacterSets (0x0008, 0x0005) is  ISO 2022 IR 13.\r\n\r\n#### Steps/Code to Reproduce\r\nThe japanese_pn and expect_encoded in the following code came from \r\n[H.3.2 Value 1 of Attribute Specific Character Set (0008,0005) is ISO 2022 IR 13.](http://dicom.nema.org/medical/dicom/2015b/output/chtml/part05/sect_H.3.2.html)\r\n\r\n```py\r\nimport pydicom\r\n\r\njapanese_pn = u\"\uff94\uff8f\uff80\uff9e^\uff80\uff9b\uff73=\u5c71\u7530^\u592a\u90ce=\u3084\u307e\u3060^\u305f\u308d\u3046\"\r\nspecific_character_sets = [\"ISO 2022 IR 13\", \"ISO 2022 IR 87\"]\r\nexpect_encoded = (\r\n    b\"\\xd4\\xcf\\xc0\\xde\\x5e\\xc0\\xdb\\xb3\\x3d\\x1b\\x24\\x42\\x3b\\x33\"\r\n    b\"\\x45\\x44\\x1b\\x28\\x4a\\x5e\\x1b\\x24\\x42\\x42\\x40\\x4f\\x3a\\x1b\"\r\n    b\"\\x28\\x4a\\x3d\\x1b\\x24\\x42\\x24\\x64\\x24\\x5e\\x24\\x40\\x1b\\x28\"\r\n    b\"\\x4a\\x5e\\x1b\\x24\\x42\\x24\\x3f\\x24\\x6d\\x24\\x26\\x1b\\x28\\x4a\"\r\n)\r\n\r\npython_encodings = pydicom.charset.convert_encodings(specific_character_sets)\r\nactual_encoded = pydicom.charset.encode_string(japanese_pn, python_encodings)\r\n\r\nprint(\"actual:{}\".format(actual_encoded))\r\nprint(\"expect:{}\".format(expect_encoded))\r\n```\r\n<!--\r\nExample:\r\n```py\r\nfrom io import BytesIO\r\nfrom pydicom import dcmread\r\n\r\nbytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\r\n             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\r\n             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\r\n             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\r\n             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\r\n\r\nfp = BytesIO(bytestream)\r\nds = dcmread(fp, force=True)\r\n\r\nprint(ds.PatientID)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n\r\nWhen possible use pydicom testing examples to reproduce the errors. Otherwise, provide\r\nan anonymous version of the data in order to replicate the errors.\r\n-->\r\n\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\n```\r\nb'\\xd4\\xcf\\xc0\\xde^\\xc0\\xdb\\xb3=\\x1b$B;3ED\\x1b(J^\\x1b$BB@O:\\x1b(J=\\x1b$B$d$^$@\\x1b(J^\\x1b$B$?$m$&\\x1b(J'\r\n```\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\n```\r\nb'\\xd4\\xcf\\xc0\\xde^\\xc0\\xdb\\xb3=\\x8eR\\x93c^\\x91\\xbe\\x98Y=\\x82\\xe2\\x82\\xdc\\x82\\xbe^\\x82\\xbd\\x82\\xeb\\x82\\xa4'\r\n```\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n```\r\nLinux-4.15.0-50-generic-x86_64-with-debian-buster-sid\r\nPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)\r\n[GCC 7.3.0]\r\npydicom 1.2.2\r\n```\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
        "hint": "In my opinion, this issue is caused by encoding to shift_jis doesn't raise UnicodeError when characters which are out of JIS X 0201 are given.  So I guess that this is fixed by encoding to jis correctly.\r\n\r\nIf you don't mind, please assign this issue to me. I will make a PR  for this issue.\nThanks for the report - of course you can make a PR for this, please go ahead! \nDear all.\r\nI'm trying to solve this issue. And I want some advice about the scope of this issue and the way of implementation.\r\nMay I discuss them in this issue thread? Or should I create a PR and add W.I.P to its title?\nWhatever suits you better - if you want to discuss concrete code, it may be easier to add a PR to be able to comment on specific lines, but that's completely up to you!\n@mrbean-bremen \r\n\r\nThank for your quick reply. I got it. First, I will write some concrete codes. And then I'll make a PR and want to discuss there. ",
        "base": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "env": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "files": [
            "pydicom/charset.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1694",
        "problem": "Dataset.to_json_dict can still generate exceptions when suppress_invalid_tags=True\n**Describe the bug**\r\nI'm using `Dataset.to_json_dict(suppress_invalid_tags=True)` and can live with losing invalid tags.  Unfortunately, I can still trigger an exception with something like  `2.0` in an `IS` field.\r\n\r\n**Expected behavior**\r\nto_json_dict shouldn't throw an error about an invalid tag when `suppress_invalid_tags` is enabled.\r\n\r\nMy thought was simply to move the `data_element = self[key]` into the try/catch block that's right after it.\r\n\r\n**Steps To Reproduce**\r\n\r\nTraceback:\r\n```\r\n  File \"dicom.py\", line 143, in create_dict\r\n    json_ds = ds.to_json_dict(suppress_invalid_tags=True)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataset.py\", line 2495, in to_json_dict\r\n    data_element = self[key]\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataset.py\", line 939, in __getitem__\r\n    self[tag] = DataElement_from_raw(elem, character_set, self)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataelem.py\", line 859, in DataElement_from_raw\r\n    value = convert_value(vr, raw, encoding)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/values.py\", line 771, in convert_value\r\n    return converter(byte_string, is_little_endian, num_format)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/values.py\", line 348, in convert_IS_string\r\n    return MultiString(num_string, valtype=pydicom.valuerep.IS)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/valuerep.py\", line 1213, in MultiString\r\n    return valtype(splitup[0])\r\n  File \"/usr/lib/python3/dist-packages/pydicom/valuerep.py\", line 1131, in __new__\r\n    raise TypeError(\"Could not convert value to integer without loss\")\r\nTypeError: Could not convert value to integer without loss\r\n```\r\n\r\n**Your environment**\r\npython 3.7, pydicom 2.3\r\n\r\n\n",
        "hint": "",
        "base": "f8cf45b6c121e5a4bf4a43f71aba3bc64af3db9c",
        "env": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680",
        "files": [
            "pydicom/dataset.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1562",
        "problem": "Revise the type annotation for pydicom.datadict.dictionary_has_tag()\n**Describe the bug**\r\n\r\nThe documentation of [`pydicom.datadict.dictionary_has_tag()`](https://pydicom.github.io/pydicom/dev/reference/generated/pydicom.datadict.dictionary_has_tag.html#pydicom.datadict.dictionary_has_tag) suggests that a query using keywords (instead of a tag integer) would work:\r\n\r\n```python\r\npydicom.datadict.dictionary_has_tag(tag: Union[int, str, Tuple[int, int], pydicom.tag.BaseTag]) -> bool\r\n```\r\n\r\nHowever, the function only accepts integer arguments.\r\n\r\n```python\r\nfrom pydicom.datadict import dictionary_has_tag, keyword_dict\r\ndictionary_has_tag(\"PixelData\")\r\n# Returns False\r\n\r\ndictionary_has_tag(keyword_dict[\"PixelData\"])\r\n# Returns True\r\n```\r\n\r\n(The problem may apply to other functions as well...)\r\n\r\n**Expected behavior**\r\nFollowing the docs, `dictionary_has_tag(\"PixelData\")` should return True. \r\n\r\nIt would be nice, if the flexible conversion of tags from names or hex-tuples (as the type annotation suggests) would also be possible for this function.\r\n\r\n**Your environment**\r\n```text\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.14.6-x86_64-i386-64bit\r\nPython       | 3.9.0 (v3.9.0:9cf6752276, Oct  5 2020, 11:29:23)  [Clang 6.0 (clang-600.0.57)]\r\npydicom      | 2.2.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.20.1\r\nPIL          | 8.0.1\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n```\r\n\r\n\n",
        "hint": "Yeah, it should be plain old `int`. Not sure how I missed that one...\r\n\r\nThe keyword to tag conversion is actually kind of (relatively) expensive, and this function is used in our initial dataset parsing so we want it to be fast.",
        "base": "e1a035a88fe36d466579b2f3940bde5b8b1bc84d",
        "env": "0fa18d2a2179c92efc22200ed6b3689e66cecf92",
        "files": [
            "pydicom/datadict.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1413",
        "problem": "Error : a bytes-like object is required, not 'MultiValue'\nHello,\r\n\r\nI am getting following error while updating the tag LongTrianglePointIndexList (0066,0040),\r\n**TypeError: a bytes-like object is required, not 'MultiValue'**\r\n\r\nI noticed that the error  gets produced only when the VR is given as \"OL\" , works fine with \"OB\", \"OF\" etc.\r\n\r\nsample code (assume 'lineSeq' is the dicom dataset sequence):\r\n```python\r\nimport pydicom\r\nimport array\r\ndata=list(range(1,10))\r\ndata=array.array('H', indexData).tostring()  # to convert to unsigned short\r\nlineSeq.add_new(0x00660040, 'OL', data)   \r\nds.save_as(\"mydicom\")\r\n```\r\noutcome: **TypeError: a bytes-like object is required, not 'MultiValue'**\r\n\r\nusing version - 2.0.0.0\r\n\r\nAny help is appreciated.\r\n\r\nThank you\n",
        "hint": "Also tried following code to get the byte string, but same error.\r\n1. data=array.array('L', indexData).tostring()  # to convert to long -> same error\r\n2. data=array.array('Q', indexData).tostring()  # to convert to long long -> same error\r\n\r\n\nO* VRs should be `bytes`. Use `array.tobytes()` instead of `tostring()`?\r\n\r\nAlso, in the future if have an issue it's much more helpful if you post the full traceback rather than the error since we can look at it to figure out where in the code the exception is occurring.\r\n\r\nIt would also help if you posted the version of Python you're using. \r\n\r\nThis works fine for me with Python 3.9 and pydicom 2.1.2:\r\n```python\r\nfrom pydicom import Dataset\r\nimport array\r\n\r\narr = array.array('H', range(10))\r\nds = Dataset()\r\nds.is_little_endian = True\r\nds.is_implicit_VR = False\r\nds.LongTrianglePointIndexList = arr.tobytes()\r\nprint(ds[\"LongTrianglePointIndexList\"].VR)  # 'OL'\r\nds.save_as('temp.dcm')\r\n```\r\nThis also works fine:\r\n```python\r\nds = Dataset()\r\nds.add_new(0x00660040, 'OL', arr.tobytes())\r\n```\nThank you for the answer.\r\nUnfortunately the error still persists with above code.\r\nPlease find the attached detailed error.\r\n[error.txt](https://github.com/pydicom/pydicom/files/6661451/error.txt)\r\n\r\nOne more information is that the 'ds' is actually read from a file in the disk (ds=pydicom.read_file(filename)). \r\nand this byte array is stored under the following sequence\r\nds[0x0066,0x0002][0][0x0066,0x0013][0][0x0066,0x0028][0][0x0066,0x0040] = arr.tobytes()\r\n\r\npydicom - 2.0.0.0\r\npython - 3.6.4\r\n\r\nThank you.\nCould you post a minimal code sample that reproduces the issue please?\r\n\r\nIf you're using something like this:\r\n`ds[0x0066,0x0002][0][0x0066,0x0013][0][0x0066,0x0028][0][0x0066,0x0040] = arr.tobytes()`\r\n\r\nThen you're missing the `.value` assignment:\r\n`ds[0x0066,0x0002][0][0x0066,0x0013][0][0x0066,0x0028][0][0x0066,0x0040].value = arr.tobytes()`\nHello,\r\nabove code line I just mentioned to give an idea where the actual data is stored (tree level).\r\n\r\nPlease find the actual code used below,\r\n```python\r\nimport pydicom\r\nfrom pydicom.sequence import Sequence\r\nfrom pydicom.dataelem import DataElement\r\nfrom pydicom.dataset import Dataset\r\n\r\nds = pydicom.read_file(filename)\r\nsurfaceSeq= ds[0x0066,0x0002]\r\n\r\n#// read existing sequence items in the dataset\r\nseqlist=[]\r\nfor n in surfaceSeq:\r\n    seqlist.append(n)\r\n\r\nnewDs = Dataset()\r\n \r\nsurfaceMeshPrimitiveSq = Dataset()\r\nlineSeq = Dataset()\r\nindexData = list(range(1,100))\r\nindexData = array.array('H', indexData)\r\nindexData = indexData.tobytes()\r\nlineSeq.add_new(0x00660040, 'OL', indexData) \r\nsurfaceMeshPrimitiveSq.add_new(0x00660028, 'SQ', [lineSeq])\r\nnewDs.add_new(0x00660013, 'SQ', [surfaceMeshPrimitiveSq])\r\n\r\n#add the new sequnce item to the list\r\nseqlist.append(newDs)\r\nds[0x0066,0x0002] = DataElement(0x00660002,\"SQ\",seqlist)\r\nds.save_as(filename)\r\n```\nOK, I can reproduce with:\r\n```python\r\n\r\nimport array\r\n\r\nfrom pydicom import Dataset\r\nfrom pydicom.uid import ExplicitVRLittleEndian\r\n\r\nds = Dataset()\r\nds.file_meta = Dataset()\r\nds.file_meta.TransferSyntaxUID = ExplicitVRLittleEndian\r\n\r\nb = array.array('H', range(100)).tobytes()\r\n\r\nds.LongPrimitivePointIndexList = b\r\nds.save_as('1421.dcm')\r\n```\r\nAnd `print(ds)` gives:\r\n```\r\n(0066, 0040) Long Primitive Point Index List     OL: [b'\\x00\\x00\\x01\\x00\\x02\\x00\\x03\\x00\\x04\\x00\\x05\\x00\\x06\\x00\\x07\\x00\\x08\\x00\\t\\x00\\n\\x00\\x0b\\x00\\x0c\\x00\\r\\x00\\x0e\\x00\\x0f\\x00\\x10\\x00\\x11\\x00\\x12\\x00\\x13\\x00\\x14\\x00\\x15\\x00\\x16\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x1a\\x00\\x1b\\x00\\x1c\\x00\\x1d\\x00\\x1e\\x00\\x1f\\x00 \\x00!\\x00\"\\x00#\\x00$\\x00%\\x00&\\x00\\'\\x00(\\x00)\\x00*\\x00+\\x00,\\x00-\\x00.\\x00/\\x000\\x001\\x002\\x003\\x004\\x005\\x006\\x007\\x008\\x009\\x00:\\x00;\\x00<\\x00=\\x00>\\x00?\\x00@\\x00A\\x00B\\x00C\\x00D\\x00E\\x00F\\x00G\\x00H\\x00I\\x00J\\x00K\\x00L\\x00M\\x00N\\x00O\\x00P\\x00Q\\x00R\\x00S\\x00T\\x00U\\x00V\\x00W\\x00X\\x00Y\\x00Z\\x00[\\x00', b'\\x00]\\x00^\\x00_\\x00`\\x00a\\x00b\\x00c\\x00']\r\n```\r\nI think this is because the byte value is hitting the hex for the backslash character during assignment. Ouch, that's kinda nasty.",
        "base": "f909c76e31f759246cec3708dadd173c5d6e84b1",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/dataelem.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1090",
        "problem": "Write deflated content when called Transfer Syntax is Deflated Explicit VR Little Endian\n**Describe the bug**\r\nAfter using `dcmread` to read a deflated .dcm file created from pydicom's [CT_small.dcm sample](https://github.com/pydicom/pydicom/blob/v1.4.2/pydicom/data/test_files/CT_small.dcm), with the following file meta information\r\n```\r\n(0002, 0000) File Meta Information Group Length  UL: 178\r\n(0002, 0001) File Meta Information Version       OB: b'\\x00\\x01'\r\n(0002, 0002) Media Storage SOP Class UID         UI: CT Image Storage\r\n(0002, 0003) Media Storage SOP Instance UID      UI: 1.3.6.1.4.1.5962.1.1.1.1.1.20040119072730.12322\r\n(0002, 0010) Transfer Syntax UID                 UI: Deflated Explicit VR Little Endian\r\n(0002, 0012) Implementation Class UID            UI: 1.2.40.0.13.1.1\r\n(0002, 0013) Implementation Version Name         SH: 'dcm4che-2.0'\r\n```\r\n\r\nI use `save_as` to save the file. The output file has an unaltered file meta information section, but the group 8 elements and beyond are not written in deflated format, instead appearing to be LEE. In particular, the specific character set element is easily readable from a hex representation of the file, rather than appearing as gobbledygook like one would expect from a deflated stream.\r\n\r\n**Expected behavior**\r\nThe bulk of the DCM to be written as Deflated Explicit VR Little Endian or the Transfer Syntax UID to be saved with a value that reflects the actual format of the DCM\r\n\r\n**Steps To Reproduce**\r\n```python\r\n\u276f py\r\n>>> # CT_small_deflated.dcm is CT_small.dcm, deflated using dcm2dcm\r\n>>> ds = pydicom.dcmread(\"CT_small_deflated.dcm\")\r\n\r\n>>> ds.save_as(\"ds_like_orig.dcm\", write_like_original=True)\r\n>>> pydicom.dcmread(\"ds_like_orig.dcm\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\blairyat\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pydicom\\filereader.py\", line 869, in dcmread\r\n    dataset = read_partial(fp, stop_when, defer_size=defer_size,\r\n  File \"C:\\Users\\blairyat\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pydicom\\filereader.py\", line 729, in read_partial\r\n    unzipped = zlib.decompress(zipped, -zlib.MAX_WBITS)\r\nzlib.error: Error -3 while decompressing data: invalid stored block lengths\r\n\r\n>>> ds.save_as(\"ds_not_like_orig.dcm\", write_like_original=False)\r\n>>> pydicom.dcmread(\"ds_not_like_orig.dcm\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\blairyat\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pydicom\\filereader.py\", line 869, in dcmread\r\n    dataset = read_partial(fp, stop_when, defer_size=defer_size,\r\n  File \"C:\\Users\\blairyat\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pydicom\\filereader.py\", line 729, in read_partial\r\n    unzipped = zlib.decompress(zipped, -zlib.MAX_WBITS)\r\nzlib.error: Error -3 while decompressing data: invalid stored block lengths\r\n```\r\n\r\n**Your environment**\r\nPlease run the following and paste the output.\r\n```powershell\r\n\u276f py -c \"import platform; print(platform.platform())\"\r\nWindows-10-10.0.18362-SP0\r\n\r\n\u276f py -c \"import sys; print('Python ', sys.version)\"\r\nPython  3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 22:39:24) [MSC v.1916 32 bit (Intel)]\r\n\r\n\u276f py -c \"import pydicom; print('pydicom ', pydicom.__version__)\"\r\npydicom  1.4.2\r\n```\r\n\n",
        "hint": "Of course, it's entirely possible I'm just not using the library properly.\r\nRegardless, here are my original and generated files, as well as the reproduction steps:\r\n[write_deflated_file.zip](https://github.com/pydicom/pydicom/files/4557981/write_deflated_file.zip)\r\n\r\nThanks for your attention in this. Assuming it's deemed to be a valid bug, I'd be happy to contribute to the development of a solution. (My initial thought is that the Transfer Syntax should be respected and that the file should be written deflated, but I've not investigated deeply.)\nTry setting `ds.is_explicit_VR = True` before saving. Changing the Transfer Syntax isn't enough to control the encoding of the written dataset.\r\n\r\nWe should probably add a warning for when the transfer syntax doesn't match `is_implicit_VR` and `is_little_endian`...\r\n\r\nI'm not sure about deflated off hand, I'll need to check\nThanks, @scaramallion. That had no effect. The output is exactly the same.\r\n\r\nI fear I may not have been clear. I never did change the Transfer Syntax. It stayed at Deflated Explicit VR Little Endian, just as it was in the source\u2026\nWe don't have anything out-of-the-box for writing deflated, this should work:\r\n```python\r\nimport zlib\r\n\r\nfrom pydicom import dcmread\r\nfrom pydicom.data import get_testdata_file\r\nfrom pydicom.filebase import DicomBytesIO\r\nfrom pydicom.filewriter import write_file_meta_info, write_dataset\r\nfrom pydicom.uid import DeflatedExplicitVRLittleEndian\r\n\r\nds = dcmread(get_testdata_file(\"CT_small.dcm\"))\r\nds.file_meta.TransferSyntaxUID = DeflatedExplicitVRLittleEndian\r\n\r\nwith open('deflated.dcm', 'wb') as f:\r\n    # Write preamble and DICM marker\r\n    f.write(b'\\x00' * 128)\r\n    f.write(b'DICM')\r\n    # Write file meta information elements\r\n    write_file_meta_info(f, ds.file_meta)\r\n\r\n    # Encode the dataset\r\n    bytesio = DicomBytesIO()\r\n    bytesio.is_little_endian = True\r\n    bytesio.is_implicit_VR = False\r\n    write_dataset(bytesio, ds)\r\n\r\n    # Compress the encoded data and write to file\r\n    compressor = zlib.compressobj(wbits=-zlib.MAX_WBITS)\r\n    deflated = compressor.compress(bytesio.parent.getvalue())\r\n    deflated += compressor.flush()\r\n    if len(deflated) %2:\r\n        deflated += b'\\x00'\r\n\r\n    f.write(deflated)\r\n\r\nds = dcmread('deflated.dcm')\r\nprint(ds)\r\n```\nThanks, @scaramallion. Are you looking for a contribution, or are you thinking the design would be would be too complicated for an enthusiastic experienced programmer but first-time pydicomer?\r\n(I'm assuming this would eventually just become automatic behaviour whenever saving something with the matching Transfer Syntax\u2026)\nI think this is definitely a contribution a first-timer could make, take a look at `filewriter.dcmwrite()`, if you add a check for the deflated transfer syntax around [line 968](https://github.com/pydicom/pydicom/blob/master/pydicom/filewriter.py#L968) then encode and deflate accordingly.\n> this is definitely a contribution a first-timer could make\r\n\r\nThen sign me up! Assuming you don't need it by tomorrow. (I won't be super slow. It's just that it's nearly bedtime and I have a day job.)\r\n\nThere's no rush, we (nearly) all have day jobs ",
        "base": "5098c9147fadcb3e5918487036867931435adeb8",
        "env": "5098c9147fadcb3e5918487036867931435adeb8",
        "files": [
            "pydicom/filereader.py",
            "pydicom/filewriter.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/800",
        "problem": "The function generate_uid() generates non-conforming \u201c2.25 .\u201d DICOM UIDs\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nIt seems there was already a discussion about this function in the past (#125), but the current implementation generates non-conforming DICOM UIDs when called with prefix \u2018none\u2019 to trigger that the function generate_uid() should generate a UUID derived UID.\r\n\r\nThe DICOM Standard requires (see DICOM PS 3.5, B.2 that when a UUID derived UID is constructed it should be in the format \u201c2.25.\u201d + uuid(in its decimal representation string representation)\r\nFor example a UUID of f81d4fae-7dec-11d0-a765-00a0c91e6bf6 should become 2.25.329800735698586629295641978511506172918\r\n\r\nThe current implementation extends the uuid part to the remaining 59 characters. By not following the DICOM formatting rule, receiving systems that are processing DICOM instances created with this library are not capable of converting the generated \u201c2.25\u201d UID back to a UUID. Due to the extra sha512 operation on the UUID, the variant and version info of the UUID are also lost.\r\n\r\n#### Steps/Code to Reproduce\r\n- call generate_uid() to generate a \"2.25.\" DICOM UID\r\n\r\n#### Expected Results\r\nA conforming unique DICOM UID is returned.\r\n\r\n#### Actual Results\r\nNon conforming UID is returned.\n",
        "hint": "Thanks for pointing this out - I wasn't aware of that section of the standard.  Do you know of a solution that respects the privacy issues pointed out in #125? \r\n\r\n> By not following the DICOM formatting rule, receiving systems that are processing DICOM instances created with this library are not capable of converting the generated \u201c2.25\u201d UID back to a UUID\r\n\r\nWhy would it be necessary for the receiving software to generate a \"real UUID\" from this (and therefore the variant and the version)?  Just curious, as if we want to be standard compliant it doesn't really matter.\r\n\r\n\r\n\nI think the solution proposed by @cancan101 in #125 to use the uuid v4 algorithm is fine. The uuid v1 algorithm leaks the MAC address. The open source dcm4che (Java) implementation for example also uses the uuid v4 algorithm, the two C# implementation I known of use the .NET Guid.NewGuid() method, which will also returns v4 uuids. \r\n\r\nDICOM is all about interoperability, there may be receiving systems that (implicitly) depend on it. A Level 2 (Full) C-STORE SCP may, (but is not required) validate the attributes of an incoming SOP instance. Personally, I have never encountered a DICOM system that had trouble with it, most systems just threat a UID as 64 bytes and are happy with it as long as it is unique.\r\n\r\nPossible (performance) scenario\r\nLarge DICOM archives need to maintain a relational database to maintain which images are stored in the system. To ensure data integrity, these systems put often a constraint on the uniqueness of the SOP Instance UID column. Most DBMS systems will create a non-cluster index to ensure that this constraint can be met. A uuid is only 16 bytes, compared to a UID that is 64 bytes, which can make the difference of keeping the index in memory or not. Some DBMS systems have a native data type to support uuid columns. The complication is of course that these systems also need to support images with \u201cOrganizationally Derived\u201d UIDs and this optimization only makes sense if a majority of UIDs are uuid derived UIDs.",
        "base": "2f3586b6f67383b1ec0c24c4772e65119c3f5261",
        "env": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "files": [
            "pydicom/uid.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/938",
        "problem": "[python 3.8] failing tests: various issues but \"max recursion depth reached\" seems to be one\n#### Description\r\nFedora is beginning to test python packages against python 3.8. Pydicom builds but tests fail with errors.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```\r\npython setup.py build\r\npython setup.py install\r\npytest\r\n```\r\n\r\nThe complete build log is attached. It includes the complete build process. The root log is also attached. These are the versions of other python libraries that are in use:\r\n\r\n```\r\npython3-dateutil-1:2.8.0-5.fc32.noarch\r\npython3-devel-3.8.0~b3-4.fc32.x86_64\r\npython3-numpy-1:1.17.0-3.fc32.x86_64\r\npython3-numpydoc-0.9.1-3.fc32.noarch\r\npython3-pytest-4.6.5-3.fc32.noarch\r\npython3-setuptools-41.0.1-8.fc32.noarch\r\npython3-six-1.12.0-5.fc32.noarch\r\n```\r\n\r\n[build-log.txt](https://github.com/pydicom/pydicom/files/3527558/build-log.txt)\r\n[root-log.txt](https://github.com/pydicom/pydicom/files/3527559/root-log.txt)\r\n\n",
        "hint": "Thanks, @sanjayankur31, we'll look into it.\nLooks like the [culprit](https://docs.python.org/3.8/whatsnew/3.8.html#changes-in-python-behavior) might be:\r\n\r\n> Removed `__str__` implementations from builtin types bool, int, float, complex and few classes from the standard library. They now inherit `__str__()` from object. As result, defining the `__repr__()` method in the subclass of these classes will affect they string representation.\r\n\r\nThe unit test results in the build log shows issues with `DSfloat.__str__()/DSfloat.__repr__()` and `IS.__repr__()` on lines [350](https://github.com/pydicom/pydicom/blob/6d8ef0bfcec983e5f8bd8a2e359ff318fe9fcf65/pydicom/valuerep.py#L353)/353 and [520](https://github.com/pydicom/pydicom/blob/6d8ef0bfcec983e5f8bd8a2e359ff318fe9fcf65/pydicom/valuerep.py#L520) of current master.\r\n",
        "base": "6d8ef0bfcec983e5f8bd8a2e359ff318fe9fcf65",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1194",
        "problem": "Error decoding dataset with ambiguous VR element when the value is None\nHi all,\r\n    I used the storescu in pynetdicom 1.5.3 to send the dicom ct files(both on mac and ubuntu): \r\n**python storescu.py 192.168.1.120 9002 ~/Downloads/test/**\r\n(I also tried https://pydicom.github.io/pynetdicom/stable/examples/storage.html#storage-scu)\r\nbut it throwed errors: \r\n\r\n_E: Failed to encode the supplied Dataset\r\nE: Store failed: /Users/me/Downloads/test/CT_S1_118.dcm\r\nE: Failed to encode the supplied Dataset\r\nTraceback (most recent call last):\r\n  File \"storescu.py\", line 283, in main\r\n    status = assoc.send_c_store(ds, ii)\r\n  File \"/Users/me/.pyenv/versions/3.8.2/lib/python3.8/site-packages/pynetdicom/association.py\", line 1736, in send_c_store\r\n    raise ValueError('Failed to encode the supplied Dataset')\r\nValueError: Failed to encode the supplied Dataset_\r\n\r\nBut I used to send same files with storescu in dcm4che successfully.\r\nFile attached.\r\n\r\n[test.zip](https://github.com/pydicom/pynetdicom/files/5258867/test.zip)\r\n\n",
        "hint": "```python\r\n>>> from pydicom import dcmread\r\n>>> dcmread(\"CT_S1_001.dcm\")\r\nTraceback (most recent call last):\r\n  File \".../pydicom/tag.py\", line 30, in tag_in_exception\r\n    yield\r\n  File \".../pydicom/filewriter.py\", line 555, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \".../pydicom/dataset.py\", line 1060, in get_item\r\n    return self[key]\r\n  File \".../pydicom/dataset.py\", line 878, in __getitem__\r\n    self[tag] = correct_ambiguous_vr_element(\r\n  File \".../pydicom/filewriter.py\", line 160, in correct_ambiguous_vr_element\r\n    _correct_ambiguous_vr_element(elem, ds, is_little_endian)\r\n  File \".../pydicom/filewriter.py\", line 86, in _correct_ambiguous_vr_element\r\n    elem_value = elem.value if elem.VM == 1 else elem.value[0]\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\nIssue occurs because the dataset is Implicit VR and the *Smallest Image Pixel Value* is ambiguous but empty,",
        "base": "5e70c1dfe09820023fec519dac4c51bebcb7f60d",
        "env": "9d69811e539774f296c2f289839147e741251716",
        "files": [
            "pydicom/filewriter.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/997",
        "problem": "Generators in encaps don't handle single fragment per frame correctly with no BOT value\n#### Description\r\nGenerators in `encaps.py` handling of encapsulated pixel data incorrect when the Basic Offset Table has no value and each frame is a single fragment.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom pydicom import dcmread\r\nfrom pydicom.encaps import generate_pixel_data_frame\r\n\r\nfpath = 'pydicom/data/test_files/emri_small_jpeg_2k_lossless.dcm'\r\nds = dcmread(fpath)\r\nds.NumberOfFrames  # 10\r\nframe_generator = generate_pixel_data_frame(ds.PixelData)\r\nnext(frame_generator)\r\nnext(frame_generator) # StopIteration raised\r\n```\r\n\r\n#### Expected Results\r\nAll 10 frames of the pixel data should be accessible.\r\n\r\n#### Actual Results\r\nOnly the first frame is accessible.\n[MRG] Some pixel handlers will not decode multiple fragments per frame\n\r\n\r\nAdded test cases to demonstrate failures for jpeg ls with multiple fragments per frame.  The test files were created with dcmtk 3.6.1 using dcmcjpls +fs 1. One file has an offset table, the other does not.\r\n\r\n#### Reference Issue\r\nSee #685 \r\n\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nThese test cases show that the pixel decoders (jpeg and jpeg_ls most likely) will not handle multiple fragments per frame.\r\n\r\nNo fix yet...\r\n\r\nAny suggestions?\n",
        "hint": "The problem is that as far as I can tell there's no unambiguous way to determine the difference between one frame with multiple fragments and multiple frames of 1 fragment each when the Basic Offset Table (Part 5, Annex A.4 and Section 8.2) has no value. You can look at the NumberOfFrames value but that won't help if the number of fragments is larger than the number of frames (indicating one or more frames has multiple fragments).\r\n\r\nI could add a parser that determines the number of Items in the Pixel Data, and if the BOT has no value then compare that against the number of frames and use that to determine the generator yields. It seems clunky though.\r\n\r\nAnyone have a better idea?\r\n\r\n\nAs long as you are not speaking about arbitrary access to single frames, it shall be possible to ignore the offset table completely and just decode fragments sequentially, checking for the decoded frame size as you go along, and yield the next frame as soon as it has the correct frame size (or larger in case of padding). \r\n\r\nThis is in case I didn't miss something here, and only valid for the generator as opposed to arbitrary frame access.\nOk, I did miss something here - the frame is not decoded by the generator, but only later by the pixel data handler, so we won't know the decoded fragment size at that point. \r\nWe could change the logic respectively (hand over each fragment to the pixel handler for decoding), but that would change the behavior, and also may be slower (or not, not sure).\r\n\r\nYou could at least check if the number of frames is the same as the number of fragments (which is probably the most common case) and handle that case, and also the case where the number of fragment is a multiple of the number of frames (which is less sure), but for other cases, you really need to decode the data to be on the sure side. \nAs I recall when I first did this some time ago,  I had an awful time trying to deal with frames and fragments.  I ended up just focusing on getting it to work with the EMRI images I had for test cases and not really worrying about individual frame boundaries.  I am pretty sure where the pixel handler code says \"for frame in CompressedPixelDataSeq\", it is actually looping over each fragment...\r\n\r\nI am pretty sure the handlers will fail when there is more than one fragment per frame...\r\n\n> I am pretty sure the handlers will fail when there is more than one fragment per frame..\r\n\r\nIn this case maybe it makes sense to restructure the code to use the handlers to decode each fragment. Not sure if this would be a compatibility issue for existing code, though. \nThe test cases I added for jpeg_ls have a different number of fragments per frame in the sequence.  I have to imagine that if the the offset table is empty for a muiltframe image, then the best you can do is assume one fragment per frame (that is what the code currently does).  Reading the standard, it is unclear to me that having an empty offset table with more than one fragment per frame is valid.  \r\n\r\nI would welcome a more expert opinion on this.  \r\n\r\n\nAs far as I can see, the standard does not enforce a value for the offset table in any case, so this would be valid. I have no idea if this is used in the real world, but I wouldn't exclude the possibility.\nI just added some changes to #688 that change the jpeg_ls handler to more properly use generate_pixel_data_frame rather than decode_data_sequence.  This fixes the test case of multiple fragments per frame with an offset table.  \r\n\r\nFor the case of no offset table, then maybe we can just go fragment by fragment with:\r\n\r\n```\r\nfor fragment in data:\r\n    frame.append(fragment)\r\n    try:\r\n        decompress(frame)\r\n        print \"yea! this is a valid frame\"\r\n    except:\r\n        print \"not quite done yet\"\r\n\r\n```\nLooks promising! I'm off to my day job now, may have another look in the evening.\nI was thinking of modifying the frame generator so the following logic applies:\r\n```\r\nif no BOT value:\r\n    if NumberOfFrames == (number of pixel data items):\r\n        # Multiple fragments, one frame each\r\n    elif NumberOfFrames == 1:\r\n        # One or more fragments, one frame\r\n    elif (number of pixel data items) == 1:\r\n        # One fragment, one frame\r\n    else:\r\n        # Multiple fragments, but no way to tell what fragments are what frame\r\n        raise exception, tell user they should use the \r\n            fragment generator and their own judgement?\r\n```\r\nIn the last case I think the only way to generate the frame correctly is if the BOT has a value.\r\n\r\nEdit - Or to try to decompress? But not all encapsulated data has to be compressed...\nIs it guaranteed that decompressing a frame will fail if the frame isn't complete? What's the performance penalty for a failed decompress?\n> I was thinking of modifying the frame generator so the following logic applies\r\n\r\nI think this sounds good. The last case is the real problem, of course. I have been thinking about decompressing it, too, but I'm not sure how decompressing/decoding separate fragments behaves. \r\nA working but probably slow solution would be to try to decompress a fragment as proposed, and if that fails, or if the resulting image is too small for a frame, try again with another fragment appended (the decoder itself could do this incrementally, but we don't have control over that). I don't know if there is the possibility to decode fragments independently - that would be easier (e.g. no need to append fragments), but I doubt that.\r\nAnyway, that would basically mean either to transfer a part of the generator logic to the pixel handlers, or make the generator dependend on the pixel handlers.\r\nIt would be good if we had some test data with multiple fragments per frame to check this... I may have to think a bit more about this, having no real experience in this field.\nFor all the JPEG syntaxes (Jpeg200, Jpeg-LS, jpeg), a single frame always ends with a End-of-Image (EOI in JPEG/JPEG-LS, EOC in JPEG2K) that is \"FF D9\".  If there are N frames, there should always be exactly N of those markers in the fragments.   \r\n\r\nSo, for JPEG family, we could loop over the fragments:\r\n\r\n```\r\n\r\nframe = bytearray()\r\nfor fragment in data:\r\n    frame.extend(fragment)\r\n    if \"FFD9\" in frame[-10:]:\r\n        print \"Hooray! The frame is done\"\r\n        break\r\n    else:\r\n        print \"Still more to do\"\r\n\r\n```\r\n\r\nAlthough the try decompress ... except should work with jpeg syntaxes, there is no requirement that a decoder fail on an incomplete frame (it could just return decompressed data and wait for more input) (the current api we use will fail on incomplete images - but no guarantees).  \r\n\r\nFor the RLE syntaxes, there is only one fragment per frame.\r\n\r\nFor uncompressed syntaxes, the size of each frame may be determined by BitsStored x Rows x Cols.\r\n\r\n\nThat is much better! As we don't support any other compressed syntaxes apart from JPEG and RLE syntaxes, this should be sufficient to handle these cases. This also makes it possible to get some kind of indexed access for the worst case by building up the missing offset table ourselves if needed.\r\nThat logic has to be in the handlers though, or at some intermediate level, as it is the same for all JPEG syntaxes.\nIs it possible to hit FF D9 because that's the value at that offset, not because we're at the end of the frame? Or is this only in the JPEG pixel data handlers?\r\n\r\nI keep getting confused whether were talking about the pixel data handlers or the generator functions in encaps.\nThis is only for JPEG handlers.  FF D9 is guaranteed to not be in the encoded data.  It is a reserved JPEG marker.\n> I keep getting confused whether were talking about the pixel data handlers or the generator functions in encaps.\r\n\r\nThis is currently a bit mixed up. The current implementation of the generator functions does not know about specific transfer syntaxes - with the proposed PR this would change. Maybe the generator could ask the pixel handler about a fragment being the last in a frame in case it doesn't know that, and the JPEG pixel handlers implement the logic (which should be in some common place for all JPEG handlers)?\r\n\n@rhaxton @scaramallion - this would be a good candidate for 1.2, in my opinion. Is this doable? Do you need help for the conflict resolving?\n@rhaxton, @scaramallion, pinging on this issue again ... was this ever resolved completely? I'll assign to v1.4 milestone for now, perhaps we can try to look at this in the coming months?\nHello @rhaxton! Thanks for updating the PR.\n\n\n\n\n\n\n\n\n\n\n\nCheers ! There are no PEP8 issues in this Pull Request. :beers: \n\n##### Comment last updated on August 06, 2018 at 18:28 Hours UTC\n@mrbean-bremen If you could take a quick look at this particularly why there are issues with python 3, that would be a great help...\r\n\nThis will still fail for uncompressed frames with no BoT value.\r\n\r\nAlso the docstrings need updating.\nI've been thinking it might be better to change `get_frame_offsets` to return an empty list if the BoT has no value rather than trying to be clever about it since the two cases (BoT has no value and BoT has single 0 offset) may represent different things. Then the functions that rely on it should be updated accordingly.\nCan someone come up with a test case for an encapsulated uncompressed PixelData image?  I can't figure out how to create one with dcmtk...\r\nAlso, the standard explicitly calls out RLE syntax as having one fragment per frame, and calls out JPEG as possibly having more than one fragment per frame, but I couldn't find any statement about the fragments per frame for uncompressed syntax.  Any ideas on that one?\nI have some code I'm working on that should at least return mixed fragments/frame native data in a single blob while still following your approach for JPEG data.\r\n\r\n```python\r\n# N fragments, M frames; without BOT values there's no generic\r\n#   way to determine where a frame ends so we try our best\r\nframe = []\r\nframe_number = 0\r\nfor fragment in generate_pixel_data_fragment(fp):\r\n    # For JPEG transfer syntaxes try to locate the EOC marker\r\n    if b'\\xFF\\xD9' in fragment[-10:]:\r\n        yield tuple(frame)\r\n        frame_number += 1\r\n        frame = []\r\n    else:\r\n        frame.append(fragment)\r\n\r\n# If we failed to locate the EOC marker then either nothing will\r\n#   have been yielded and/or one or more frames will have been\r\n#   skipped. This will be the case with native transfer syntaxes\r\nif frame_number != no_frames:\r\n    yield tuple(frame)\r\n```\r\n\r\n[Here](https://github.com/pydicom/pydicom/files/2252088/encap_jpeg.zip) is some JPEG 2x2 11 frame encapsulated data with and without BOT (1 fragment per frame and 2 fragments per frame).\r\n\r\n[Here](https://github.com/pydicom/pydicom/files/2252147/encap_native.zip) is some native 2x2 11 frame encapsulated data with and without BOT (1 fragment per frame and 2 fragments per frame).\r\n\r\nImage pattern is black upper left, white upper right, white lower left, black lower right and the black pixels shade to white over the frames. Maybe just double check the encapsulation has been performed correctly. \r\n\r\nWith the code above the no BOT/native/2 fragments per frame returns a blob of the entire pixel data, but I'd expect that in that case the numpy pixel handler should be able to rearrange the data into frames correctly, provided the blob is the right length. The no BOT/jpeg/2 fragments per frame returns the frames correctly.\nI'm a bit confused by this.  When I read [this section](http://dicom.nema.org/dicom/2013/output/chtml/part05/sect_8.2.html) of DICOM,  it seems to imply that Uncompressed and Encapsulated are mutually exclusive?  I don't know if I have ever seen a DICOM uncompressed image with encapsulated pixel data.  Has anyone else?\r\n\nWhoops, obviously I've gotten confused about this somewhere. Nevermind me then...\r\n\r\n[This is what I've done.](https://github.com/scaramallion/pydicom/blob/dev-encaps/pydicom/encaps.py#L247)\n@rhaxton , @scaramallion - what is the state of this PR (apart from having conflicts) - can it make it into the 1.4 release?",
        "base": "41e984c8df5533805ae13cbcf419e6c5f63da30c",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/encaps.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/811",
        "problem": "0 byte file causes traceback on dcmreader\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nTrying to open a 0 byte file with dcmreader causes a traceback originating in the read_partial method. The problem is line 692 in filereader.py (GitHub):\r\n`    fileobj.seek(-1, 1)`\r\nChanging this to:\r\n`    if peek != b'':\r\n        fileobj.seek(-1, 1)`\r\nAppears to solve the problem, but I don't have the experience to test thoroughly.\r\n\r\n#### Steps/Code to Reproduce\r\nCreate a 0 byte file\r\n$ touch mysillyfile.dcm\r\nStart python, import pydicom and read the the file\r\n$ python3\r\nPython 3.6.8 (default, Jan  3 2019, 16:11:14) \r\n[GCC 8.2.1 20181215 (Red Hat 8.2.1-6)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import pydicom\r\n>>> image = pydicom.dcmread('mysillyfile.dcm',force=True)\r\n\r\n#### Expected Results\r\nShould either warn that the file is not DICOM or exit gracefully\r\n\r\n#### Actual Results\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/pydicom/filereader.py\", line 880, in dcmread\r\n    force=force, specific_tags=specific_tags)\r\n  File \"/usr/local/lib/python3.6/site-packages/pydicom/filereader.py\", line 693, in read_partial\r\n    fileobj.seek(-1, 1)\r\nOSError: [Errno 22] Invalid argument\r\n\r\n#### Versions\r\n>>> import platform; print(platform.platform())\r\nLinux-4.19.16-200.fc28.x86_64-x86_64-with-fedora-28-Twenty_Eight\r\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.6.8 (default, Jan  3 2019, 16:11:14) \r\n[GCC 8.2.1 20181215 (Red Hat 8.2.1-6)]\r\n>>> import numpy; print(\"numpy\", numpy.__version__)\r\nnumpy 1.16.1\r\n>>> import pydicom; print(\"pydicom\", pydicom.__version__)\r\npydicom 1.2.2\r\n>>> \r\n\r\nRegards\r\nAlan\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
        "hint": "Good catch! There is actually a test for this, but it tests a byte stream instead of a file, and seeking back past begin in a byte stream seems not to be a problem.",
        "base": "7d0889e7143f5d4773fa74606efa816ed4e54c9f",
        "env": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "files": [
            "pydicom/filereader.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1539",
        "problem": "pydicom produces invalid DICOM files if ds.EncapsulatedDocument contains byte array of odd length\n**Bug Description**\r\nWhen inserting a byte array of odd length into the ds.EncapsulatedDocument field, and saving this as a DICOM file, the DICOM file produced is not valid. This happens because the resulting file produced also have an odd number of bytes in the (0042,0011) OB Encapsulated Document DICOM tag which is not allowed according to the DICOM sepcification for Value Fields, http://dicom.nema.org/dicom/2013/output/chtml/part05/chapter_7.html\r\n\r\n**Expected behavior**\r\nEither pydicom could through and error specifying that the ds.EncapsulatedDocument field should contain an array of even length, or it could fix the problem by add and extra zero byte to the end of the ds.EncapsulatedDocument byte array when the length is odd.\r\n\r\n**Steps To Reproduce**\r\nI have written the following pdf2dcm.py command line utility to mimic the behaviour of pdf2dcm in the dcmtk suite:\r\n\r\n```python\r\n# inspired by: https://github.com/rohithkumar31/pdf2dicom\r\n\r\nimport argparse\r\nimport pydicom\r\n\r\nEncapsulatedPDFStorage = '1.2.840.10008.5.1.4.1.1.104.1'\r\n\r\n\r\ndef generate_dicom_from_pdf(input_file, output_file, zero_pad=True):\r\n    file_meta = pydicom.dataset.Dataset()\r\n\r\n    # FileMetaInformationGroupLength only gets rewritten when saved if present\r\n    file_meta.FileMetaInformationGroupLength = 206\r\n\r\n    file_meta.MediaStorageSOPClassUID = EncapsulatedPDFStorage\r\n\r\n    file_meta.MediaStorageSOPInstanceUID = pydicom.uid.generate_uid(pydicom.uid.PYDICOM_ROOT_UID)\r\n\r\n    # from: https://pydicom.github.io/pydicom/dev/reference/uid.html\r\n    file_meta.TransferSyntaxUID = pydicom.uid.ExplicitVRLittleEndian\r\n\r\n    pydicom.dataset.validate_file_meta(file_meta, enforce_standard=True)\r\n\r\n    # see: http://dicom.nema.org/dicom/2013/output/chtml/part10/chapter_7.html\r\n    preamble = b\"\\0\" * 128\r\n\r\n    ds = pydicom.dataset.FileDataset(output_file, {}, file_meta=file_meta, preamble=preamble)\r\n    # ds.fix_meta_info()\r\n\r\n    ds.is_little_endian = True\r\n    ds.is_implicit_VR = False\r\n\r\n    ds.SpecificCharacterSet = 'ISO_IR 100'\r\n\r\n    import datetime\r\n    dt = datetime.datetime.now()\r\n    ds.InstanceCreationDate = dt.strftime('%Y%m%d')\r\n    ds.InstanceCreationTime = dt.strftime('%H%M%S')  # ('%H%M%S.%f')\r\n\r\n    ds.SOPClassUID = EncapsulatedPDFStorage\r\n    ds.SOPInstanceUID = file_meta.MediaStorageSOPInstanceUID\r\n    ds.StudyDate = None\r\n    ds.AcquisitionDateTime = None\r\n    ds.StudyTime = None\r\n    ds.ContentTime = None\r\n    ds.ContentDate = None\r\n    ds.AccessionNumber = None\r\n    ds.Modality = 'DOC'  # document\r\n    ds.ConversionType = 'WSD'  # workstation\r\n    ds.Manufacturer = None\r\n    ds.ReferringPhysicianName = None\r\n    ds.PatientName = None\r\n    ds.PatientID = None\r\n    ds.PatientBirthDate = None\r\n    ds.PatientSex = None\r\n    ds.StudyInstanceUID = pydicom.uid.generate_uid()\r\n    ds.SeriesInstanceUID = pydicom.uid.generate_uid()\r\n    ds.StudyID = None\r\n    ds.SeriesNumber = 1\r\n    ds.InstanceNumber = 1\r\n    ds.BurnedInAnnotation = 'YES'\r\n    ds.ConceptNameCodeSequence = None\r\n    # ConceptNameCodeSequence also sets: ds.SequenceDelimitationItem\r\n    ds.DocumentTitle = None\r\n\r\n    with open(input_file, 'rb') as f:\r\n        pdf_file_as_bytes = f.read()\r\n\r\n    # DICOM Value Fields must according to the\r\n    # specification be an even number of bytes, see:\r\n    # http://dicom.nema.org/dicom/2013/output/chtml/part05/chapter_7.html\r\n    if zero_pad and len(pdf_file_as_bytes) % 2 != 0:\r\n        pdf_file_as_bytes += b\"\\0\"\r\n\r\n    ds.EncapsulatedDocument = pdf_file_as_bytes\r\n    ds.MIMETypeOfEncapsulatedDocument = 'application/pdf'\r\n\r\n    ds.save_as(output_file)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--disable-zero-padding', action='store_false')\r\n    parser.add_argument('-i', '--input-file')\r\n    parser.add_argument('-o', '--output-file')\r\n    args = parser.parse_args()\r\n\r\n    generate_dicom_from_pdf(args.input_file, args.output_file, args.disable_zero_padding)\r\n```\r\n\r\nTo reproduce the problem the --disable-zero-padding parameter can be added, and a pdf file of odd number of bytes in length can be used as input to the program, this will then product an output DICOM file containing an odd number of bytes in the (0042,0011) OB Encapsulated Document DICOM tag, which can be checked using the dciodvfy validation tool from the dicom3tools package:\r\n\r\n```bash\r\nwget http://dicom.nema.org/medical/dicom/current/output/pdf/part05.pdf\r\nls -l part05.pdf # should be odd number of bytes, currently 4676213 for this file\r\npython pdf2dcm.py --disable-zero-padding -i part05.pdf -o part05.dcm\r\n\r\n(0x0042,0x0011) OB Encapsulated Document  - Error - Bad Value Length - not a multiple of 2 - VL is 0x475a75 should be 0x475a76\r\nError - Dicom dataset read failed\r\n```\r\n\r\n**Environment**\r\n\r\n```bash\r\n$ python -m pydicom.env_info\r\n\r\nmodule       | version\r\n------       | -------\r\nplatform     | Linux-5.13.0-7614-generic-x86_64-with-glibc2.31\r\nPython       | 3.9.5 (default, Jun  4 2021, 12:28:51)  [GCC 7.5.0]\r\npydicom      | 2.2.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.21.2\r\nPIL          | _module not found_\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n```\r\n\n",
        "hint": "",
        "base": "a125a02132c2db5ff5cad445e4722802dd5a8d55",
        "env": "0fa18d2a2179c92efc22200ed6b3689e66cecf92",
        "files": [
            "pydicom/filewriter.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/901",
        "problem": "pydicom should not define handler, formatter and log level.\nThe `config` module (imported when pydicom is imported) defines a handler and set the log level for the pydicom logger. This should not be the case IMO. It should be the responsibility of the client code of pydicom to configure the logging module to its convenience. Otherwise one end up having multiple logs record as soon as pydicom is imported:\r\n\r\nExample:\r\n```\r\nCould not import pillow\r\n2018-03-25 15:27:29,744 :: DEBUG :: pydicom \r\n  Could not import pillow\r\nCould not import jpeg_ls\r\n2018-03-25 15:27:29,745 :: DEBUG :: pydicom \r\n  Could not import jpeg_ls\r\nCould not import gdcm\r\n2018-03-25 15:27:29,745 :: DEBUG :: pydicom \r\n  Could not import gdcm\r\n``` \r\nOr am I missing something?\n",
        "hint": "In addition, I don't understand what the purpose of the `config.debug` function since the default behavor of the logging module in absence of configuartion seems to already be the one you want.\r\n\r\nFrom https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library:\r\n\r\n> If the using application does not use logging, and library code makes logging calls, then (as described in the previous section) events of severity WARNING and greater will be printed to sys.stderr. This is regarded as the best default behaviour.\r\n\r\nand\r\n\r\n>**It is strongly advised that you do not add any handlers other than NullHandler to your library\u2019s loggers.** This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers \u2018under the hood\u2019, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements. \r\n\nI think you make good points here.  I support changing the logging to comply with python's suggested behavior.\r\n\r\n> In addition, I don't understand what the purpose of the config.debug function\r\n\r\nOne reason is that the core loop in pydicom (data_element_generator in filereader.py) is extremely optimized for speed - it checks the `debugging` flag set by config.debug, to avoid composing messages and doing function calls to logger when not needed.",
        "base": "3746878d8edf1cbda6fbcf35eec69f9ba79301ca",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/config.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1598",
        "problem": "KeyError when saving a FileSet\n**Describe the bug**\r\nSaving a fileset that was loaded using DICOMDIR returns a Key Error.\r\n\r\n**Expected behavior**\r\nFileset is saved without error\r\n\r\n**Steps To Reproduce**\r\nCode:\r\n```python\r\nfrom pydicom.fileset import FileSet\r\n\r\nfpath=\"DICOMDIR\"\r\ndata=FileSet(fpath)\r\n\r\nprint(data)\r\n\r\ndata.write(use_existing=True)\r\n```\r\n\r\n```\r\nTraceback:\r\nKeyError                                  \r\n\r\nTraceback (most recent call last) \r\n\\<ipython-input-183-effc2d1f6bc9\\> in \\<module\\>\r\n      6 print(data)\r\n      7 \r\n----> 8 data.write(use_existing=True)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/fileset.py in write(self, path, use_existing, force_implicit)\r\n   2146                 self._write_dicomdir(f, force_implicit=force_implicit)\r\n   2147 \r\n-> 2148             self.load(p, raise_orphans=True)\r\n   2149 \r\n   2150             return\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/fileset.py in load(self, ds_or_path, include_orphans, raise_orphans)\r\n   1641             ds = ds_or_path\r\n   1642         else:\r\n-> 1643             ds = dcmread(ds_or_path)\r\n   1644 \r\n   1645         sop_class = ds.file_meta.get(\"MediaStorageSOPClassUID\", None)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in dcmread(fp, defer_size, stop_before_pixels, force, specific_tags)\r\n   1032             defer_size=size_in_bytes(defer_size),\r\n   1033             force=force,\r\n-> 1034             specific_tags=specific_tags,\r\n   1035         )\r\n   1036     finally:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in read_partial(fileobj, stop_when, defer_size, force, specific_tags)\r\n    885             file_meta_dataset,\r\n    886             is_implicit_VR,\r\n--> 887             is_little_endian,\r\n    888         )\r\n    889     else:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/dicomdir.py in __init__(self, filename_or_obj, dataset, preamble, file_meta, is_implicit_VR, is_little_endian)\r\n     94 \r\n     95         self.patient_records: List[Dataset] = []\r\n---> 96         self.parse_records()\r\n     97 \r\n     98     def parse_records(self) -> None:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/dicomdir.py in parse_records(self)\r\n    143                 )\r\n    144                 if child_offset:\r\n--> 145                     child = map_offset_to_record[child_offset]\r\n    146                     record.children = get_siblings(child, map_offset_to_record)\r\n    147 \r\n\r\nKeyError: 572\r\n```\r\n\r\n**Your environment**\r\n\r\nmodule       | version\r\n------       | -------\r\nplatform     | Linux-4.15.0-142-generic-x86_64-with-debian-stretch-sid\r\nPython       | 3.7.10 (default, Feb 26 2021, 18:47:35)  [GCC 7.3.0]\r\npydicom      | 2.2.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.2\r\nPIL          | 8.2.0\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n\n",
        "hint": "This is going to be difficult to troubleshoot without the original DICOMDIR dataset. Could you create an anonymised version of it using the following and attach it please?\r\n\r\n```python\r\nfrom pydicom import dcmread\r\n\r\nds = dcmread(\"DICOMDIR\")\r\nfor item in ds.DirectoryRecordSequence:\r\n    if item.DirectoryRecordType == \"PATIENT\":\r\n        item.PatientName = \"X\" * len(item.PatientName)\r\n        item.PatientID = \"X\" * len(item.PatientID)\r\n\r\nds.save_as(\"DICOMDIR_anon\", write_like_original=True)\r\n```\r\nIf there are any other identifying elements in the DICOMDIR then just anonymise them using the same method of overwriting with a value of the same length.\nI can't reproduce with:\r\n```python\r\nfrom tempfile import TemporaryDirectory\r\nfrom pathlib import Path\r\nimport shutil\r\n\r\nfrom pydicom.data import get_testdata_file\r\nfrom pydicom.fileset import FileSet\r\n\r\n\r\n# Copy test file set to temporary directory\r\nt = TemporaryDirectory()\r\nsrc = Path(get_testdata_file(\"DICOMDIR\")).parent\r\ndst = Path(t.name)\r\n\r\nshutil.copyfile(src / 'DICOMDIR', dst / 'DICOMDIR')\r\nshutil.copytree(src / \"77654033\", dst / \"77654033\")\r\nshutil.copytree(src / \"98892003\", dst / \"98892003\")\r\nshutil.copytree(src / \"98892001\", dst / \"98892001\")\r\n\r\n# Load\r\nfs = FileSet(dst / \"DICOMDIR\")\r\n# Write without changes\r\nfs.write(use_existing=True)\r\n```\r\nI strongly suspect there's a bad offset being written in your DICOMDIR for some reason, but without seeing the original I can't really do much.",
        "base": "e9fc645cd8e75d71f7835c0d6e3c0b94b22c2808",
        "env": "0fa18d2a2179c92efc22200ed6b3689e66cecf92",
        "files": [
            "pydicom/dataset.py",
            "pydicom/fileset.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1139",
        "problem": "Make PersonName3 iterable\n```python\r\nfrom pydicom import Dataset\r\n\r\nds = Dataset()\r\nds.PatientName = 'SomeName'\r\n\r\n'S' in ds.PatientName\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: argument of type 'PersonName3' is not iterable\r\n```\r\n\r\nI'm not really sure if this is intentional or if PN elements should support `str` methods. And yes I know I can `str(ds.PatientName)` but it's a bit silly, especially when I keep having to write exceptions to my element iterators just for PN elements.\n",
        "hint": "I think it is reasonable to support at least some `str` methods (definitely `__contains__` for the example above), but there are many that don't make a lot of sense in this context though - e.g. `join`, `ljust`, `maketrans`, `splitlines` just to name a few, but I suppose each would either never be actually used or would have no effect.\r\n\r\nI have a vague memory that one or more of the `PersonName` classes was at one time subclassed from `str`, or at least that it was discussed... does anyone remember?  Maybe it would be easier now with only Python 3 supported.\n`PersonName` was derived from `str` or `unicode` in Python 2, but that caused a number of problems, which is why you switched to `PersonName3` in Python 3, I think. I agree though that it makes sense to implement `str` methods, either by implementing some of them, or generically by adding `__getattr__` that converts it to `str` and applies the attribute to that string. ",
        "base": "b9fb05c177b685bf683f7f57b2d57374eb7d882d",
        "env": "9d69811e539774f296c2f289839147e741251716",
        "files": [
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/793",
        "problem": "Print byte values for unknown VR during read\n#### Description\r\nIf the dataset read fails due to an unknown VR then the exception message prints the VR bytes in a format that isn't useful for debugging.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom io import BytesIO\r\nfrom pydicom.filereader import read_dataset\r\nds = read_dataset(BytesIO(b'\\x08\\x00\\x01\\x00\\x04\\x00\\x00\\x00\\x00\\x08\\x00\\x49'), False, True)\r\nprint(ds)\r\n```\r\n\r\n#### Expected Results\r\n```\r\nNotImplementedError: Unknown Value Representation: '32 31' in tag (0000, 0002)\r\n```\r\n#### Actual Results\r\n```\r\nFile \"<stdin>\", line 1, in <module>\r\n  File \".../pydicom/pydicom/dataset.py\", line 1284, in __str__\r\n    return self._pretty_str()\r\n  File \".../pydicom/pydicom/dataset.py\", line 1022, in _pretty_str\r\n    for data_element in self:\r\n  File \".../pydicom/pydicom/dataset.py\", line 751, in __iter__\r\n    yield self[tag]\r\n  File \".../pydicom/pydicom/dataset.py\", line 637, in __getitem__\r\n    self[tag] = DataElement_from_raw(data_elem, character_set)\r\n  File \".../pydicom/pydicom/dataelem.py\", line 447, in DataElement_from_raw\r\n    raise NotImplementedError(\"{0:s} in tag {1!r}\".format(str(e), raw.tag))\r\nNotImplementedError: Unknown Value Representation '\u0004' in tag (0008, 0001)\r\n```\r\n[Or see here for another example](https://user-images.githubusercontent.com/28559755/51027486-4abf4100-1591-11e9-8f44-a739b00ca300.PNG)\r\n\r\n\n",
        "hint": "",
        "base": "897fe092ae3ef282a21c894b47134233bdd5cdd0",
        "env": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "files": [
            "pydicom/values.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1192",
        "problem": "\"TypeError: 'NoneType' object is not subscriptable\" when reading dcm file with empty string as Chartset and \"use_none_as_empty_text_VR_value=True\"\n**Describe the bug**\r\nOnce thing I noticed is that `convert_encodings` in `charset.py` expects a list of encodings (according to the docstrings) from tag `0008,0005` but it can be just a value. \r\n\r\nThe problem is when reading Dicom files in production environments I noticed that some devices that are capturing the DICOMs are not very DICOM Compliant and is sending empty string , which it should be allowed as `0008,0005` is a 1C type, which means that if present it should have a valid value. \r\n\r\nI enabled `use_none_as_empty_text_VR_value` to make sure other tags whose value should be float or int have None instead of empty string, but if `0008,0005` value is empty string is switched to None and `convert_encodings` fails with `TypeError: 'NoneType' object is not subscriptable`\r\n\r\n**Expected behavior**\r\nThe expected behavior should be that if empty string or not present it should default to:\r\n```\r\n# default encoding if no encoding defined - corresponds to ISO IR 6 / ASCII\r\ndefault_encoding = \"iso8859\"\r\n```\r\n\r\n**Steps To Reproduce**\r\n\r\nout.dcm file if provided for testing with mock data but `Specific Character Set` set to empty string\r\n\r\nIf setting the `(0008, 0005) Specific Character Set` to empty string and setting `pydicom.config.use_none_as_empty_text_VR_value = True`\r\n\r\n```\r\n>>> import pydicom\r\n>>> pydicom.config.datetime_conversion = True\r\n>>> pydicom.config.allow_DS_float = True\r\n>>> pydicom.config.use_none_as_empty_text_VR_value = True\r\n>>> dataset = pydicom.dcmread(\"test.dcm\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/bernardo/.virtualenvs/backend-api/lib/python3.7/site-packages/pydicom/filereader.py\", line 871, in dcmread\r\n    force=force, specific_tags=specific_tags)\r\n  File \"/Users/bernardo/.virtualenvs/backend-api/lib/python3.7/site-packages/pydicom/filereader.py\", line 744, in read_partial\r\n    specific_tags=specific_tags)\r\n  File \"/Users/bernardo/.virtualenvs/backend-api/lib/python3.7/site-packages/pydicom/filereader.py\", line 383, in read_dataset\r\n    encoding = convert_encodings(char_set)\r\n  File \"/Users/bernardo/.virtualenvs/backend-api/lib/python3.7/site-packages/pydicom/charset.py\", line 638, in convert_encodings\r\n    encodings = encodings[:]\r\nTypeError: 'NoneType' object is not subscriptable\r\n>>> pydicom.config.use_none_as_empty_text_VR_value = False\r\n>>> dataset = pydicom.dcmread(\"test.dcm\")\r\n```\r\n`(0008, 0005) Specific Character Set              CS: ''`\r\n\r\n**Your environment**\r\n\r\n```bash\r\npython -m pydicom.env_info\r\nmodule       | version\r\n------       | -------\r\nplatform     | Darwin-19.6.0-x86_64-i386-64bit\r\nPython       | 3.7.6 (default, Dec 30 2019, 19:38:26)  [Clang 11.0.0 (clang-1100.0.33.16)]\r\npydicom      | 2.0.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | _module not found_\r\nPIL          | 7.0.0\r\n```\r\n\r\n\r\n[out.dcm.zip](https://github.com/pydicom/pydicom/files/5248618/out.dcm.zip)\r\n\n",
        "hint": "As you wrote, an empty string is not allowed as Specific Character Set, but I agree that this is a case that we shall handle gracefully (e.g. just handle as if the tag were absent). I will have a look.",
        "base": "1f099ae0f75f0e2ed402a21702e584aac54a30ef",
        "env": "9d69811e539774f296c2f289839147e741251716",
        "files": [
            "pydicom/charset.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1236",
        "problem": "apply_voi_lut - unclear what it does if both WL/VOILUTFunction _and_ VOILUTSequence are present\nhttps://pydicom.github.io/pydicom/dev/reference/generated/pydicom.pixel_data_handlers.util.html#pydicom.pixel_data_handlers.util.apply_voi_lut\r\n\r\nHi all,\r\n\r\nI'm working with some mammo image (digital) that have both \r\n- window/level (0028,1050 0028,1051) plus VOILUTFunction (0028,1056) (set to SIGMOID) (set of 3 WL values)\r\n- VOILUT sequences (0028, 3010)\r\n\r\nspecified.\r\n\r\nProblem\r\n---\r\n\r\nIt's unclear from the documentation when both a VOILUT (0028,3010) _and_ WL (0028,1051...) are present which is applied - the lut or the wl.\r\n\r\nIt just says if a LUT's present, it will apply that, and if a WL set is present it will apply that.\r\n\r\nQuestions\r\n---\r\n\r\n- If both LUT and WL are supplied, by the dicom standard, which should be applied?\r\n- Separately to the above question about which is applied, if _both_ LUT and WL sequences are supplied, is there a way in `apply_voi_lut` to specify applying one or the other?  (ie force application of the WL instead of LUT etc)\r\n\r\n- Also, if an image has a sequence of WL values rather than being single valued (so 0028,1050 & 0028,1051 are sequences), does the `index` parameter to `apply_voi_lut` apply to specify which in the sequence you want to use?\r\n\r\nThanks!\r\n\napply_voi_lut can't handle missing DICOM meta info\nI have encountered two real life examples where `apply_voi_lut` does not handle corruption in DICOM meta fields\r\n\r\ncase 1:\r\n```\r\n(0028, 1050) Window Center                       DS: \"128.0\"\r\n(0028, 1051) Window Width                        DS: \"256.0\"\r\n(0028, 1052) Rescale Intercept                   DS: None\r\n(0028, 1053) Rescale Slope                       DS: None\r\n```\r\nthrows an exception\r\n\r\n```\r\n  File \"python3.7/site-packages/pydicom/pixel_data_handlers/util.py\", line 380, in apply_voi_lut\r\n    y_min = y_min * ds.RescaleSlope + ds.RescaleIntercept\r\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType' \r\n```\r\n\r\n\r\ncase 2:\r\n\r\n```\r\n(0028, 1050) Window Center                       DS: \"2607.0\"\r\n(0028, 1051) Window Width                        DS: \"2785.0\"\r\n(0028, 1052) Rescale Intercept                   DS: \"0.0\"\r\n(0028, 1053) Rescale Slope                       DS: \"1.0\"\r\n(0028, 1054) Rescale Type                        LO: 'US'\r\n(0028, 2110) Lossy Image Compression             CS: '00'\r\n(0028, 3010)  VOI LUT Sequence   1 item(s) ---- \r\n   (0028, 3002) LUT Descriptor                      SS: None\r\n   (0028, 3003) LUT Explanation                     LO: 'Noramal'\r\n   (0028, 3006) LUT Data                            OW: None\r\n```\r\n\r\nthrows an exception\r\n\r\n```\r\n  File \"python3.7/site-packages/pydicom/pixel_data_handlers/util.py\", line 312, in apply_voi_lut\r\n    nr_entries = item.LUTDescriptor[0] or 2**16\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\n\r\nSo far I have handled this with:\r\n\r\n```\r\n    def _lut_convert(self):\r\n        return apply_voi_lut(self.input_dicom.pixel_array, self.input_dicom)\r\n\r\n    def _get_raw_data(self):\r\n\r\n        # convert to presentation LUT\r\n        try:\r\n            data = self._lut_convert()\r\n        # many things can be corrupted in the VOILUTSequence attribute,\r\n        # fall back to default WC/WW conversion\r\n        except Exception as e:\r\n            try:\r\n                if \"VOILUTSequence\" in self.input_dicom:\r\n                    del self.input_dicom[\"VOILUTSequence\"]\r\n                    data = self._lut_convert()\r\n            except Exception as e:\r\n                raise InvalidImage(f\"Could not convert to presentation LUT due to: {e}\")\r\n```\r\n\r\nWhile the case 1 could be seen as an expected behavior (?), I imagine case 2 should be handled by WC/WW transformations if followed DICOM standard?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n",
        "hint": "For some reason when I wrote it, I assumed it was a case of either/or for *VOI LUT Sequence*/*Window Center*, but now that I look at the Standard again I see it possible that both can be present (although only one can be applied). I'll probably add a flag to allow selecting which one is used when both are present.\n> If both LUT and WL are supplied, by the dicom standard, which should be applied?\r\n\r\nThat should be up to the user\r\n\r\n> Separately to the above question about which is applied, if both LUT and WL sequences are supplied, is there a way in apply_voi_lut to specify applying one or the other? (ie force application of the WL instead of LUT etc)\r\n\r\nNot at the moment, although you could force which is applied by deleting the corresponding element(s) for the operation you don't want\r\n\r\n> Also, if an image has a sequence of WL values rather than being single valued (so 0028,1050 & 0028,1051 are sequences), does the index parameter to apply_voi_lut apply to specify which in the sequence you want to use?\r\n\r\nYes\nIs it possible that you could attach an anonymised dataset we could add to our test data? It'd be helpful to have a real dataset.\n",
        "base": "c2c6145d679adc97924d6c8a761a50b8e2819e3f",
        "env": "9d69811e539774f296c2f289839147e741251716",
        "files": [
            "pydicom/pixel_data_handlers/__init__.py",
            "pydicom/pixel_data_handlers/util.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/955",
        "problem": "LUT Descriptor values don't follow standard\n**Describe the bug**\r\n(0028,3002) [LUT Descriptor](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.11.html#sect_C.11.1.1) has VM = 3, with value as `[number of entries in LUT, first stored pixel value mapped, LUT entry bit depth]`. The VR for the element is ambiguous and may be US or SS depending on the value of (0028,0103) Pixel Representation, however this only affects the second value, not the first or last which are always US.\r\n\r\nThe problem is that a Pixel Representation value of 1 (i.e. 2s complement) gives a LUT Descriptor value 1 as signed when it should always be unsigned.\r\n\r\n> Since LUT Descriptor (0028,3002) is multi-valued, in an Explicit VR Transfer Syntax, only one value representation (US or SS) may be specified, even though the first and third values are always by definition interpreted as unsigned. The explicit VR actually used is dictated by the VR needed to represent the second value, which will be consistent with Pixel Representation (0028,0103).\r\n\r\nAlso affects Red/Green/Blue Palette Color Lookup Table Descriptor.\r\n\r\n**Steps To Reproduce**\r\n```python\r\nfrom pydicom import dcmread\r\nfrom pydicom.filebase import DicomBytesIO\r\n\r\n# Explicit VR: SS\r\nlut = b'\\x28\\x00\\x02\\x30\\x53\\x53\\x06\\x00\\x00\\xf5\\x00\\xf8\\x10\\x00'\r\n\r\nbs = DicomBytesIO(lut)\r\nbs.is_little_endian = True\r\nbs.is_implicit_VR = False\r\n\r\nds = dcmread(bs, force=True)\r\nassert ds.LUTDescriptor == [62720, -2048, 16]\r\n```\r\n\r\n**Your environment**\r\nAffects current `master`\r\n\n",
        "hint": "",
        "base": "fdd4fc8098920b1cda6127bdc05ff1e542b519fb",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/dataelem.py",
            "pydicom/pixel_data_handlers/util.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1031",
        "problem": "Crash writing DICOM with 1.4.0\npydicom 1.4.0\r\nWindows-10-10.0.18362-SP0\r\nPython  3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]\r\nGDCM 3.0.2\r\nPillow 7.0.0\r\n\r\nType error raises when writing file with pydicom 1.4.0, works in 1.3.0.\r\n\r\n```\r\nds = pydicom.read_file('fail2404.anon.dcm')\r\n#print(ds.get((0x0040, 0x0275)))\r\nds.save_as('bort.dcm')\r\n```\r\n\r\nInterestingly, the crash goes away if the offending tag is accessed (uncomment the print and then the `save_as` works fine).\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\tag.py\", line 30, in tag_in_exception\r\n    yield\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 555, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 463, in write_data_element\r\n    buffer.write(data_element.value)\r\nTypeError: a bytes-like object is required, not 'list'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./pydcmbug.py\", line 7, in <module>\r\n    ds.save_as('bort.dcm')\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\dataset.py\", line 1810, in save_as\r\n    pydicom.dcmwrite(filename, self, write_like_original)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 946, in dcmwrite\r\n    write_dataset(fp, get_item(dataset, slice(0x00010000, None)))\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 555, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"C:\\Program Files\\Python37\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\tag.py\", line 37, in tag_in_exception\r\n    raise type(ex)(msg)\r\nTypeError: With tag (0040, 0275) got exception: a bytes-like object is required, not 'list'\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\tag.py\", line 30, in tag_in_exception\r\n    yield\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 555, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 463, in write_data_element\r\n    buffer.write(data_element.value)\r\nTypeError: a bytes-like object is required, not 'list'\r\n```\r\n\r\n[fail.zip](https://github.com/pydicom/pydicom/files/4072693/fail.zip)\r\n\n",
        "hint": "Ok, found the problem. This has been introduced by by PR #965 (by myself). Will provide a fix ASAP...\r\n",
        "base": "64f5b8daaa798836579c56912244b7732ab073be",
        "env": "5098c9147fadcb3e5918487036867931435adeb8",
        "files": [
            "pydicom/dataelem.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1555",
        "problem": "Converting Dicom image to Png\n**Describe the issue**\r\nhi, i am trying to convert Dicom image to png but in case of some particular file i am getting this \"list out of range error\".\r\n\r\n**Expected behavior**\r\ndicom image converted to png pne\r\n\r\n**Steps To Reproduce**\r\nHow to reproduce the issue. Please include:\r\n1. A minimum working code sample\r\n```\r\nfrom pydicom import dcmread\r\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\r\n    dicom = dcmread(path, force=True)\r\n    \r\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\r\n    if voi_lut:\r\n        data = apply_voi_lut(dicom.pixel_array, dicom)\r\n    else:\r\n        data = dicom.pixel_array\r\n               \r\n    # depending on this value, X-ray may look inverted - fix that:\r\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\r\n        data = np.amax(data) - data\r\n        \r\n    data = data - np.min(data)\r\n    data = data / np.max(data)\r\n    data = (data * 255).astype(np.uint8)\r\n        \r\n    return data\r\n\r\nimg = read_xray('/content/a.5545da1153f57ff8425be6f4bc712c090e7e22efff194da525210c84aba2a947.dcm')\r\nplt.figure(figsize = (12,12))\r\nplt.imshow(img)\r\n```\r\n2. The traceback (if one occurred)\r\n```\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-13-6e53d7d16b90> in <module>()\r\n     19     return data\r\n     20 \r\n---> 21 img = read_xray('/content/a.5545da1153f57ff8425be6f4bc712c090e7e22efff194da525210c84aba2a947.dcm')\r\n     22 plt.figure(figsize = (12,12))\r\n     23 plt.imshow(img)\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/pydicom/multival.py in __getitem__(self, index)\r\n     93         self, index: Union[slice, int]\r\n     94     ) -> Union[MutableSequence[_ItemType], _ItemType]:\r\n---> 95         return self._list[index]\r\n     96 \r\n     97     def insert(self, position: int, val: _T) -> None:\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\n3. Which of the following packages are available and their versions:\r\n  * Numpy : latest as of 29th dec\r\n  * Pillow : latest as of 29th dec\r\n  * JPEG-LS : latest as of 29th dec\r\n  * GDCM : latest as of 29th dec\r\n4. The anonymized DICOM dataset (if possible).\r\nimage link : https://drive.google.com/file/d/1j13XTTPCLX-8e7FE--1n5Staxz7GGNWm/view?usp=sharing\r\n\r\n**Your environment**\r\nIf you're using **pydicom 2 or later**, please use the `pydicom.env_info`\r\nmodule to gather information about your environment and paste it in the issue:\r\n\r\n```bash\r\n$ python -m pydicom.env_info\r\n```\r\n\r\nFor **pydicom 1.x**, please run the following code snippet and paste the\r\noutput.\r\n\r\n```python\r\nimport platform, sys, pydicom\r\nprint(platform.platform(),\r\n      \"\\nPython\", sys.version,\r\n      \"\\npydicom\", pydicom.__version__)\r\n```\r\n\n",
        "hint": "```\r\nTraceback (most recent call last):\r\n  File \"pyd1554.py\", line 29, in <module>\r\n    img = read_xray('datasets/pyd1554.dcm')\r\n  File \"...pyd1554.py\", line 14, in read_xray\r\n    data = apply_voi_lut(dicom.pixel_array, dicom)\r\n  File \".../pydicom/pixel_data_handlers/util.py\", line 348, in apply_voi_lut\r\n    ds.VOILUTSequence[0].get('LUTDescriptor', None),\r\n  File \".../pydicom/multival.py\", line 95, in __getitem__\r\n    return self._list[index]\r\nIndexError: list index out of range\r\n```\r\nThe *VOI LUT Sequence* is empty, which is probably non-conformant but I can't actually tell what the *SOP Class UID* is (your anonymiser is weird, but probably mammo).\r\n\r\nHere's the whole (also weird) *VOI LUT Sequence* (in hex):\r\n```\r\n       Tag | SQ  |     | Length    | Seq end delimiter     |\r\n28 00 10 30 53 51 00 00 FF FF FF FF FE FF DD E0 00 00 00 00\r\n```\r\n\r\nI might add a check for an empty sequence in `apply_voi_lut` (and the other visualisation functions).\r\n",
        "base": "9db89e1d8f5e82dc617f9c8cbf303fe23a0632b9",
        "env": "0fa18d2a2179c92efc22200ed6b3689e66cecf92",
        "files": [
            "pydicom/dataset.py",
            "pydicom/encoders/base.py",
            "pydicom/filebase.py",
            "pydicom/fileset.py",
            "pydicom/filewriter.py",
            "pydicom/pixel_data_handlers/util.py",
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/809",
        "problem": "\"Printing\" of certain dicom files fails once, but works the second time\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n\"Printing\" of certain dicom files (see [example](https://github.com/pydicom/pydicom/files/2865551/dicom_exception.zip)) fails once, but not the second time\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom pydicom import read_file\r\n\r\na = read_file('...')\r\nprint(a)\r\n# triggers exception: AttributeError: With tag (0028, 3000) got exception: Failed to resolve ambiguous VR for tag (0028, 3002): 'Dataset' object has no attribute 'PixelRepresentation'\r\n\r\n# try same thing again...\r\nprint(a)\r\n# just works...\r\n```\r\n\r\n#### Versions\r\nBehaviour as described above at least on:\r\n```\r\nLinux-4.18.0-15-generic-x86_64-with-Ubuntu-18.10-cosmic\r\n('Python', '2.7.15+ (default, Oct  2 2018, 22:12:08) \\n[GCC 8.2.0]')\r\n('numpy', '1.14.5')\r\n('pydicom', '1.3.0.dev0')\r\n```\r\nand\r\n\r\n\r\n```\r\n('pydicom', '1.2.2')\r\n```\r\n\r\nWorks as expected on:\r\n```\r\nLinux-4.18.0-15-generic-x86_64-with-Ubuntu-18.10-cosmic\r\n('Python', '2.7.15+ (default, Oct  2 2018, 22:12:08) \\n[GCC 8.2.0]')\r\n('pydicom', '1.0.1')\r\n```\n",
        "hint": "Occurs because Pixel Representation is in the top level-dataset while the ambiguous element is in a sequence.\r\n\r\nRegression test:\r\n```python\r\nfrom pydicom.dataset import Dataset\r\n\r\nds = Dataset()\r\nds.PixelRepresentation = 0\r\nds.ModalityLUTSequence = [Dataset()]\r\nds.ModalityLUTSequence[0].LUTDescriptor = [0, 0, 16]\r\nds.ModalityLUTSequence[0].LUTExplanation = None\r\nds.ModalityLUTSequence[0].ModalityLUTType = 'US'  # US = unspecified\r\nds.ModalityLUTSequence[0].LUTData = b'\\x0000\\x149a\\x1f1c\\c2637'\r\n\r\nds.is_little_endian= True\r\nds.is_implicit_VR = False\r\nds.save_as('test.dcm')\r\n```\r\n\r\nThe reason it works the second time is the ambiguous VR correction only gets used during the initial decoding (pydicom uses deferred decoding which is triggered by the first `print()`).\r\n\r\nThis might be a bit tricky to fix elegantly...\nOne thing we should probably change is to warn rather than raise if ambiguous correction fails during decoding. Should still raise if occurs during encoding\n> This might be a bit tricky to fix elegantly...\r\n\r\nYes... we have to support the cases where the tag needed to resolve the ambiguity is in the sequence item, or in any parent dataset (for nested sequences). Having the parent dataset as a member in the dataset would allow this, but this would also mean, that it has always to be set on creating a sequence item... not sure if this is a good idea. \nAnother option is to pass the function a dict/Dataset containing elements required for resolving ambiguity (if they're present)",
        "base": "356a51ab4bc54fd18950041ebc44dbfa1a425a10",
        "env": "b4b44acbf1ddcaf03df16210aac46cb3a8acd6b9",
        "files": [
            "pydicom/dataset.py",
            "pydicom/filewriter.py",
            "pydicom/sequence.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/933",
        "problem": "Deferred Read Fails For File-Like Objects\n#### Description\r\nDeferred reads are failing when dcmread is passed a file-like object (instead of a filepath).  There are two old issues from 2014 which describe the same issue which were apparently fixed, but I'm still seeing it on v1.3:\r\nhttps://github.com/pydicom/pydicom/issues/104\r\nhttps://github.com/pydicom/pydicom/issues/74\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\nimport io\r\nimport pydicom\r\n\r\nwith open(\"./0.dcm\", \"rb\") as fp:\r\n   data = fp.read()\r\nfilelike = io.BytesIO(data)\r\n\r\ndataset = pydicom.dcmread(filelike, defer_size=1024)\r\nprint(len(dataset.PixelData))\r\n```\r\n\r\n#### Expected Results\r\nPydicom should hold onto the supplied file-like and use that for the deferred read, rather than trying to grab the file-like's .name/.filename attr and use that to re-open.  It could also hold onto it's own open'd file-like (if supplied a file_path) and use that for deferred reads to simplify things.\r\n\r\n#### Actual Results\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/pydicom/dataset.py\", line 747, in __getattr__\r\n    data_elem = self[tag]\r\n  File \"/usr/local/lib/python3.6/dist-packages/pydicom/dataset.py\", line 826, in __getitem__\r\n    data_elem)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pydicom/filereader.py\", line 911, in read_deferred_data_element\r\n    raise IOError(\"Deferred read -- original filename not stored. \"\r\nOSError: Deferred read -- original filename not stored. Cannot re-open\r\n\r\n#### Versions\r\nLinux-4.18.0-25-generic-x86_64-with-Ubuntu-18.10-cosmic\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\npydicom 1.3.0\n",
        "hint": "This certainly makes sense, though deferred reads have not been implemented for file-like, so I see this as a feature request. \r\n@darcymason - I suggest to flag this for the 1.4 release.\nNote that the two mentioned issues have not addressed this - the first one was not about deferred read, the second one was about deferred read from a gzip file (the fix just made sure the correct filename was used).",
        "base": "38436b6824c079564b8760ea6acfa4c0fd3ee9c3",
        "env": "7241f5d9db0de589b230bb84212fbb643a7c86c3",
        "files": [
            "pydicom/dataset.py",
            "pydicom/filereader.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1633",
        "problem": "OverflowError \"VR of 'DS' must be <= 16 characters long\" triggered when element is 16 characters long\n**Describe the bug**\r\n\r\n`OverflowError` triggered while accessing `PixelData`, which the values compliant with the standard. In the sample referenced in the example below, we have this, which satisfies DS VR:\r\n\r\n```\r\n(0028,0030) DS [.002006091181818\\.002006091181818]      #  34, 2 PixelSpacing\r\n```\r\n\r\nBut nevertheless the error is triggered while trying to access `PixelData`:\r\n\r\n```\r\nOverflowError: Values for elements with a VR of 'DS' must be <= 16 characters long, \r\nbut the float provided requires > 16 characters to be accurately represented. Use a \r\nsmaller string, set 'config.settings.reading_validation_mode' to 'WARN' to override \r\nthe length check, or explicitly construct a DS object with 'auto_format' set to True\r\n```\r\n\r\n**Expected behavior**\r\n\r\n`OverflowError` does not get triggered.\r\n\r\n**Steps To Reproduce**\r\n\r\nFollow the steps of this Colab notebook: https://colab.research.google.com/drive/1FcSgjBKazh0YN-jlJYdID0YUTh90CAvZ?usp=sharing\r\n\r\n**Your environment**\r\n\r\n```\r\nmodule       | version\r\n------       | -------\r\nplatform     | Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\nPython       | 3.7.13 (default, Mar 16 2022, 17:37:17)  [GCC 7.5.0]\r\npydicom      | 2.3.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.21.5\r\nPIL          | 9.1.0\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n```\r\n\r\nRelated issue: https://github.com/imi-bigpicture/wsidicom/issues/49\r\n\r\ncc: @DanielaSchacherer @dclunie @hackermd \n",
        "hint": "For reference, a possibly similar issue came up in dcmjs: https://github.com/dcmjs-org/dcmjs/issues/175\nI had a quick look, and the problem seems to be that the length is not taken from the original string, but from the string representation, which in this case adds a leading zero... This check has been introduced in pydicom 2.2.0.",
        "base": "98ac88706e7ab17cd279c94949ac6af4e87f341d",
        "env": "a8be738418dee0a2b93c241fbd5e0bc82f4b8680",
        "files": [
            "pydicom/valuerep.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1428",
        "problem": "Allow to search a list of elements in a `FileSet` while only loading instances once, to drastically improve execution time\n**Is your feature request related to a problem? Please describe.**\r\nCurrently, `fileset.FileSet.find_values` only allows for elements to be searched for one at a time. When executing this action while setting `load` to `True`, this results in a substantial overhead.\r\n\r\n**Describe the solution you'd like**\r\nThe following example code allows loading the instances once, and iterating over a list of elements to find:\r\n```python\r\ndef find_values_quick(self, elements, instances=None):\r\n    results = {element: [] for element in elements}\r\n    instances = instances or iter(self)\r\n    for instance in instances:\r\n        instance = instance.load()\r\n        for element in elements:\r\n            if element not in instance:\r\n                continue\r\n            val = instance[element].value\r\n            if val not in results[element]:\r\n                results[element].append(val)\r\n    return results\r\n```\n",
        "hint": "Sounds good, do you want to do the PR? Just change `FileSet.find_values` to support an element or iterable of elements.",
        "base": "674da68db47a71ee6929288a047b56cf31cf8168",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/fileset.py"
        ]
    },
    {
        "pr": "pydicom/pydicom/1256",
        "problem": "from_json does not correctly convert BulkDataURI's in SQ data elements\n**Describe the bug**\r\nWhen a DICOM object contains large data elements in SQ elements and is converted to JSON, those elements are correctly turned into BulkDataURI's. However, when the JSON is converted back to DICOM using from_json, the BulkDataURI's in SQ data elements are not converted back and warnings are thrown.\r\n\r\n**Expected behavior**\r\nThe BulkDataURI's in SQ data elements get converted back correctly.\r\n\r\n**Steps To Reproduce**\r\nTake the `waveform_ecg.dcm` in the test data, convert it to JSON, and then convert the JSON to DICOM\r\n\r\n**Your environment**\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.7-x86_64-i386-64bit\r\nPython       | 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18)  [Clang 6.0 (clang-600.0.57)]\r\npydicom      | 2.1.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | _module not found_\r\nPIL          | _module not found_\r\n\r\nThe problem is in `jsonrep.py` at line 227. I plan on submitting a pull-request today for this.\n",
        "hint": "",
        "base": "49a3da4a3d9c24d7e8427a25048a1c7d5c4f7724",
        "env": "506ecea8f378dc687d5c504788fc78810a190b7a",
        "files": [
            "pydicom/jsonrep.py"
        ]
    }
]